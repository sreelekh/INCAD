{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.414842Z",
     "start_time": "2020-05-20T04:31:42.428915Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Import Libraries\n",
    "# import matplotlib\n",
    "# matplotlib.use('agg')\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
    "# %load_ext line_profiler\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import time, random               # add some random sleep time\n",
    "import scipy\n",
    "import glob\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "import re\n",
    "import itertools\n",
    "import six\n",
    "import zipfile\n",
    "import shutil\n",
    "import h5py\n",
    "from scipy.io import arff\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "# from spot import SPOT\n",
    "from os.path import isfile, join\n",
    "# from statsutils import *\n",
    "from boltons.statsutils import *\n",
    "from datetime import datetime\n",
    "from itertools import repeat\n",
    "from ipywidgets import interact\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils import check_random_state, shuffle\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.svm import OneClassSVM as ocsvm\n",
    "from sklearn import metrics, mixture,svm\n",
    "from sklearn.neighbors import KNeighborsClassifier,LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "from scipy import linalg\n",
    "from scipy.special import gamma, factorial, digamma,betaln, gammaln\n",
    "from scipy.stats import beta, multivariate_normal, wishart,invwishart,t, mode\n",
    "from scipy.stats import genextreme as gev\n",
    "import scipy.spatial as sp\n",
    "import scipy.io\n",
    "from scipy.io import arff\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib as mpl\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=2)\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spot\n",
    "print(dir(spot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.751796Z",
     "start_time": "2020-05-20T04:31:46.747646Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Global Figure Parameters\n",
    "import matplotlib.pylab as pylab\n",
    "plot_params = {'legend.fontsize': 60,\n",
    "          'figure.figsize': (25*2, 15*2),\n",
    "         'axes.labelsize': 60*2,\n",
    "         'axes.titlesize':80*2,\n",
    "         'xtick.labelsize':40*2,\n",
    "         'ytick.labelsize':40*2}\n",
    "pylab.rcParams.update(plot_params)\n",
    "global fig_len\n",
    "global fig_wid\n",
    "global m_size\n",
    "\n",
    "fig_len=8\n",
    "fig_wid=8\n",
    "m_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.814010Z",
     "start_time": "2020-05-20T04:31:46.773140Z"
    },
    "code_folding": [
     0,
     11,
     119,
     150,
     184
    ]
   },
   "outputs": [],
   "source": [
    "def eval_perf(X,y):\n",
    "    np.random.seed(4323)\n",
    "    labels=y\n",
    "    outliers_fraction= np.sum(y)/len(y)\n",
    "    eval_metrics={}\n",
    "    eval_preds={}\n",
    "#     ros = RandomOverSampler(random_state=0)\n",
    "#     X_resampled, y_resampled = ros.fit_resample(np.arange(0,len(X),1).reshape(-1,1), y)\n",
    "#     train, test = train_test_split(X_resampled, test_size=0.2,random_state=200)\n",
    "#     train=train[:,0]\n",
    "    train=np.arange(0,len(X),1)\n",
    "    if 1:\n",
    "        eval_metrics['LOF']={}\n",
    "        eval_metrics['LOF']['fpr']={}\n",
    "        eval_metrics['LOF']['tpr']={}\n",
    "        eval_metrics['LOF']['thresholds']={}\n",
    "        eval_metrics['LOF']['fp']={}\n",
    "        eval_metrics['LOF']['tp']={}\n",
    "        eval_metrics['LOF']['fn']={}\n",
    "        eval_metrics['LOF']['tn']={}\n",
    "        eval_metrics['LOF']['recall']={}\n",
    "        eval_metrics['LOF']['specificity']={}\n",
    "        eval_metrics['LOF']['precision']={}\n",
    "        eval_metrics['LOF']['accuracy']={}\n",
    "        eval_metrics['LOF']['fmeasure']={}\n",
    "        eval_metrics['LOF']['purity']={}\n",
    "        eval_metrics['LOF']['auc']={}    \n",
    "\n",
    "        eval_metrics['Kmeans--']={}\n",
    "        eval_metrics['Kmeans--']['fpr']={}\n",
    "        eval_metrics['Kmeans--']['tpr']={}\n",
    "        eval_metrics['Kmeans--']['thresholds']={}\n",
    "        eval_metrics['Kmeans--']['fp']={}\n",
    "        eval_metrics['Kmeans--']['tp']={}\n",
    "        eval_metrics['Kmeans--']['fn']={}\n",
    "        eval_metrics['Kmeans--']['tn']={}\n",
    "        eval_metrics['Kmeans--']['recall']={}\n",
    "        eval_metrics['Kmeans--']['specificity']={}\n",
    "        eval_metrics['Kmeans--']['precision']={}\n",
    "        eval_metrics['Kmeans--']['accuracy']={}\n",
    "        eval_metrics['Kmeans--']['fmeasure']={}\n",
    "        eval_metrics['Kmeans--']['purity']={}\n",
    "        eval_metrics['Kmeans--']['auc']={}    \n",
    "\n",
    "        eval_metrics['knn']={}\n",
    "        eval_metrics['knn']['fpr']={}\n",
    "        eval_metrics['knn']['tpr']={}\n",
    "        eval_metrics['knn']['thresholds']={}\n",
    "        eval_metrics['knn']['fp']={}\n",
    "        eval_metrics['knn']['tp']={}\n",
    "        eval_metrics['knn']['fn']={}\n",
    "        eval_metrics['knn']['tn']={}\n",
    "        eval_metrics['knn']['recall']={}\n",
    "        eval_metrics['knn']['specificity']={}\n",
    "        eval_metrics['knn']['precision']={}\n",
    "        eval_metrics['knn']['accuracy']={}\n",
    "        eval_metrics['knn']['fmeasure']={}\n",
    "        eval_metrics['knn']['purity']={}\n",
    "        eval_metrics['knn']['auc']={}    \n",
    "\n",
    "        eval_metrics['ocsvm']={}\n",
    "        eval_metrics['ocsvm']['fpr']={}\n",
    "        eval_metrics['ocsvm']['tpr']={}\n",
    "        eval_metrics['ocsvm']['thresholds']={}\n",
    "        eval_metrics['ocsvm']['fp']={}\n",
    "        eval_metrics['ocsvm']['tp']={}\n",
    "        eval_metrics['ocsvm']['fn']={}\n",
    "        eval_metrics['ocsvm']['tn']={}\n",
    "        eval_metrics['ocsvm']['recall']={}\n",
    "        eval_metrics['ocsvm']['specificity']={}\n",
    "        eval_metrics['ocsvm']['precision']={}\n",
    "        eval_metrics['ocsvm']['accuracy']={}\n",
    "        eval_metrics['ocsvm']['fmeasure']={}\n",
    "        eval_metrics['ocsvm']['purity']={}\n",
    "        eval_metrics['ocsvm']['auc']={}    \n",
    "\n",
    "\n",
    "    # # try lof\n",
    "    for nn in range(10,90,10):\n",
    "        # fit the model for outlier detection (default)\n",
    "        clf = LocalOutlierFactor(n_neighbors=nn, contamination=outliers_fraction)\n",
    "        # use fit_predict to compute the predicted labels of the training samples\n",
    "        # (when LOF is used for outlier detection, the estimator has no predict,\n",
    "        # decision_function and score_samples methods).\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        n_errors = (y_pred != y).sum()\n",
    "        pred = clf.negative_outlier_factor_ \n",
    "\n",
    "        preds=np.zeros(len(np.array(pred)))\n",
    "        preds[np.argsort(-np.array(pred))[0:int(sum(y))]]=1\n",
    "        tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "        recall=tp/(tp+fn)\n",
    "        specificity=tn/(tn+fp)\n",
    "        precision=tp/(tp+fp)\n",
    "        accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "        fmeasure=2*precision*recall/(precision + recall)\n",
    "        purity=purity_score(y, preds)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "\n",
    "        eval_preds['LOF']=preds\n",
    "        eval_metrics['LOF']['fpr'][nn]=fpr\n",
    "        eval_metrics['LOF']['tpr'][nn]=tpr\n",
    "        eval_metrics['LOF']['thresholds'][nn]=thresholds\n",
    "        eval_metrics['LOF']['fp'][nn]=fp\n",
    "        eval_metrics['LOF']['tp'][nn]=tp\n",
    "        eval_metrics['LOF']['fn'][nn]=fn\n",
    "        eval_metrics['LOF']['tn'][nn]=tn\n",
    "        eval_metrics['LOF']['recall'][nn]=recall\n",
    "        eval_metrics['LOF']['specificity'][nn]=specificity\n",
    "        eval_metrics['LOF']['precision'][nn]=precision\n",
    "        eval_metrics['LOF']['accuracy'][nn]=accuracy\n",
    "        eval_metrics['LOF']['fmeasure'][nn]=fmeasure\n",
    "        eval_metrics['LOF']['purity'][nn]=purity\n",
    "        eval_metrics['LOF']['auc'][nn]=auc\n",
    "\n",
    "    # # Kmeans--\n",
    "    ks = [1,3,5,7,9,11,21]\n",
    "    for i in range(len(ks)):   \n",
    "        if ks[i]<len(X):\n",
    "            linds, C, c = kmeans__(X,ks[i],int(np.sum(labels)))\n",
    "            preds = np.zeros([labels.shape[0],])\n",
    "            preds[linds] = 1\n",
    "            tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "            recall=tp/(tp+fn)\n",
    "            specificity=tn/(tn+fp)\n",
    "            precision=tp/(tp+fp)\n",
    "            accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "            fmeasure=2*precision*recall/(precision + recall)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y, preds)\n",
    "            purity=purity_score(y, preds)\n",
    "            eval_preds['Kmeans-- '+str(ks[i])]=preds\n",
    "            eval_metrics['Kmeans--']['fpr'][ks[i]]=fpr\n",
    "            eval_metrics['Kmeans--']['tpr'][ks[i]]=tpr\n",
    "            eval_metrics['Kmeans--']['thresholds'][ks[i]]=thresholds\n",
    "            eval_metrics['Kmeans--']['fp'][ks[i]]=fp\n",
    "            eval_metrics['Kmeans--']['tp'][ks[i]]=tp\n",
    "            eval_metrics['Kmeans--']['fn'][ks[i]]=fn\n",
    "            eval_metrics['Kmeans--']['tn'][ks[i]]=tn\n",
    "            eval_metrics['Kmeans--']['recall'][ks[i]]=recall\n",
    "            eval_metrics['Kmeans--']['specificity'][ks[i]]=specificity\n",
    "            eval_metrics['Kmeans--']['precision'][ks[i]]=precision\n",
    "            eval_metrics['Kmeans--']['accuracy'][ks[i]]=accuracy\n",
    "            eval_metrics['Kmeans--']['fmeasure'][ks[i]]=fmeasure\n",
    "            eval_metrics['Kmeans--']['purity'][ks[i]]=purity\n",
    "            eval_metrics['Kmeans--']['auc'][ks[i]]=auc\n",
    "\n",
    "    # # KNN\n",
    "    ks = [1,3,5,7,9,11,21]\n",
    "    for i in range(len(ks)):    \n",
    "        #try knn\n",
    "        knn = KNeighborsClassifier(n_neighbors=ks[i])\n",
    "        knn.fit(X[train], y[train])\n",
    "        preds=knn.predict(X)\n",
    "        pred = preds\n",
    "        preds=np.zeros(len(np.array(pred)))\n",
    "        preds[np.argsort(-np.array(pred))[0:int(sum(y))]]=1\n",
    "        tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "        recall=tp/(tp+fn)\n",
    "        specificity=tn/(tn+fp)\n",
    "        precision=tp/(tp+fp)\n",
    "        accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "        fmeasure=2*precision*recall/(precision + recall)\n",
    "        purity=purity_score(y, preds)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "        eval_preds['knn '+str(ks[i])]=preds\n",
    "        eval_metrics['knn']['fpr'][ks[i]]=fpr\n",
    "        eval_metrics['knn']['tpr'][ks[i]]=tpr\n",
    "        eval_metrics['knn']['thresholds'][ks[i]]=thresholds\n",
    "        eval_metrics['knn']['fp'][ks[i]]=fp\n",
    "        eval_metrics['knn']['tp'][ks[i]]=tp\n",
    "        eval_metrics['knn']['fn'][ks[i]]=fn\n",
    "        eval_metrics['knn']['tn'][ks[i]]=tn\n",
    "        eval_metrics['knn']['recall'][ks[i]]=recall\n",
    "        eval_metrics['knn']['specificity'][ks[i]]=specificity\n",
    "        eval_metrics['knn']['precision'][ks[i]]=precision\n",
    "        eval_metrics['knn']['accuracy'][ks[i]]=accuracy\n",
    "        eval_metrics['knn']['fmeasure'][ks[i]]=fmeasure\n",
    "        eval_metrics['knn']['purity'][ks[i]]=purity\n",
    "        eval_metrics['knn']['auc'][ks[i]]=auc\n",
    "\n",
    "#     # try ocsvm\n",
    "    for gmma in np.arange(0.05,1,0.05):\n",
    "        ##print(gmma)\n",
    "#             oc = ocsvm(nu=nu,gamma=gmma)\n",
    "        oc=svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\",gamma=gmma)\n",
    "        oc.fit(X[train])\n",
    "        p = oc.predict(X)\n",
    "        preds = np.zeros(p.shape)\n",
    "        preds[p == -1] = 1\n",
    "        preds[p == 1] = 0\n",
    "        tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "        recall=tp/(tp+fn)\n",
    "        specificity=tn/(tn+fp)\n",
    "        precision=tp/(tp+fp)\n",
    "        accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "        fmeasure=2*precision*recall/(precision + recall)\n",
    "        purity=purity_score(y, preds)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, preds)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "\n",
    "        eval_preds['ocsvm '+str(gmma)]=preds\n",
    "        eval_metrics['ocsvm']['fpr'][gmma]=fpr\n",
    "        eval_metrics['ocsvm']['tpr'][gmma]=tpr\n",
    "        eval_metrics['ocsvm']['thresholds'][gmma]=thresholds\n",
    "        eval_metrics['ocsvm']['fp'][gmma]=fp\n",
    "        eval_metrics['ocsvm']['tp'][gmma]=tp\n",
    "        eval_metrics['ocsvm']['fn'][gmma]=fn\n",
    "        eval_metrics['ocsvm']['tn'][gmma]=tn\n",
    "        eval_metrics['ocsvm']['recall'][gmma]=recall\n",
    "        eval_metrics['ocsvm']['specificity'][gmma]=specificity\n",
    "        eval_metrics['ocsvm']['precision'][gmma]=precision\n",
    "        eval_metrics['ocsvm']['accuracy'][gmma]=accuracy\n",
    "        eval_metrics['ocsvm']['fmeasure'][gmma]=fmeasure\n",
    "        eval_metrics['ocsvm']['purity'][gmma]=purity\n",
    "        eval_metrics['ocsvm']['auc'][gmma]=auc\n",
    "\n",
    "    return eval_metrics,eval_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.859302Z",
     "start_time": "2020-05-20T04:31:46.832975Z"
    },
    "code_folding": [
     0,
     19,
     33,
     52,
     80,
     82,
     91,
     121,
     129,
     143,
     150
    ]
   },
   "outputs": [],
   "source": [
    "def plotClusters(thetas,z,samples):\n",
    "    thetameans = []\n",
    "    K = len(thetas)\n",
    "    plt.figure(figsize=(fig_len,fig_wid))\n",
    "    ax=plt.gca()\n",
    "    ax.set_facecolor('white')\n",
    "    ax.tick_params(labelsize=25)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.grid(color='k', linestyle='-.', linewidth=0.3)\n",
    "\n",
    "    for k in range(K):\n",
    "        thetameans.append(thetas[k][0])\n",
    "    thetameans = np.array(thetameans)\n",
    "    for k in range(K):\n",
    "        plt.scatter(samples[z == k,0],samples[z == k,1],marker='*',s=m_size)\n",
    "    plt.legend([str(k) for k in range(K)])\n",
    "    #plt.scatter(thetameans[:,0],thetameans[:,1],marker='x')\n",
    "    for k in range(K):\n",
    "        plt.text(thetameans[k,0],thetameans[k,1],str(k))\n",
    "def multivariatet(mu,Sigma,N,M):\n",
    "    '''\n",
    "    Output:\n",
    "    Produce M samples of d-dimensional multivariate t distribution\n",
    "    Input:\n",
    "    mu = mean (d dimensional numpy array or scalar)\n",
    "    Sigma = scale matrix (dxd numpy array)\n",
    "    N = degrees of freedom\n",
    "    M = # of samples to produce\n",
    "    '''\n",
    "    d = len(Sigma)\n",
    "    g = np.tile(np.random.gamma(N/2.,2./N,M),(d,1)).T\n",
    "    Z = np.random.multivariate_normal(np.zeros(d),Sigma,M)\n",
    "    return mu + Z/np.sqrt(g)\n",
    "def normalinvwishartsample(params):\n",
    "    '''\n",
    "    Generate sample from a Normal Inverse Wishart distribution\n",
    "\n",
    "    Inputs:\n",
    "    params - Parameters for the NIW distribution \n",
    "        mu    - Mean parameter: n x 1 numpy array\n",
    "        W     - Precision parameter: d x d numpy array\n",
    "        kappa - Scalar parameter for normal distribution covariance matrix\n",
    "        nu    - Scalar parameter for Wishart distribution\n",
    "\n",
    "    Output:\n",
    "    Sample - Sample mean vector, mu_s and Sample covariance matrix, W_s\n",
    "    '''\n",
    "    mu,W,kappa,nu = params\n",
    "    # first sample W from a Inverse Wishart distribution\n",
    "    W_s = invwishart(df=nu, scale=W).rvs()\n",
    "    mu_s = np.random.multivariate_normal(mu.flatten(),W_s/kappa,1) \n",
    "    return np.transpose(mu_s),W_s\n",
    "def normalinvwishartmarginal(X,params):\n",
    "    '''\n",
    "    Marginal likelihood of dataset X using a Normal Inverse Wishart prior\n",
    "\n",
    "    Inputs:\n",
    "    X      - Dataset matrix: n x d numpy array\n",
    "    params - Parameters for the NIW distribution \n",
    "        mu    - Mean parameter: n x 1 numpy array\n",
    "        W     - Precision parameter: d x d numpy array\n",
    "        kappa - Scalar parameter for normal distribution covariance matrix\n",
    "        nu    - Scalar parameter for Wishart distribution\n",
    "\n",
    "    Output:\n",
    "    Marginal likelihood of X - scalar\n",
    "    '''\n",
    "    mu,W,kappa,nu = params\n",
    "    mu=X\n",
    "\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    nu_n = nu + n\n",
    "    kappa_n = kappa + n\n",
    "    X_mean = np.mean(X,axis=0)\n",
    "    X_mean = X_mean[:,np.newaxis]\n",
    "    S = scatter(X)\n",
    "    W_n = W + S + ((kappa*n)/(kappa+n))*np.dot(mu - X_mean,np.transpose(mu - X_mean))\n",
    "    #(1/np.power(np.pi,n*d*0.5))*(gamma(nu_n*0.5)/gamma(nu*0.5))*(np.power(np.linalg.det(W),nu*0.5)/np.power(np.linalg.det(W_n),nu_n*0.5))*np.power(kappa/kappa_n,0.5*d)\n",
    "    return (1/np.power(np.pi,n*d*0.5))*(gamma(nu_n*0.5)/gamma(nu*0.5))*(np.power(np.linalg.det(W)/np.linalg.det(W_n),nu*0.5)/np.power(np.linalg.det(W_n),(nu_n-nu)*0.5))*np.power(kappa/kappa_n,0.5*d)\n",
    "def scatter(x):\n",
    "    return np.dot(np.transpose(x - np.mean(x,0)),x - np.mean(x,0))\n",
    "def plotAnomalies(I,samples):\n",
    "    for k in range(2):\n",
    "        plt.figure(figsize=(fig_len,fig_wid))\n",
    "        ax=plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "        ax.tick_params(labelsize=20)\n",
    "        ax.set_facecolor('white')\n",
    "        ax.grid(color='k', linestyle='-.', linewidth=0.3)\n",
    "        plt.scatter(samples[I == k,0],samples[I == k,1],marker='*',s=m_size)\n",
    "def kmeans__(data,k,l,maxiters=100,eps=0.0001):\n",
    "\n",
    "    # select k cluster centers\n",
    "    C = data[np.random.permutation(range(data.shape[0]))[0:k],:]\n",
    "    objVal = 0\n",
    "    for jj in range(maxiters):\n",
    "        # compute distance of each point to the clusters\n",
    "        dMat = pdist2(data,C)\n",
    "        d = np.min(dMat,axis=1).flatten()\n",
    "        c = np.argmin(dMat,axis=1).flatten()\n",
    "        # sort points by distance to their closest center\n",
    "        inds = np.argsort(d)[::-1]\n",
    "        linds = inds[0:l]\n",
    "        cinds = inds[l+1:]\n",
    "        # extract the non-outlier data objects\n",
    "        ci = c[cinds]\n",
    "        # recompute the means\n",
    "        for kk in range(k):\n",
    "            C[kk,:] = np.mean(data[np.where(ci == kk)[0],:],axis=0)\n",
    "        # compute objective function\n",
    "        objVal_ = objVal\n",
    "        objVal = 0\n",
    "        for kk in range(k):\n",
    "            objVal += np.sum(pdist2(data[np.where(ci == kk)[0],:],C[kk,:]))\n",
    "        if np.abs(objVal - objVal_) < eps:\n",
    "            break\n",
    "    # one final time\n",
    "    dMat = pdist2(data,C)\n",
    "    c = np.argmin(dMat,axis=1).flatten()\n",
    "    return linds, C, c\n",
    "def pdist2(X,C):\n",
    "    if len(C.shape) == 1:\n",
    "        C = C[:,np.newaxis]\n",
    "    distMat = np.zeros([X.shape[0],C.shape[0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(C.shape[0]):\n",
    "            distMat[i,j] = np.linalg.norm(X[i,:] - C[j,:])\n",
    "    return distMat\n",
    "def precAtK(true,predicted):\n",
    "    # find number of anomalies\n",
    "    k = np.sum(true)\n",
    "#     print(\"k=\",k)\n",
    "    # find the score of the k^th predicted anomaly\n",
    "    v = np.sort(predicted,axis=0)[::-1][k-1]\n",
    "#     print(\"v=\",v)\n",
    "    # find all objects that are above the threshold\n",
    "    inds = np.where(predicted >= v)[0]\n",
    "#     print(\"inds=\",inds)\n",
    "#     print(\"np.sum(true[inds])=\",np.sum(true[inds]))\n",
    "#     print(\"len(inds)=\",len(inds))\n",
    "#     print(\"np.sum(true[inds])/len(inds)=\",np.sum(true[inds])/len(inds))\n",
    "    return float(np.sum(true[inds]))/float(len(inds))\n",
    "def averageRank(true,predicted):\n",
    "    inds = np.where(true == 1)[0]\n",
    "    s = np.argsort(predicted)[::-1]\n",
    "    v = []\n",
    "    for ind in inds:\n",
    "        v.append(float(np.where(s == ind)[0]+1))\n",
    "    return np.mean(v)\n",
    "def purity_score(y_true, y_pred):\n",
    "    \"\"\"Purity score\n",
    "\n",
    "    To compute purity, each cluster is assigned to the class which is most frequent \n",
    "    in the cluster [1], and then the accuracy of this assignment is measured by counting \n",
    "    the number of correctly assigned documents and dividing by the number of documents.\n",
    "    We suppose here that the ground truth labels are integers, the same with the predicted clusters i.e\n",
    "    the clusters index.\n",
    "\n",
    "    Args:\n",
    "        y_true(np.ndarray): n*1 matrix Ground truth labels\n",
    "        y_pred(np.ndarray): n*1 matrix Predicted clusters\n",
    "    \n",
    "    Returns:\n",
    "        float: Purity score\n",
    "    \n",
    "    References:\n",
    "        [1] https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html\n",
    "    \"\"\"\n",
    "    # matrix which will hold the majority-voted labels\n",
    "    y_voted_labels = np.zeros(y_true.shape)\n",
    "    # Ordering labels\n",
    "    ## Labels might be missing e.g with set like 0,2 where 1 is missing\n",
    "    ## First find the unique labels, then map the labels to an ordered set\n",
    "    ## 0,2 should become 0,1\n",
    "    labels = np.unique(y_true)\n",
    "    ordered_labels = np.arange(labels.shape[0])\n",
    "    for k in range(labels.shape[0]):\n",
    "        y_true[y_true==labels[k]] = ordered_labels[k]\n",
    "    # Update unique labels\n",
    "    labels = np.unique(y_true)\n",
    "    # We set the number of bins to be n_classes+2 so that \n",
    "    # we count the actual occurence of classes between two consecutive bin\n",
    "    # the bigger being excluded [bin_i, bin_i+1[\n",
    "    bins = np.concatenate((labels, [np.max(labels)+1]), axis=0)\n",
    "\n",
    "    for cluster in np.unique(y_pred):\n",
    "        hist, _ = np.histogram(y_true[y_pred==cluster], bins=bins)\n",
    "        # Find the most present label in the cluster\n",
    "        winner = np.argmax(hist)\n",
    "        y_voted_labels[y_pred==cluster] = winner\n",
    "    \n",
    "    return accuracy_score(y_true, y_voted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.936119Z",
     "start_time": "2020-05-20T04:31:46.881996Z"
    },
    "code_folding": [
     0,
     47,
     92,
     106,
     140,
     167,
     188,
     204,
     211,
     247,
     264,
     270,
     291,
     320,
     341,
     367
    ]
   },
   "outputs": [],
   "source": [
    "def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n",
    "    \"\"\"Estimate the log Gaussian probability.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "    precisions_chol : array-like\n",
    "        Cholesky decompositions of the precision matrices.\n",
    "        'full' : shape of (n_components, n_features, n_features)\n",
    "        'tied' : shape of (n_features, n_features)\n",
    "        'diag' : shape of (n_components, n_features)\n",
    "        'spherical' : shape of (n_components,)\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "    Returns\n",
    "    -------\n",
    "    log_prob : array, shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components, _ = means.shape\n",
    "    # det(precision_chol) is half of det(precision)\n",
    "    log_det = _compute_log_det_cholesky(\n",
    "        precisions_chol, covariance_type, n_features)\n",
    "\n",
    "    if covariance_type == 'full':\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n",
    "            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == 'tied':\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, mu in enumerate(means):\n",
    "            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == 'diag':\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (np.sum((means ** 2 * precisions), 1) -\n",
    "                    2. * np.dot(X, (means * precisions).T) +\n",
    "                    np.dot(X ** 2, precisions.T))\n",
    "\n",
    "    elif covariance_type == 'spherical':\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (np.sum(means ** 2, 1) * precisions -\n",
    "                    2 * np.dot(X, means.T * precisions) +\n",
    "                    np.outer(row_norms(X, squared=True), precisions))\n",
    "    return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n",
    "def _compute_precision_cholesky(covariances, covariance_type):\n",
    "    \"\"\"Compute the Cholesky decomposition of the precisions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    covariances : array-like\n",
    "        The covariance matrix of the current components.\n",
    "        The shape depends of the covariance_type.\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        The type of precision matrices.\n",
    "    Returns\n",
    "    -------\n",
    "    precisions_cholesky : array-like\n",
    "        The cholesky decomposition of sample precisions of the current\n",
    "        components. The shape depends of the covariance_type.\n",
    "    \"\"\"\n",
    "    estimate_precision_error_message = (\n",
    "        \"Fitting the mixture model failed because some components have \"\n",
    "        \"ill-defined empirical covariance (for instance caused by singleton \"\n",
    "        \"or collapsed samples). Try to decrease the number of components, \"\n",
    "        \"or increase reg_covar.\")\n",
    "\n",
    "    if covariance_type == 'full':\n",
    "        n_components, n_features, _ = covariances.shape\n",
    "        precisions_chol = np.empty((n_components, n_features, n_features))\n",
    "        for k, covariance in enumerate(covariances):\n",
    "            try:\n",
    "                cov_chol = linalg.cholesky(covariance, lower=True)\n",
    "            except linalg.LinAlgError:\n",
    "                raise ValueError(estimate_precision_error_message)\n",
    "            precisions_chol[k] = linalg.solve_triangular(cov_chol,\n",
    "                                                         np.eye(n_features),\n",
    "                                                         lower=True).T\n",
    "    elif covariance_type == 'tied':\n",
    "        _, n_features = covariances.shape\n",
    "        try:\n",
    "            cov_chol = linalg.cholesky(covariances, lower=True)\n",
    "        except linalg.LinAlgError:\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),\n",
    "                                                  lower=True).T\n",
    "    else:\n",
    "        if np.any(np.less_equal(covariances, 0.0)):\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = 1. / np.sqrt(covariances)\n",
    "    return precisions_chol\n",
    "def _estimate_log_prob(means_,precisions_cholesky_,covariance_type,degrees_of_freedom_,mean_precision_, X):\n",
    "        _, n_features = X.shape\n",
    "        # We remove `n_features * np.log(degrees_of_freedom_)` because\n",
    "        # the precision matrix is normalized\n",
    "        log_gauss = (_estimate_log_gaussian_prob(\n",
    "            X, means_, precisions_cholesky_, covariance_type) -\n",
    "            .5 * n_features * np.log(degrees_of_freedom_))\n",
    "\n",
    "        log_lambda = n_features * np.log(2.) + np.sum(digamma(\n",
    "            .5 * (degrees_of_freedom_ -\n",
    "                  np.arange(0, n_features)[:, np.newaxis])), 0)\n",
    "\n",
    "        return log_gauss + .5 * (log_lambda -\n",
    "                                 n_features / mean_precision_)\n",
    "def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n",
    "    \"\"\"Compute the log-det of the cholesky decomposition of matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_chol : array-like\n",
    "        Cholesky decompositions of the matrices.\n",
    "        'full' : shape of (n_components, n_features, n_features)\n",
    "        'tied' : shape of (n_features, n_features)\n",
    "        'diag' : shape of (n_components, n_features)\n",
    "        'spherical' : shape of (n_components,)\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    log_det_precision_chol : array-like, shape (n_components,)\n",
    "        The determinant of the precision matrix for each component.\n",
    "    \"\"\"\n",
    "    if covariance_type == 'full':\n",
    "        n_components, _, _ = matrix_chol.shape\n",
    "        log_det_chol = (np.sum(np.log(\n",
    "            matrix_chol.reshape(\n",
    "                n_components, -1)[:, ::n_features + 1]), 1))\n",
    "\n",
    "    elif covariance_type == 'tied':\n",
    "        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))\n",
    "\n",
    "    elif covariance_type == 'diag':\n",
    "        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))\n",
    "\n",
    "    else:\n",
    "        log_det_chol = n_features * (np.log(matrix_chol))\n",
    "\n",
    "    return log_det_chol\n",
    "def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):\n",
    "    \"\"\"Estimate the Gaussian distribution parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input data array.\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "        The responsibilities for each data sample in X.\n",
    "    reg_covar : float\n",
    "        The regularization added to the diagonal of the covariance matrices.\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        The type of precision matrices.\n",
    "    Returns\n",
    "    -------\n",
    "    nk : array-like, shape (n_components,)\n",
    "        The numbers of data samples in the current components.\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "        The centers of the current components.\n",
    "    covariances : array-like\n",
    "        The covariance matrix of the current components.\n",
    "        The shape depends of the covariance_type.\n",
    "    \"\"\"\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    covariances = {\"full\": _estimate_gaussian_covariances_full                   \n",
    "                  }[covariance_type](resp, X, nk, means, reg_covar)\n",
    "    return nk, means, covariances\n",
    "def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):\n",
    "    \"\"\"Estimate the full covariance matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "    nk : array-like, shape (n_components,)\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "    reg_covar : float\n",
    "    Returns\n",
    "    -------\n",
    "    covariances : array, shape (n_components, n_features, n_features)\n",
    "        The covariance matrix of the current components.\n",
    "    \"\"\"\n",
    "    n_components, n_features = means.shape\n",
    "    covariances = np.empty((n_components, n_features, n_features))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n",
    "        covariances[k].flat[::n_features + 1] += reg_covar\n",
    "    return covariances\n",
    "def initialization(X,K,numiters,r0,alpha=1):\n",
    "    ########################### Data preprocessing\n",
    "    X,z,thetas,N,params,D=preprocess(X,K)\n",
    "\n",
    "    clusters, sizes = np.unique(z, return_counts=True)\n",
    "    m_para=sizes/N\n",
    "    F=np.zeros(N)\n",
    "#     params=tuple((np.array(pd.DataFrame(X).mean()),((np.array(pd.DataFrame(X).cov()))), 1, D))\n",
    "    \n",
    "#     m_para,F=F_est(np.ones,N,N,thetas,params,X)\n",
    "    \n",
    "    threshold=0.3\n",
    "    \n",
    "    I=(np.random.binomial(1, threshold, N))\n",
    "        \n",
    "    return X,z,I,thetas,N,params,D,clusters,sizes,m_para,F,threshold\n",
    "def convergence_check(thetas,centroids_old,conv_criteria):\n",
    "    centroids=np.copy(np.array(list([thetas[i][0] for i in range(len(thetas))])))\n",
    "    if len(centroids)<len(centroids_old):\n",
    "        change=np.sum(list(np.min(np.abs(np.linalg.norm(centroids_old-centroids[i],axis=1))) for i in range(len(centroids))))\n",
    "    else:\n",
    "        change=np.sum(list(np.min(np.abs(np.linalg.norm(centroids_old[i]-centroids,axis=1))) for i in range(len(centroids_old))))\n",
    "    return change\n",
    "def preprocess(X,K):\n",
    "    \n",
    "# Try different mean precision prior ie params[2] : No difference\n",
    "# mean_precision_prior float | None, optional.\n",
    "# The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. \n",
    "# Larger values concentrate the cluster means around mean_prior. The value of the parameter must be greater \n",
    "# than 0. If it is None, it is set to 1.\n",
    "\n",
    "# Try different reg_covar : Too volatile\n",
    "    if type(X) == list:\n",
    "        X = np.array(X)\n",
    "    if len(X.shape) == 1:\n",
    "        X = X[:,np.newaxis]\n",
    "    N = X.shape[0] #rows: observations\n",
    "    D = X.shape[1] #columns: dimensions\n",
    "\n",
    "    # Fit your data on the scaler object\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "#     X=normalize(X)\n",
    "    # Initialize z\n",
    "#     z=np.random.randint(K,size=N)\n",
    "    z=KMeans(K).fit(X).predict(X)\n",
    "    z+=1\n",
    "\n",
    "    if D>1:\n",
    "        params = tuple((np.mean(X,axis=0),(np.cov(X.T)), 1, D))\n",
    "    elif D==1:\n",
    "        params = tuple((np.mean(X),(np.var(X.T)),1, D))\n",
    "\n",
    "    z=np.random.randint(K,size=N)\n",
    "\n",
    "    thetas=[normalinvwishartsample(params) for k in range(K)]\n",
    "\n",
    "    return X,z,thetas,N,params,D\n",
    "def remove_cluster_new(X,z,K,thetas,params):\n",
    "#     if len(thetas)>len(np.unique(np.abs(z))):\n",
    "#         print(len(thetas)-len(np.unique(np.abs(z))),\" clusters removed\", len(np.unique(np.abs(z))) )\n",
    "    N=len(z)\n",
    "    z_temp=np.copy(z)\n",
    "    clusters, sizes = np.unique(np.abs(z_temp), return_counts=True)\n",
    "\n",
    "    c2=pd.DataFrame(clusters).copy()\n",
    "    temp=c2.index.copy()+1\n",
    "    c2.index=c2[0].copy()\n",
    "    c2[0]=temp.copy()\n",
    "    z=np.multiply(np.copy(c2[0][np.abs(z_temp)]),np.sign(z_temp+0.5))\n",
    "    \n",
    "    clusters, sizes = np.unique(np.abs(z), return_counts=True)\n",
    "    K=len(clusters)\n",
    "    \n",
    "    return z,K,thetas\n",
    "def compute_mixture_pdf(means_,precisions_cholesky_,covariance_type,mean_precision_, X,N,sizes):\n",
    "    degrees_of_freedom_=sizes+X.shape[1]\n",
    "    log_probs=_estimate_log_prob(means_,precisions_cholesky_,covariance_type,degrees_of_freedom_,mean_precision_, X)\n",
    "    MN=(np.exp(log_probs))\n",
    "    F=np.dot(sizes/N,MN.T)\n",
    "    return degrees_of_freedom_,log_probs,MN,F\n",
    "def compute_cluster_params(z,X,params,clusters,sizes,ind_matrix,reg_covar,covariance_type):\n",
    "    K=(len(clusters))\n",
    "    N=len(z)\n",
    "    thetas=[]\n",
    "    for k in (clusters-1):\n",
    "        ind_k=np.where((z) == (k+1))[0]\n",
    "        c = len(ind_k)\n",
    "        if c<1:\n",
    "#             print(\"Group anomaly\")\n",
    "            ind_k=np.where(np.abs(z) == (k+1))[0]\n",
    "            c = len(ind_k)\n",
    "        thetas.append((_estimate_gaussian_parameters(X[ind_k], \n",
    "                                            np.ones((c,1)), reg_covar, covariance_type)[1:3]))    \n",
    "    nk=sizes\n",
    "    means_=np.array([thetas[k][0].T for k in clusters-1])[:,:,0]\n",
    "    covariances=np.array([thetas[k][1][0,:,:] for k in clusters-1])\n",
    "    para_tuple=nk,means_,covariances\n",
    "    precisions_cholesky_= np.array([_compute_precision_cholesky(cov, \n",
    "                                                covariance_type) for cov in [covariances]])[0,:,:,:]\n",
    "    \n",
    "    return para_tuple,thetas,nk,means_,covariances,precisions_cholesky_\n",
    "def update_anomaly_labels(N,F,u,K,z,threshold,ppsa):\n",
    "    prob=1-ppsa\n",
    "    I=np.array(list(np.random.binomial(1,prob[i],1)[0] for i in range(N)))\n",
    "    z=(np.abs(z)*np.power(-1,I))\n",
    "    I[z<0]=1\n",
    "    for k in range(K):\n",
    "        ind_k=((z) == k)\n",
    "        c=sum(ind_k)\n",
    "        if (c<0.1*N) or ((c>0) and (np.mean(I[ind_k])>=0.75)):\n",
    "            I[ind_k]=1\n",
    "            z[ind_k]=np.abs(z[ind_k])*(-1)\n",
    "    #     else:\n",
    "    #         I[ind_k]=np.zeros(c)\n",
    "    #         z[ind_k]=np.abs(z[ind_k])\n",
    "\n",
    "    if np.mean(I)>0.3:\n",
    "        I[np.argsort(F)[np.int(0.3*N):]]=0\n",
    "        z[np.argsort(F)[np.int(0.3*N):]]=np.abs(z[np.argsort(F)[np.int(0.3*N):]])\n",
    "        \n",
    "#     if np.sum(z<0)<(0.01*N):\n",
    "    if np.sum(z<0)==0:\n",
    "        min_ana_count=max(np.int(0.01*N),10)\n",
    "        I[np.argsort(F)[:min_ana_count]]=1\n",
    "        z[np.argsort(F)[:min_ana_count]]=np.abs(z[np.argsort(F)[:min_ana_count]])*(-1)\n",
    "#         print(\"min_ana_count\",min_ana_count, \"sum(I)\", sum(I))\n",
    "\n",
    "    threshold=min(max(beta.ppf(threshold , sum(I)+1,N-sum(I)+1),4/N,0.01), 0.3)\n",
    "\n",
    "    return prob,I,threshold,z \n",
    "def _log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features):\n",
    "    \"\"\"Compute the log of the Wishart distribution normalization term.\n",
    "    Parameters\n",
    "    ----------\n",
    "    degrees_of_freedom : array-like, shape (n_components,)\n",
    "        The number of degrees of freedom on the covariance Wishart\n",
    "        distributions.\n",
    "    log_det_precision_chol : array-like, shape (n_components,)\n",
    "         The determinant of the precision matrix for each component.\n",
    "    n_features : int\n",
    "        The number of features.\n",
    "    Return\n",
    "    ------\n",
    "    log_wishart_norm : array-like, shape (n_components,)\n",
    "        The log normalization of the Wishart distribution.\n",
    "    \"\"\"\n",
    "    # To simplify the computation we have removed the np.log(np.pi) term\n",
    "    return -(degrees_of_freedom * log_det_precisions_chol +\n",
    "             degrees_of_freedom * n_features * .5 * math.log(2.) +\n",
    "             np.sum(gammaln(.5 * (degrees_of_freedom -\n",
    "                                  np.arange(n_features)[:, np.newaxis])), 0))\n",
    "def compute_log_likelihhod(z,sizes,K,precisions_cholesky_,covariance_type,features,degrees_of_freedom_,\n",
    "                           mean_precision_):\n",
    "        # Contrary to the original formula, we have done some simplification\n",
    "        # and removed all the constant terms.\n",
    "        log_resp=np.log(np.abs(z))\n",
    "        weight_concentration_ = (\n",
    "                1. + sizes,\n",
    "                (1/K +\n",
    "                 np.hstack((np.cumsum(sizes[::-1])[-2::-1], 0))))\n",
    "\n",
    "        # We removed `.5 * features * np.log(degrees_of_freedom_)`\n",
    "        # because the precision matrix is normalized.\n",
    "        log_det_precisions_chol = (_compute_log_det_cholesky(\n",
    "            precisions_cholesky_, covariance_type, features) -\n",
    "            .5 * features * np.log(degrees_of_freedom_))\n",
    "\n",
    "        log_wishart = np.sum(_log_wishart_norm(\n",
    "            degrees_of_freedom_, log_det_precisions_chol, features))\n",
    "        \n",
    "        log_norm_weight = -np.sum(betaln(weight_concentration_[0],\n",
    "                                         weight_concentration_[1]))\n",
    "\n",
    "        curr_log_likelihood=(-np.sum(np.exp(log_resp) * log_resp) -\n",
    "                log_wishart - log_norm_weight -\n",
    "                0.5 * features * np.sum(np.log(mean_precision_)))\n",
    "        return curr_log_likelihood\n",
    "def ppsa_vals(F,I,threshold):\n",
    "    N=len(F)\n",
    "    u=np.unique(F)\n",
    "    ps1=F\n",
    "    domain=((u>np.quantile(F,0.01))*1==(u<np.quantile(F,0.3))*1)\n",
    "    G_Y_domain=u*domain\n",
    "    G_Y_domain=(G_Y_domain[G_Y_domain>0])\n",
    "\n",
    "    G_Y=domain*[np.sum(F[(F<=u)]) for n,u in enumerate(np.unique(F))]\n",
    "    G_Y=G_Y[G_Y>0]\n",
    "    G_Y=np.array(G_Y/max(G_Y))\n",
    "    \n",
    "    g_Y=np.diff(G_Y)/np.diff(G_Y_domain)\n",
    "\n",
    "    aa=np.array(list(stats.percentileofscore(F, i)/100 for i in np.unique(F)))\n",
    "    th3=aa[aa<0.3][np.argmax(np.abs(np.diff(np.quantile(G_Y,aa[aa<0.3]))))]\n",
    "#     th3=threshold\n",
    "    len_tail=np.int(th3*N) #drop in F\n",
    "\n",
    "    u=np.quantile(ps1, th3)\n",
    "    inds = np.where(ps1<=u)[0]\n",
    "\n",
    "    iz=np.union1d(inds, np.where(I==1)[0])\n",
    "#     inds0=iz[np.argsort(ps1[iz])[:min(np.int(0.3*N),len(iz),np.int(th3*N))]]\n",
    "    inds0=np.argsort(ps1)[:min(np.int(0.3*N),len(iz),np.int(th3*N))]\n",
    "    psa = np.abs(ps1[inds0] - u) \n",
    "\n",
    "    gpdparams = stats.genpareto.fit(psa)\n",
    "    i_ppsa=np.zeros(N)\n",
    "    \n",
    "    i_ppsa[inds0] = stats.genpareto(1,0,gpdparams[2]).cdf(psa)\n",
    "    ppsa=np.ones(N)\n",
    "    ppsa[inds0]=1-(i_ppsa[inds0])\n",
    "    \n",
    "#     i_ppsa[iz] = stats.genpareto(1,0,gpdparams[2]).cdf(np.abs(ps1[iz] - u))\n",
    "#     ppsa=np.ones(N)\n",
    "#     ppsa[iz]=1-(i_ppsa[iz])\n",
    "    \n",
    "    ppsa[ppsa>1]=1\n",
    "    ppsa[ppsa==0]=sys.float_info.min\n",
    "    return ppsa,i_ppsa, inds, inds0,u,F, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.992016Z",
     "start_time": "2020-05-20T04:31:46.955901Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def incad_new_labels5(X,K,numiters,r0,conv_criteria):\n",
    "# if 1:\n",
    "    # initialization\n",
    "    start_time=time.time()\n",
    "    \n",
    "    output={}\n",
    "\n",
    "    log_likelihood=[]\n",
    "    converged_=0\n",
    "    covariance_type=\"full\"\n",
    "    \n",
    "    X,z,I,thetas,N,params,features,clusters,sizes,m_para,F,threshold = initialization(X,K,numiters,r0,alpha=1)\n",
    "    \n",
    "    DistanceMatrix = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(X, 'euclidean'))\n",
    "    \n",
    "    mean_precision_=params[2]\n",
    "    \n",
    "    reg_covar=np.power(np.unique(DistanceMatrix)[1],1)/2\n",
    "\n",
    "    niw_mat=np.array(list(normalinvwishartmarginal(X[i:i+1],\n",
    "                         tuple((np.mean(X[i:i+1],axis=0),params[1],params[2],params[3]))\n",
    "                                                  ) for i in range(N)))\n",
    "\n",
    "    clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "    \n",
    "    ind_matrix=np.array(list(1*(np.abs(z).astype('int')==c) for c in clusters.astype('int'))) # all points  \n",
    "    \n",
    "    para_tuple,thetas,nk,means_,covariances,precisions_cholesky_= compute_cluster_params(z,X,\n",
    "                                                    params,clusters,sizes,ind_matrix,reg_covar,covariance_type)\n",
    "    \n",
    "    degrees_of_freedom_,log_probs,MN,F = compute_mixture_pdf(means_,precisions_cholesky_,\n",
    "                                         covariance_type,mean_precision_, X,N,sizes)\n",
    "    \n",
    "    if r0:\n",
    "        ppsa,i_ppsa, inds, inds0,u,F, threshold = ppsa_vals(F,I,threshold)\n",
    "        alpha2=1/(ppsa**r0)\n",
    "        prob,I,threshold,z = update_anomaly_labels(N,F,u,K,z,threshold,ppsa)\n",
    "        \n",
    "    else:\n",
    "        alpha2=np.ones(N)\n",
    "        prob=np.zeros(N)\n",
    "        ppsa=i_ppsa=inds=inds0=[]\n",
    "        u=0\n",
    "        threshold=0\n",
    "        \n",
    "    init_time=time.time()\n",
    "\n",
    "    for n in range(numiters):\n",
    "        sys.stdout.write('*'); sys.stdout.flush()\n",
    "#         print(sizes)\n",
    "\n",
    "        ps_new_clust=((alpha2/(N + alpha2 - 1))*niw_mat)[:,np.newaxis]\n",
    "\n",
    "        ps_log=np.array([np.hstack(((np.log((sizes-(clusters==k0))[:,np.newaxis])-np.log(N+alpha2-1)).T+log_probs,\n",
    "                    np.log(ps_new_clust))) for k0 in clusters])\n",
    "        \n",
    "        z=np.array(list(((1+np.argmax(np.random.multinomial(1, \n",
    "                (np.exp(ps_log[np.int(np.abs(z[i])-1),i,:])+sys.float_info.min)/np.sum(\n",
    "                    np.exp(ps_log[np.int(np.abs(z[i])-1),i,:])+sys.float_info.min), \n",
    "                            size=1)))*np.power(-1,np.random.binomial(1,prob[i],1)[0])) for i in range(N)))\n",
    "        \n",
    "        # Update labels : drop empty clusters \n",
    "        z,K,thetas = remove_cluster_new(X,z,K,thetas,params)\n",
    "\n",
    "        clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "        \n",
    "        ind_matrix=np.array(list(1*(np.abs(z).astype('int')==c) for c in clusters.astype('int'))) # all points  \n",
    "\n",
    "        para_tuple,thetas,nk,means_,covariances,precisions_cholesky_= compute_cluster_params(z,X,\n",
    "                                                        params,clusters,sizes,ind_matrix,reg_covar,covariance_type)\n",
    "\n",
    "        degrees_of_freedom_,log_probs,MN,F = compute_mixture_pdf(means_,precisions_cholesky_,\n",
    "                                             covariance_type,mean_precision_, X,N,sizes)\n",
    "\n",
    "        if r0:\n",
    "            ppsa,i_ppsa, inds, inds0,u,F, threshold = ppsa_vals(F,I,threshold)\n",
    "            alpha2=1/(ppsa**r0)\n",
    "            prob,I,threshold,z = update_anomaly_labels(N,F,u,K,z,threshold,ppsa)\n",
    "            \n",
    "        \n",
    "        log_likelihood.append(compute_log_likelihhod(z,sizes,K,precisions_cholesky_,covariance_type,features,degrees_of_freedom_,\n",
    "                           mean_precision_))\n",
    "        \n",
    "        if n > 50 and (np.max(np.abs(np.diff(log_likelihood[-3:])))<conv_criteria):\n",
    "            converged_ += 1\n",
    "            if converged_>0:\n",
    "                print(\"Converged\")\n",
    "                break\n",
    "                \n",
    "    batch_time=time.time()\n",
    "            \n",
    "    output={}\n",
    "    output['n']=n\n",
    "    output['X']=np.copy(X)\n",
    "    output['time_lapsed_init_ms']=(init_time-start_time)*1000.0\n",
    "    output['time_lapsed_stream_ms']=(time.time()-batch_time)*1000.0\n",
    "    output['time_lapsed_batch_ms']=(batch_time-init_time)*1000.0\n",
    "    output['z']=np.copy(z)\n",
    "    output['u']=np.copy(u)\n",
    "    output['K']=copy.copy(K)\n",
    "    output['r0']=copy.copy(r0)\n",
    "    output['F']=np.copy(F)\n",
    "    output['I']=np.copy(I)\n",
    "    output['prob']=np.copy(prob)\n",
    "    output['alpha2']=np.copy(alpha2)\n",
    "    output['thetas']=thetas[:]\n",
    "    output['log_likelihood']=copy.copy(log_likelihood)\n",
    "    output['threshold']=copy.copy(threshold)\n",
    "    output['ppsa']=np.copy(ppsa)\n",
    "    output['i_ppsa']=np.copy(i_ppsa)\n",
    "    output['inds']=np.copy(inds)\n",
    "    output['inds0']=np.copy(inds0) \n",
    "    output['converged_']=converged_\n",
    "    output['niw_mat']=np.copy(niw_mat)\n",
    "    output['DistanceMatrix']=np.copy(DistanceMatrix)\n",
    "    output['reg_covar']=np.copy(reg_covar)\n",
    "    output['ind_matrix']=np.copy(ind_matrix)\n",
    "    output['para_tuple']=copy.copy(para_tuple)\n",
    "    output['means_']=np.copy(means_)\n",
    "    output['covariances']=np.copy(covariances)\n",
    "    output['precisions_cholesky_']=np.copy(precisions_cholesky_)\n",
    "    output['degrees_of_freedom_']=np.copy(degrees_of_freedom_)\n",
    "    output['log_probs']=np.copy(log_probs)\n",
    "    output['MN']=np.copy(MN)\n",
    "    output['params']=np.copy(params)\n",
    "    \n",
    "    return output           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:47.061456Z",
     "start_time": "2020-05-20T04:31:47.030617Z"
    },
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "def incad_new_labels_stream(X_full,K,batch_proportion,numiters,r0,conv_criteria):\n",
    "# if 1:\n",
    "    if 1:\n",
    "        start_time=time.time()\n",
    "        output={}\n",
    "\n",
    "        log_likelihood=[]\n",
    "        converged_=0\n",
    "        covariance_type=\"full\"\n",
    "\n",
    "        batch_size=np.int(X_full.shape[0]*batch_proportion)\n",
    "\n",
    "        N_full,D=X_full.shape\n",
    "\n",
    "        N=copy.copy(batch_size)\n",
    "\n",
    "        X=X_full[:batch_size]\n",
    "        \n",
    "        batch_output=incad_new_labels5(X,K,numiters,r0,conv_criteria)\n",
    "        \n",
    "        n=batch_output['n']\n",
    "        X=batch_output['X']\n",
    "        time_lapsed_init_ms=batch_output['time_lapsed_init_ms']\n",
    "        time_lapsed_stream_ms=batch_output['time_lapsed_stream_ms']\n",
    "        time_lapsed_batch_ms=batch_output['time_lapsed_batch_ms']\n",
    "        z=batch_output['z']\n",
    "        u=batch_output['u']\n",
    "        K=batch_output['K']\n",
    "        r0=batch_output['r0']\n",
    "        F=batch_output['F']\n",
    "        I=batch_output['I']\n",
    "        prob=batch_output['prob']\n",
    "        alpha2=batch_output['alpha2']\n",
    "        thetas=batch_output['thetas']\n",
    "        log_likelihood=batch_output['log_likelihood']\n",
    "        threshold=batch_output['threshold']\n",
    "        ppsa=batch_output['ppsa']\n",
    "        i_ppsa=batch_output['i_ppsa']\n",
    "        inds=batch_output['inds']\n",
    "        inds0=batch_output['inds0'] \n",
    "        converged_=batch_output['converged_']\n",
    "        niw_mat=batch_output['niw_mat']\n",
    "        DistanceMatrix=batch_output['DistanceMatrix']\n",
    "        reg_covar=batch_output['reg_covar']\n",
    "        ind_matrix=batch_output['ind_matrix']\n",
    "        para_tuple=batch_output['para_tuple']\n",
    "        means_=batch_output['means_']\n",
    "        covariances=batch_output['covariances']\n",
    "        precisions_cholesky_=batch_output['precisions_cholesky_']\n",
    "        degrees_of_freedom_=batch_output['degrees_of_freedom_']\n",
    "        log_probs=batch_output['log_probs']\n",
    "        MN=batch_output['MN']\n",
    "        params=batch_output['params']\n",
    "\n",
    "\n",
    "        mean_precision_=params[2]\n",
    "        \n",
    "        clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "\n",
    "        batch_time=time.time()\n",
    "    \n",
    "    print(\"Begin Streaming\")\n",
    "    \n",
    "    for i in np.arange(batch_size,N_full,1):\n",
    "        sys.stdout.write('*'); sys.stdout.flush()\n",
    "#         if i%100==0:\n",
    "#             print(sizes)\n",
    "        \n",
    "        # Need full obs info        \n",
    "        X=X_full[:i+1]\n",
    "        \n",
    "        if type(X) == list:\n",
    "            X = np.array(X)\n",
    "        if len(X.shape) == 1:\n",
    "            X = X[:,np.newaxis]\n",
    "\n",
    "        # Fit your data on the scaler object\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        if i%(0.1*N)==0:\n",
    "            if D>1:\n",
    "                params = tuple((np.mean(X,axis=0),(np.cov(X.T)), 1, D))\n",
    "            elif D==1:\n",
    "                params = tuple((np.mean(X),(np.var(X.T)),1, D))\n",
    "            DistanceMatrix = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(X[-np.int(0.1*N+1):,:],\n",
    "                                                                                            'euclidean'))\n",
    "            mean_precision_=params[2]\n",
    "            reg_covar=np.min([reg_covar,np.power(np.unique(DistanceMatrix)[1],1)/2])\n",
    "\n",
    "        z=np.append(z,K+1)\n",
    "        I=np.append(I,0)\n",
    "        prob=np.append(prob,0)\n",
    "        N=N+1\n",
    "        niw_mat=np.append(niw_mat,normalinvwishartmarginal(X[-1][:,np.newaxis],\n",
    "                                        tuple((X[-1],params[1],params[2],params[3]))))\n",
    "\n",
    "        clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "\n",
    "        ind_matrix=np.array(list(1*(np.abs(z).astype('int')==c) for c in clusters.astype('int'))) # all points  \n",
    "\n",
    "        para_tuple,thetas,nk,means_,covariances,precisions_cholesky_= compute_cluster_params(z,X,\n",
    "                                                        params,clusters,sizes,ind_matrix,reg_covar,covariance_type)\n",
    "\n",
    "        degrees_of_freedom_,log_probs,MN,F = compute_mixture_pdf(means_,precisions_cholesky_,\n",
    "                                             covariance_type,mean_precision_, X,N,sizes)\n",
    "        \n",
    "        \n",
    "        if r0:\n",
    "            ppsa,i_ppsa, inds, inds0,u,F, threshold = ppsa_vals(F,I,threshold)\n",
    "            alpha2=1/(ppsa**r0)\n",
    "            prob,I,threshold,z = update_anomaly_labels(N,F,u,K,z,threshold,ppsa)\n",
    "        \n",
    "        \n",
    "        # Update only tail points\n",
    "        inds_update=np.union1d(inds0,np.where(I==1))\n",
    "        inds_update=np.union1d(inds_update,i)\n",
    "        \n",
    "\n",
    "        ps_new_clust=((alpha2[inds_update]/(N + alpha2[inds_update] - 1))*niw_mat[inds_update])[:,np.newaxis]\n",
    "\n",
    "        ps_log=np.array([np.hstack(((np.log((sizes-(clusters==k0))[:,np.newaxis])-np.log(N+alpha2[inds_update]-1)).T\n",
    "                                    +log_probs[inds_update],\n",
    "                    np.log(ps_new_clust))) for k0 in clusters])\n",
    "        \n",
    "        ps_log+=sys.float_info.min\n",
    "        \n",
    "        z[inds_update]=np.array(list(((1+np.argmax(np.random.multinomial(1, \n",
    "                (np.exp(ps_log[np.int(np.abs(z[i])-1),ii,:])+sys.float_info.min)/np.sum(\n",
    "                    np.exp(ps_log[np.int(np.abs(z[i])-1),ii,:])+sys.float_info.min), \n",
    "                            size=1)))*np.power(-1,np.random.binomial(1,prob[i],1)[0])) \n",
    "                                      for ii,i in enumerate(inds_update)))\n",
    "        \n",
    "        # Update labels : drop empty clusters \n",
    "        z,K,thetas = remove_cluster_new(X,z,K,thetas,params)\n",
    "\n",
    "    clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "\n",
    "    ind_matrix=np.array(list(1*(np.abs(z).astype('int')==c) for c in clusters.astype('int'))) # all points  \n",
    "\n",
    "    para_tuple,thetas,nk,means_,covariances,precisions_cholesky_= compute_cluster_params(z,X,\n",
    "                                                    params,clusters,sizes,ind_matrix,reg_covar,covariance_type)\n",
    "\n",
    "    degrees_of_freedom_,log_probs,MN,F = compute_mixture_pdf(means_,precisions_cholesky_,\n",
    "                                         covariance_type,mean_precision_, X,N,sizes)\n",
    "\n",
    "    output={}\n",
    "    output['n']=n\n",
    "    output['X']=np.copy(X)\n",
    "    output['time_lapsed_init_ms']=time_lapsed_init_ms\n",
    "    output['time_lapsed_stream_ms']=(time.time()-batch_time)*1000.0\n",
    "    output['time_lapsed_batch_ms']=time_lapsed_batch_ms\n",
    "    output['z']=np.copy(z)\n",
    "    output['u']=np.copy(u)\n",
    "    output['K']=copy.copy(K)\n",
    "    output['r0']=copy.copy(r0)\n",
    "    output['F']=np.copy(F)\n",
    "    output['I']=np.copy(I)\n",
    "    output['prob']=np.copy(prob)\n",
    "    output['alpha2']=np.copy(alpha2)\n",
    "    output['thetas']=thetas[:]\n",
    "    output['log_likelihood']=copy.copy(log_likelihood)\n",
    "    output['threshold']=copy.copy(threshold)\n",
    "    output['ppsa']=np.copy(ppsa)\n",
    "    output['i_ppsa']=np.copy(i_ppsa)\n",
    "    output['inds']=np.copy(inds)\n",
    "    output['inds0']=np.copy(inds0) \n",
    "    output['converged_']=converged_\n",
    "    output['niw_mat']=np.copy(niw_mat)\n",
    "    output['DistanceMatrix']=np.copy(DistanceMatrix)\n",
    "    output['reg_covar']=np.copy(reg_covar)\n",
    "    output['ind_matrix']=np.copy(ind_matrix)\n",
    "    output['para_tuple']=copy.copy(para_tuple)\n",
    "    output['means_']=np.copy(means_)\n",
    "    output['covariances']=np.copy(covariances)\n",
    "    output['precisions_cholesky_']=np.copy(precisions_cholesky_)\n",
    "    output['degrees_of_freedom_']=np.copy(degrees_of_freedom_)\n",
    "    output['log_probs']=np.copy(log_probs)\n",
    "    output['MN']=np.copy(MN)\n",
    "    output['params']=np.copy(params)\n",
    "    \n",
    "    return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
