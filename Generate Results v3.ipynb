{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.414842Z",
     "start_time": "2020-05-20T04:31:42.428915Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# # Import Libraries\n",
    "# import matplotlib\n",
    "# matplotlib.use('agg')\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
    "%load_ext line_profiler\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import time, random               # add some random sleep time\n",
    "import scipy\n",
    "import glob\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "import re\n",
    "import itertools\n",
    "import six\n",
    "import zipfile\n",
    "import shutil\n",
    "import h5py\n",
    "from scipy.io import arff\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "from spot import SPOT\n",
    "from os.path import isfile, join\n",
    "from statsutils import *\n",
    "from boltons.statsutils import *\n",
    "from datetime import datetime\n",
    "from itertools import repeat\n",
    "from ipywidgets import interact\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils import check_random_state, shuffle\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.svm import OneClassSVM as ocsvm\n",
    "from sklearn import metrics, mixture,svm\n",
    "from sklearn.neighbors import KNeighborsClassifier,LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "from scipy import linalg\n",
    "from scipy.special import gamma, factorial, digamma,betaln, gammaln\n",
    "from scipy.stats import beta, multivariate_normal, wishart,invwishart,t, mode\n",
    "from scipy.stats import genextreme as gev\n",
    "import scipy.spatial as sp\n",
    "import scipy.io\n",
    "from scipy.io import arff\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib as mpl\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=2)\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.720215Z",
     "start_time": "2020-05-20T04:31:46.443142Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# SPOT Algorithm\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Dec 12 10:08:16 2016\n",
    "\n",
    "@author: Alban Siffer \n",
    "@company: Amossys\n",
    "@license: GNU GPLv3\n",
    "\"\"\"\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from math import log\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# colors for plot\n",
    "deep_saffron = '#FF9933'\n",
    "air_force_blue = '#5D8AA8'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "================================= MAIN CLASS ==================================\n",
    "\"\"\"\n",
    "\n",
    "class SPOT:\n",
    "    \"\"\"\n",
    "    This class allows to run SPOT algorithm on univariate dataset (upper-bound)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    proba : float\n",
    "        Detection level (risk), chosen by the user\n",
    "        \n",
    "    extreme_quantile : float\n",
    "        current threshold (bound between normal and abnormal events)\n",
    "        \n",
    "    data : numpy.array\n",
    "        stream\n",
    "    \n",
    "    init_data : numpy.array\n",
    "        initial batch of observations (for the calibration/initialization step)\n",
    "    \n",
    "    init_threshold : float\n",
    "        initial threshold computed during the calibration step\n",
    "    \n",
    "    peaks : numpy.array\n",
    "        array of peaks (excesses above the initial threshold)\n",
    "    \n",
    "    n : int\n",
    "        number of observed values\n",
    "    \n",
    "    Nt : int\n",
    "        number of observed peaks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, q = 1e-4):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "\t    Parameters\n",
    "\t    ----------\n",
    "\t    q\n",
    "\t\t    Detection level (risk)\n",
    "\t\n",
    "\t    Returns\n",
    "\t    ----------\n",
    "    \tSPOT object\n",
    "        \"\"\"\n",
    "        self.proba = q\n",
    "        self.extreme_quantile = None\n",
    "        self.data = None\n",
    "        self.init_data = None\n",
    "        self.init_threshold = None\n",
    "        self.peaks = None\n",
    "        self.n = 0\n",
    "        self.Nt = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        s += 'Streaming Peaks-Over-Threshold Object\\n'\n",
    "        s += 'Detection level q = %s\\n' % self.proba\n",
    "        if self.data is not None:\n",
    "            s += 'Data imported : Yes\\n'\n",
    "            s += '\\t initialization  : %s values\\n' % self.init_data.size\n",
    "            s += '\\t stream : %s values\\n' % self.data.size\n",
    "        else:\n",
    "            s += 'Data imported : No\\n'\n",
    "            return s\n",
    "            \n",
    "        if self.n == 0:\n",
    "            s += 'Algorithm initialized : No\\n'\n",
    "        else:\n",
    "            s += 'Algorithm initialized : Yes\\n'\n",
    "            s += '\\t initial threshold : %s\\n' % self.init_threshold\n",
    "            \n",
    "            r = self.n-self.init_data.size\n",
    "            if r > 0:\n",
    "                s += 'Algorithm run : Yes\\n'\n",
    "                s += '\\t number of observations : %s (%.2f %%)\\n' % (r,100*r/self.n)\n",
    "            else:\n",
    "                s += '\\t number of peaks  : %s\\n' % self.Nt\n",
    "                s += '\\t extreme quantile : %s\\n' % self.extreme_quantile\n",
    "                s += 'Algorithm run : No\\n'\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def fit(self,init_data,data):\n",
    "        \"\"\"\n",
    "        Import data to SPOT object\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    init_data : list, numpy.array or pandas.Series\n",
    "\t\t    initial batch to calibrate the algorithm\n",
    "            \n",
    "        data : numpy.array\n",
    "\t\t    data for the run (list, np.array or pd.series)\n",
    "\t\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            self.data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            self.data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "            \n",
    "        if isinstance(init_data,list):\n",
    "            self.init_data = np.array(init_data)\n",
    "        elif isinstance(init_data,np.ndarray):\n",
    "            self.init_data = init_data\n",
    "        elif isinstance(init_data,pd.Series):\n",
    "            self.init_data = init_data.values\n",
    "        elif isinstance(init_data,int):\n",
    "            self.init_data = self.data[:init_data]\n",
    "            self.data = self.data[init_data:]\n",
    "        elif isinstance(init_data,float) & (init_data<1) & (init_data>0):\n",
    "            r = int(init_data*data.size)\n",
    "            self.init_data = self.data[:r]\n",
    "            self.data = self.data[r:]\n",
    "        else:\n",
    "            print('The initial data cannot be set')\n",
    "            return\n",
    "        \n",
    "    def add(self,data):\n",
    "        \"\"\"\n",
    "        This function allows to append data to the already fitted data\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    data : list, numpy.array, pandas.Series\n",
    "\t\t    data to append\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "        \n",
    "        self.data = np.append(self.data,data)\n",
    "        return\n",
    "    \n",
    "    def initialize(self, verbose = True):\n",
    "        \"\"\"\n",
    "        Run the calibration (initialization) step\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    verbose : bool\n",
    "\t\t    (default = True) If True, gives details about the batch initialization\n",
    "        \"\"\"\n",
    "        n_init = self.init_data.size\n",
    "        \n",
    "        S = np.sort(self.init_data)     # we sort X to get the empirical quantile\n",
    "        self.init_threshold = S[int(0.98*n_init)] # t is fixed for the whole algorithm\n",
    "\n",
    "        # initial peaks\n",
    "        self.peaks = self.init_data[self.init_data>self.init_threshold]-self.init_threshold \n",
    "        self.Nt = self.peaks.size\n",
    "        self.n = n_init\n",
    "        \n",
    "        if verbose:\n",
    "            print('Initial threshold : %s' % self.init_threshold)\n",
    "            print('Number of peaks : %s' % self.Nt)\n",
    "            print('Grimshaw maximum log-likelihood estimation ... ', end = '')\n",
    "            \n",
    "        g,s,l = self._grimshaw()\n",
    "        self.extreme_quantile = self._quantile(g,s)\n",
    "        \n",
    "        if verbose:\n",
    "            print('[done]')\n",
    "            print('\\t'+chr(0x03B3) + ' = ' + str(g))\n",
    "            print('\\t'+chr(0x03C3) + ' = ' + str(s))\n",
    "            print('\\tL = ' + str(l))\n",
    "            print('Extreme quantile (probability = %s): %s' % (self.proba,self.extreme_quantile))\n",
    "        \n",
    "        return \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _rootsFinder(fun,jac,bounds,npoints,method):\n",
    "        \"\"\"\n",
    "        Find possible roots of a scalar function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fun : function\n",
    "\t\t    scalar function \n",
    "        jac : function\n",
    "            first order derivative of the function  \n",
    "        bounds : tuple\n",
    "            (min,max) interval for the roots search    \n",
    "        npoints : int\n",
    "            maximum number of roots to output      \n",
    "        method : str\n",
    "            'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.array\n",
    "            possible roots of the function\n",
    "        \"\"\"\n",
    "        if method == 'regular':\n",
    "            step = (bounds[1]-bounds[0])/(npoints+1)\n",
    "            X0 = np.arange(bounds[0]+step,bounds[1],step)\n",
    "        elif method == 'random':\n",
    "            X0 = np.random.uniform(bounds[0],bounds[1],npoints)\n",
    "        \n",
    "        def objFun(X,f,jac):\n",
    "            g = 0\n",
    "            j = np.zeros(X.shape)\n",
    "            i = 0\n",
    "            for x in X:\n",
    "                fx = f(x)\n",
    "                g = g+fx**2\n",
    "                j[i] = 2*fx*jac(x)\n",
    "                i = i+1\n",
    "            return g,j\n",
    "        \n",
    "        opt = minimize(lambda X:objFun(X,fun,jac), X0, \n",
    "                       method='L-BFGS-B', \n",
    "                       jac=True, bounds=[bounds]*len(X0))\n",
    "        \n",
    "        X = opt.x\n",
    "        np.round(X,decimals = 5)\n",
    "        return np.unique(X)\n",
    "    \n",
    "    \n",
    "    def _log_likelihood(Y,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : numpy.array\n",
    "\t\t    observations\n",
    "        gamma : float\n",
    "            GPD index parameter\n",
    "        sigma : float\n",
    "            GPD scale parameter (>0)   \n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        n = Y.size\n",
    "        if gamma != 0:\n",
    "            tau = gamma/sigma\n",
    "            L = -n * log(sigma) - ( 1 + (1/gamma) ) * ( np.log(1+tau*Y) ).sum()\n",
    "        else:\n",
    "            L = n * ( 1 + log(Y.mean()) )\n",
    "        return L\n",
    "\n",
    "\n",
    "    def _grimshaw(self,epsilon = 1e-8, n_points = 10):\n",
    "        \"\"\"\n",
    "        Compute the GPD parameters estimation with the Grimshaw's trick\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "\t\t    numerical parameter to perform (default : 1e-8)\n",
    "        n_points : int\n",
    "            maximum number of candidates for maximum likelihood (default : 10)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        gamma_best,sigma_best,ll_best\n",
    "            gamma estimates, sigma estimates and corresponding log-likelihood\n",
    "        \"\"\"\n",
    "        def u(s):\n",
    "            return 1 + np.log(s).mean()\n",
    "            \n",
    "        def v(s):\n",
    "            return np.mean(1/s)\n",
    "        \n",
    "        def w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            return us*vs-1\n",
    "        \n",
    "        def jac_w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            jac_us = (1/t)*(1-vs)\n",
    "            jac_vs = (1/t)*(-vs+np.mean(1/s**2))\n",
    "            return us*jac_vs+vs*jac_us\n",
    "            \n",
    "    \n",
    "        Ym = self.peaks.min()\n",
    "        YM = self.peaks.max()\n",
    "        Ymean = self.peaks.mean()\n",
    "        \n",
    "        \n",
    "        a = -1/YM\n",
    "        if abs(a)<2*epsilon:\n",
    "            epsilon = abs(a)/n_points\n",
    "        \n",
    "        a = a + epsilon\n",
    "        b = 2*(Ymean-Ym)/(Ymean*Ym)\n",
    "        c = 2*(Ymean-Ym)/(Ym**2)\n",
    "    \n",
    "        # We look for possible roots\n",
    "        left_zeros = SPOT._rootsFinder(lambda t: w(self.peaks,t),\n",
    "                                 lambda t: jac_w(self.peaks,t),\n",
    "                                 (a+epsilon,-epsilon),\n",
    "                                 n_points,'regular')\n",
    "        \n",
    "        right_zeros = SPOT._rootsFinder(lambda t: w(self.peaks,t),\n",
    "                                  lambda t: jac_w(self.peaks,t),\n",
    "                                  (b,c),\n",
    "                                  n_points,'regular')\n",
    "    \n",
    "        # all the possible roots\n",
    "        zeros = np.concatenate((left_zeros,right_zeros))\n",
    "        \n",
    "        # 0 is always a solution so we initialize with it\n",
    "        gamma_best = 0\n",
    "        sigma_best = Ymean\n",
    "        ll_best = SPOT._log_likelihood(self.peaks,gamma_best,sigma_best)\n",
    "        \n",
    "        # we look for better candidates\n",
    "        for z in zeros:\n",
    "            gamma = u(1+z*self.peaks)-1\n",
    "            sigma = gamma/z\n",
    "            ll = SPOT._log_likelihood(self.peaks,gamma,sigma)\n",
    "            if ll>ll_best:\n",
    "                gamma_best = gamma\n",
    "                sigma_best = sigma\n",
    "                ll_best = ll\n",
    "    \n",
    "        return gamma_best,sigma_best,ll_best\n",
    "\n",
    "    \n",
    "\n",
    "    def _quantile(self,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the quantile at level 1-q\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "\t\t    GPD parameter\n",
    "        sigma : float\n",
    "            GPD parameter\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            quantile at level 1-q for the GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        r = self.n * self.proba / self.Nt\n",
    "        if gamma != 0:\n",
    "            return self.init_threshold + (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "        else:\n",
    "            return self.init_threshold - sigma*log(r)\n",
    "\n",
    "        \n",
    "    def run(self, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Run SPOT on the stream\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If False, SPOT will adapt the threshold assuming \\\n",
    "            there is no abnormal values\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            keys : 'thresholds' and 'alarms'\n",
    "            \n",
    "            'thresholds' contains the extreme quantiles and 'alarms' contains \\\n",
    "            the indexes of the values which have triggered alarms\n",
    "            \n",
    "        \"\"\"\n",
    "        if (self.n>self.init_data.size):\n",
    "            print('Warning : the algorithm seems to have already been run, you \\\n",
    "            should initialize before running again')\n",
    "            return {}\n",
    "        \n",
    "        # list of the thresholds\n",
    "        th = []\n",
    "        alarm = []\n",
    "        # Loop over the stream\n",
    "        for i in tqdm.tqdm(range(self.data.size)):\n",
    "    \n",
    "            # If the observed value exceeds the current threshold (alarm case)\n",
    "            if self.data[i]>self.extreme_quantile:\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks = np.append(self.peaks,self.data[i]-self.init_threshold)\n",
    "                    self.Nt += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw()\n",
    "                    self.extreme_quantile = self._quantile(g,s)\n",
    "\n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif self.data[i]>self.init_threshold:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks = np.append(self.peaks,self.data[i]-self.init_threshold)\n",
    "                    self.Nt += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw()\n",
    "                    self.extreme_quantile = self._quantile(g,s)\n",
    "            else:\n",
    "                self.n += 1\n",
    "\n",
    "                \n",
    "            th.append(self.extreme_quantile) # thresholds record\n",
    "        \n",
    "        return {'thresholds' : th, 'alarms': alarm}\n",
    "    \n",
    "\n",
    "    def plot(self,run_results,with_alarm = True):\n",
    "        \"\"\"\n",
    "        Plot the results of given by the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        run_results : dict\n",
    "            results given by the 'run' method\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If True, alarms are plotted.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        list\n",
    "            list of the plots\n",
    "            \n",
    "        \"\"\"\n",
    "        x = range(self.data.size)\n",
    "        K = run_results.keys()\n",
    "        \n",
    "        ts_fig, = plt.plot(x,self.data,color=\"blue\")\n",
    "        fig = [ts_fig]\n",
    "        ax=plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        \n",
    "        if 'thresholds' in K:\n",
    "            th = run_results['thresholds']\n",
    "            th_fig, = plt.plot(x,th,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(th_fig)\n",
    "        \n",
    "        if with_alarm and ('alarms' in K):\n",
    "            alarm = run_results['alarms']\n",
    "            al_fig = plt.scatter(alarm,self.data[alarm],color='red')\n",
    "            fig.append(al_fig)\n",
    "            \n",
    "        plt.xlim((0,self.data.size))\n",
    "\n",
    "        \n",
    "        return fig\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "============================ UPPER & LOWER BOUNDS =============================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class biSPOT:\n",
    "    \"\"\"\n",
    "    This class allows to run biSPOT algorithm on univariate dataset (upper and lower bounds)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    proba : float\n",
    "        Detection level (risk), chosen by the user\n",
    "        \n",
    "    extreme_quantile : float\n",
    "        current threshold (bound between normal and abnormal events)\n",
    "        \n",
    "    data : numpy.array\n",
    "        stream\n",
    "    \n",
    "    init_data : numpy.array\n",
    "        initial batch of observations (for the calibration/initialization step)\n",
    "    \n",
    "    init_threshold : float\n",
    "        initial threshold computed during the calibration step\n",
    "    \n",
    "    peaks : numpy.array\n",
    "        array of peaks (excesses above the initial threshold)\n",
    "    \n",
    "    n : int\n",
    "        number of observed values\n",
    "    \n",
    "    Nt : int\n",
    "        number of observed peaks\n",
    "    \"\"\"\n",
    "    def __init__(self, q = 1e-4):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "\t    Parameters\n",
    "\t    ----------\n",
    "\t    q\n",
    "\t\t    Detection level (risk)\n",
    "\t\n",
    "\t    Returns\n",
    "\t    ----------\n",
    "        biSPOT object\n",
    "        \"\"\"\n",
    "        self.proba = q\n",
    "        self.data = None\n",
    "        self.init_data = None\n",
    "        self.n = 0\n",
    "        nonedict =  {'up':None,'down':None}\n",
    "        \n",
    "        self.extreme_quantile = dict.copy(nonedict)\n",
    "        self.init_threshold = dict.copy(nonedict)\n",
    "        self.peaks = dict.copy(nonedict)\n",
    "        self.gamma = dict.copy(nonedict)\n",
    "        self.sigma = dict.copy(nonedict)\n",
    "        self.Nt = {'up':0,'down':0}\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        s += 'Streaming Peaks-Over-Threshold Object\\n'\n",
    "        s += 'Detection level q = %s\\n' % self.proba\n",
    "        if self.data is not None:\n",
    "            s += 'Data imported : Yes\\n'\n",
    "            s += '\\t initialization  : %s values\\n' % self.init_data.size\n",
    "            s += '\\t stream : %s values\\n' % self.data.size\n",
    "        else:\n",
    "            s += 'Data imported : No\\n'\n",
    "            return s\n",
    "            \n",
    "        if self.n == 0:\n",
    "            s += 'Algorithm initialized : No\\n'\n",
    "        else:\n",
    "            s += 'Algorithm initialized : Yes\\n'\n",
    "            s += '\\t initial threshold : %s\\n' % self.init_threshold\n",
    "            \n",
    "            r = self.n-self.init_data.size\n",
    "            if r > 0:\n",
    "                s += 'Algorithm run : Yes\\n'\n",
    "                s += '\\t number of observations : %s (%.2f %%)\\n' % (r,100*r/self.n)\n",
    "                s += '\\t triggered alarms : %s (%.2f %%)\\n' % (len(self.alarm),100*len(self.alarm)/self.n)\n",
    "            else:\n",
    "                s += '\\t number of peaks  : %s\\n' % self.Nt\n",
    "                s += '\\t upper extreme quantile : %s\\n' % self.extreme_quantile['up']\n",
    "                s += '\\t lower extreme quantile : %s\\n' % self.extreme_quantile['down']\n",
    "                s += 'Algorithm run : No\\n'\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def fit(self,init_data,data):\n",
    "        \"\"\"\n",
    "        Import data to biSPOT object\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    init_data : list, numpy.array or pandas.Series\n",
    "\t\t    initial batch to calibrate the algorithm ()\n",
    "            \n",
    "        data : numpy.array\n",
    "\t\t    data for the run (list, np.array or pd.series)\n",
    "\t\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            self.data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            self.data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "            \n",
    "        if isinstance(init_data,list):\n",
    "            self.init_data = np.array(init_data)\n",
    "        elif isinstance(init_data,np.ndarray):\n",
    "            self.init_data = init_data\n",
    "        elif isinstance(init_data,pd.Series):\n",
    "            self.init_data = init_data.values\n",
    "        elif isinstance(init_data,int):\n",
    "            self.init_data = self.data[:init_data]\n",
    "            self.data = self.data[init_data:]\n",
    "        elif isinstance(init_data,float) & (init_data<1) & (init_data>0):\n",
    "            r = int(init_data*data.size)\n",
    "            self.init_data = self.data[:r]\n",
    "            self.data = self.data[r:]\n",
    "        else:\n",
    "            print('The initial data cannot be set')\n",
    "            return\n",
    "        \n",
    "    def add(self,data):\n",
    "        \"\"\"\n",
    "        This function allows to append data to the already fitted data\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    data : list, numpy.array, pandas.Series\n",
    "\t\t    data to append\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "        \n",
    "        self.data = np.append(self.data,data)\n",
    "        return\n",
    "\n",
    "    def initialize(self, verbose = True):\n",
    "        \"\"\"\n",
    "        Run the calibration (initialization) step\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    verbose : bool\n",
    "\t\t    (default = True) If True, gives details about the batch initialization\n",
    "        \"\"\"\n",
    "        n_init = self.init_data.size\n",
    "        \n",
    "        S = np.sort(self.init_data)     # we sort X to get the empirical quantile\n",
    "        self.init_threshold['up'] = S[int(0.98*n_init)] # t is fixed for the whole algorithm\n",
    "        self.init_threshold['down'] = S[int(0.02*n_init)] # t is fixed for the whole algorithm\n",
    "\n",
    "        # initial peaks\n",
    "        self.peaks['up'] = self.init_data[self.init_data>self.init_threshold['up']]-self.init_threshold['up']\n",
    "        self.peaks['down'] = -(self.init_data[self.init_data<self.init_threshold['down']]-self.init_threshold['down'])\n",
    "        self.Nt['up'] = self.peaks['up'].size\n",
    "        self.Nt['down'] = self.peaks['down'].size\n",
    "        self.n = n_init\n",
    "        \n",
    "        if verbose:\n",
    "            print('Initial threshold : %s' % self.init_threshold)\n",
    "            print('Number of peaks : %s' % self.Nt)\n",
    "            print('Grimshaw maximum log-likelihood estimation ... ', end = '')\n",
    "            \n",
    "        l = {'up':None,'down':None}\n",
    "        for side in ['up','down']:\n",
    "            g,s,l[side] = self._grimshaw(side)\n",
    "            self.extreme_quantile[side] = self._quantile(side,g,s)\n",
    "            self.gamma[side] = g\n",
    "            self.sigma[side] = s\n",
    "        \n",
    "        ltab = 20\n",
    "        form = ('\\t'+'%20s' + '%20.2f' + '%20.2f')\n",
    "        if verbose:\n",
    "            print('[done]')\n",
    "            print('\\t' + 'Parameters'.rjust(ltab) + 'Upper'.rjust(ltab) + 'Lower'.rjust(ltab))\n",
    "            print('\\t' + '-'*ltab*3)\n",
    "            print(form % (chr(0x03B3),self.gamma['up'],self.gamma['down']))\n",
    "            print(form % (chr(0x03C3),self.sigma['up'],self.sigma['down']))\n",
    "            print(form % ('likelihood',l['up'],l['down']))\n",
    "            print(form % ('Extreme quantile',self.extreme_quantile['up'],self.extreme_quantile['down']))\n",
    "            print('\\t' + '-'*ltab*3)\n",
    "        return \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _rootsFinder(fun,jac,bounds,npoints,method):\n",
    "        \"\"\"\n",
    "        Find possible roots of a scalar function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fun : function\n",
    "\t\t    scalar function \n",
    "        jac : function\n",
    "            first order derivative of the function  \n",
    "        bounds : tuple\n",
    "            (min,max) interval for the roots search    \n",
    "        npoints : int\n",
    "            maximum number of roots to output      \n",
    "        method : str\n",
    "            'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.array\n",
    "            possible roots of the function\n",
    "        \"\"\"\n",
    "        if method == 'regular':\n",
    "            step = (bounds[1]-bounds[0])/(npoints+1)\n",
    "            X0 = np.arange(bounds[0]+step,bounds[1],step)\n",
    "        elif method == 'random':\n",
    "            X0 = np.random.uniform(bounds[0],bounds[1],npoints)\n",
    "        \n",
    "        def objFun(X,f,jac):\n",
    "            g = 0\n",
    "            j = np.zeros(X.shape)\n",
    "            i = 0\n",
    "            for x in X:\n",
    "                fx = f(x)\n",
    "                g = g+fx**2\n",
    "                j[i] = 2*fx*jac(x)\n",
    "                i = i+1\n",
    "            return g,j\n",
    "        opt = minimize(lambda X:objFun(X,fun,jac), X0, \n",
    "                       method='L-BFGS-B', \n",
    "                       jac=True, bounds=[bounds]*len(X0))\n",
    "        \n",
    "        X = opt.x\n",
    "        np.round(X,decimals = 5)\n",
    "        return np.unique(X)\n",
    "    \n",
    "    \n",
    "    def _log_likelihood(Y,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : numpy.array\n",
    "\t\t    observations\n",
    "        gamma : float\n",
    "            GPD index parameter\n",
    "        sigma : float\n",
    "            GPD scale parameter (>0)   \n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        n = Y.size\n",
    "        if gamma != 0:\n",
    "            tau = gamma/sigma\n",
    "            L = -n * log(sigma) - ( 1 + (1/gamma) ) * ( np.log(1+tau*Y) ).sum()\n",
    "        else:\n",
    "            L = n * ( 1 + log(Y.mean()) )\n",
    "        return L\n",
    "\n",
    "\n",
    "    def _grimshaw(self,side,epsilon = 1e-8, n_points = 10):\n",
    "        \"\"\"\n",
    "        Compute the GPD parameters estimation with the Grimshaw's trick\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "\t\t    numerical parameter to perform (default : 1e-8)\n",
    "        n_points : int\n",
    "            maximum number of candidates for maximum likelihood (default : 10)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        gamma_best,sigma_best,ll_best\n",
    "            gamma estimates, sigma estimates and corresponding log-likelihood\n",
    "        \"\"\"\n",
    "        def u(s):\n",
    "            return 1 + np.log(s).mean()\n",
    "            \n",
    "        def v(s):\n",
    "            return np.mean(1/s)\n",
    "        \n",
    "        def w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            return us*vs-1\n",
    "        \n",
    "        def jac_w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            jac_us = (1/t)*(1-vs)\n",
    "            jac_vs = (1/t)*(-vs+np.mean(1/s**2))\n",
    "            return us*jac_vs+vs*jac_us\n",
    "            \n",
    "    \n",
    "        Ym = self.peaks[side].min()\n",
    "        YM = self.peaks[side].max()\n",
    "        Ymean = self.peaks[side].mean()\n",
    "        \n",
    "        \n",
    "        a = -1/YM\n",
    "        if abs(a)<2*epsilon:\n",
    "            epsilon = abs(a)/n_points\n",
    "        \n",
    "        a = a + epsilon\n",
    "        b = 2*(Ymean-Ym)/(Ymean*Ym)\n",
    "        c = 2*(Ymean-Ym)/(Ym**2)\n",
    "    \n",
    "        # We look for possible roots\n",
    "        left_zeros = biSPOT._rootsFinder(lambda t: w(self.peaks[side],t),\n",
    "                                 lambda t: jac_w(self.peaks[side],t),\n",
    "                                 (a+epsilon,-epsilon),\n",
    "                                 n_points,'regular')\n",
    "        \n",
    "        right_zeros = biSPOT._rootsFinder(lambda t: w(self.peaks[side],t),\n",
    "                                  lambda t: jac_w(self.peaks[side],t),\n",
    "                                  (b,c),\n",
    "                                  n_points,'regular')\n",
    "    \n",
    "        # all the possible roots\n",
    "        zeros = np.concatenate((left_zeros,right_zeros))\n",
    "        \n",
    "        # 0 is always a solution so we initialize with it\n",
    "        gamma_best = 0\n",
    "        sigma_best = Ymean\n",
    "        ll_best = biSPOT._log_likelihood(self.peaks[side],gamma_best,sigma_best)\n",
    "        \n",
    "        # we look for better candidates\n",
    "        for z in zeros:\n",
    "            gamma = u(1+z*self.peaks[side])-1\n",
    "            sigma = gamma/z\n",
    "            ll = biSPOT._log_likelihood(self.peaks[side],gamma,sigma)\n",
    "            if ll>ll_best:\n",
    "                gamma_best = gamma\n",
    "                sigma_best = sigma\n",
    "                ll_best = ll\n",
    "    \n",
    "        return gamma_best,sigma_best,ll_best\n",
    "\n",
    "    \n",
    "\n",
    "    def _quantile(self,side,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the quantile at level 1-q for a given side\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        side : str\n",
    "            'up' or 'down'\n",
    "        gamma : float\n",
    "\t\t    GPD parameter\n",
    "        sigma : float\n",
    "            GPD parameter\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            quantile at level 1-q for the GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        if side == 'up':\n",
    "            r = self.n * self.proba / self.Nt[side]\n",
    "            if gamma != 0:\n",
    "                return self.init_threshold['up'] + (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "            else:\n",
    "                return self.init_threshold['up'] - sigma*log(r)\n",
    "        elif side == 'down':\n",
    "            r = self.n * self.proba / self.Nt[side]\n",
    "            if gamma != 0:\n",
    "                return self.init_threshold['down'] - (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "            else:\n",
    "                return self.init_threshold['down'] + sigma*log(r)\n",
    "        else:\n",
    "            print('error : the side is not right')\n",
    "\n",
    "        \n",
    "    def run(self, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Run biSPOT on the stream\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If False, SPOT will adapt the threshold assuming \\\n",
    "            there is no abnormal values\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            keys : 'upper_thresholds', 'lower_thresholds' and 'alarms'\n",
    "            \n",
    "            '***-thresholds' contains the extreme quantiles and 'alarms' contains \\\n",
    "            the indexes of the values which have triggered alarms\n",
    "            \n",
    "        \"\"\"\n",
    "        if (self.n>self.init_data.size):\n",
    "            print('Warning : the algorithm seems to have already been run, you \\\n",
    "            should initialize before running again')\n",
    "            return {}\n",
    "        \n",
    "        # list of the thresholds\n",
    "        thup = []\n",
    "        thdown = []\n",
    "        alarm = []\n",
    "        # Loop over the stream\n",
    "        for i in tqdm.tqdm(range(self.data.size)):\n",
    "    \n",
    "            # If the observed value exceeds the current threshold (alarm case)\n",
    "            if self.data[i]>self.extreme_quantile['up'] :\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks['up'] = np.append(self.peaks['up'],self.data[i]-self.init_threshold['up'])\n",
    "                    self.Nt['up'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('up')\n",
    "                    self.extreme_quantile['up'] = self._quantile('up',g,s)\n",
    "\n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif self.data[i]>self.init_threshold['up']:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks['up'] = np.append(self.peaks['up'],self.data[i]-self.init_threshold['up'])\n",
    "                    self.Nt['up'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('up')\n",
    "                    self.extreme_quantile['up'] = self._quantile('up',g,s)\n",
    "                    \n",
    "            elif self.data[i]<self.extreme_quantile['down'] :\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks['down'] = np.append(self.peaks['down'],-(self.data[i]-self.init_threshold['down']))\n",
    "                    self.Nt['down'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('down')\n",
    "                    self.extreme_quantile['down'] = self._quantile('down',g,s)\n",
    "\n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif self.data[i]<self.init_threshold['down']:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks['down'] = np.append(self.peaks['down'],-(self.data[i]-self.init_threshold['down']))\n",
    "                    self.Nt['down'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('down')\n",
    "                    self.extreme_quantile['down'] = self._quantile('down',g,s)\n",
    "            else:\n",
    "                self.n += 1\n",
    "\n",
    "                \n",
    "            thup.append(self.extreme_quantile['up']) # thresholds record\n",
    "            thdown.append(self.extreme_quantile['down']) # thresholds record\n",
    "        \n",
    "        return {'upper_thresholds' : thup,'lower_thresholds' : thdown, 'alarms': alarm}\n",
    "    \n",
    "    def plot(self,run_results,with_alarm = True):\n",
    "        \"\"\"\n",
    "        Plot the results of given by the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        run_results : dict\n",
    "            results given by the 'run' method\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If True, alarms are plotted.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        list\n",
    "            list of the plots\n",
    "            \n",
    "        \"\"\"\n",
    "        x = range(self.data.size)\n",
    "        K = run_results.keys()\n",
    "        \n",
    "        ts_fig, = plt.plot(x,self.data,color=\"blue\")\n",
    "        fig = [ts_fig]\n",
    "        ax=plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        if 'upper_thresholds' in K:\n",
    "            thup = run_results['upper_thresholds']\n",
    "            uth_fig, = plt.plot(x,thup,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(uth_fig)\n",
    "            \n",
    "        if 'lower_thresholds' in K:\n",
    "            thdown = run_results['lower_thresholds']\n",
    "            lth_fig, = plt.plot(x,thdown,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(lth_fig)\n",
    "        \n",
    "        if with_alarm and ('alarms' in K):\n",
    "            alarm = run_results['alarms']\n",
    "            al_fig = plt.scatter(alarm,self.data[alarm],color='red')\n",
    "            fig.append(al_fig)\n",
    "            \n",
    "        plt.xlim((0,self.data.size))\n",
    "\n",
    "        \n",
    "        return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "================================= WITH DRIFT ==================================\n",
    "\"\"\"\n",
    "\n",
    "def backMean(X,d):\n",
    "    M = []\n",
    "    w = X[:d].sum()\n",
    "    M.append(w/d)\n",
    "    for i in range(d,len(X)):\n",
    "        w = w - X[i-d] + X[i]\n",
    "        M.append(w/d)\n",
    "    return np.array(M)\n",
    "\n",
    "\n",
    "\n",
    "class dSPOT:\n",
    "    \"\"\"\n",
    "    This class allows to run DSPOT algorithm on univariate dataset (upper-bound)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    proba : float\n",
    "        Detection level (risk), chosen by the user\n",
    "        \n",
    "    depth : int\n",
    "        Number of observations to compute the moving average\n",
    "        \n",
    "    extreme_quantile : float\n",
    "        current threshold (bound between normal and abnormal events)\n",
    "        \n",
    "    data : numpy.array\n",
    "        stream\n",
    "    \n",
    "    init_data : numpy.array\n",
    "        initial batch of observations (for the calibration/initialization step)\n",
    "    \n",
    "    init_threshold : float\n",
    "        initial threshold computed during the calibration step\n",
    "    \n",
    "    peaks : numpy.array\n",
    "        array of peaks (excesses above the initial threshold)\n",
    "    \n",
    "    n : int\n",
    "        number of observed values\n",
    "    \n",
    "    Nt : int\n",
    "        number of observed peaks\n",
    "    \"\"\"\n",
    "    def __init__(self, q, depth):\n",
    "        self.proba = q\n",
    "        self.extreme_quantile = None\n",
    "        self.data = None\n",
    "        self.init_data = None\n",
    "        self.init_threshold = None\n",
    "        self.peaks = None\n",
    "        self.n = 0\n",
    "        self.Nt = 0\n",
    "        self.depth = depth\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        s += 'Streaming Peaks-Over-Threshold Object\\n'\n",
    "        s += 'Detection level q = %s\\n' % self.proba\n",
    "        if self.data is not None:\n",
    "            s += 'Data imported : Yes\\n'\n",
    "            s += '\\t initialization  : %s values\\n' % self.init_data.size\n",
    "            s += '\\t stream : %s values\\n' % self.data.size\n",
    "        else:\n",
    "            s += 'Data imported : No\\n'\n",
    "            return s\n",
    "            \n",
    "        if self.n == 0:\n",
    "            s += 'Algorithm initialized : No\\n'\n",
    "        else:\n",
    "            s += 'Algorithm initialized : Yes\\n'\n",
    "            s += '\\t initial threshold : %s\\n' % self.init_threshold\n",
    "            \n",
    "            r = self.n-self.init_data.size\n",
    "            if r > 0:\n",
    "                s += 'Algorithm run : Yes\\n'\n",
    "                s += '\\t number of observations : %s (%.2f %%)\\n' % (r,100*r/self.n)\n",
    "                s += '\\t triggered alarms : %s (%.2f %%)\\n' % (len(self.alarm),100*len(self.alarm)/self.n)\n",
    "            else:\n",
    "                s += '\\t number of peaks  : %s\\n' % self.Nt\n",
    "                s += '\\t extreme quantile : %s\\n' % self.extreme_quantile\n",
    "                s += 'Algorithm run : No\\n'\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def fit(self,init_data,data):\n",
    "        \"\"\"\n",
    "        Import data to DSPOT object\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    init_data : list, numpy.array or pandas.Series\n",
    "\t\t    initial batch to calibrate the algorithm\n",
    "            \n",
    "        data : numpy.array\n",
    "\t\t    data for the run (list, np.array or pd.series)\n",
    "\t\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            self.data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            self.data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "            \n",
    "        if isinstance(init_data,list):\n",
    "            self.init_data = np.array(init_data)\n",
    "        elif isinstance(init_data,np.ndarray):\n",
    "            self.init_data = init_data\n",
    "        elif isinstance(init_data,pd.Series):\n",
    "            self.init_data = init_data.values\n",
    "        elif isinstance(init_data,int):\n",
    "            self.init_data = self.data[:init_data]\n",
    "            self.data = self.data[init_data:]\n",
    "        elif isinstance(init_data,float) & (init_data<1) & (init_data>0):\n",
    "            r = int(init_data*data.size)\n",
    "            self.init_data = self.data[:r]\n",
    "            self.data = self.data[r:]\n",
    "        else:\n",
    "            print('The initial data cannot be set')\n",
    "            return\n",
    "        \n",
    "    def add(self,data):\n",
    "        \"\"\"\n",
    "        This function allows to append data to the already fitted data\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    data : list, numpy.array, pandas.Series\n",
    "\t\t    data to append\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "        \n",
    "        self.data = np.append(self.data,data)\n",
    "        return\n",
    "    \n",
    "    def initialize(self, verbose = True):\n",
    "        \"\"\"\n",
    "        Run the calibration (initialization) step\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    verbose : bool\n",
    "\t\t    (default = True) If True, gives details about the batch initialization\n",
    "        \"\"\"\n",
    "        n_init = self.init_data.size - self.depth\n",
    "        \n",
    "        M = backMean(self.init_data,self.depth)\n",
    "        T = self.init_data[self.depth:]-M[:-1] # new variable\n",
    "        \n",
    "        S = np.sort(T)     # we sort X to get the empirical quantile\n",
    "        self.init_threshold = S[int(0.98*n_init)] # t is fixed for the whole algorithm\n",
    "\n",
    "        # initial peaks\n",
    "        self.peaks = T[T>self.init_threshold]-self.init_threshold \n",
    "        self.Nt = self.peaks.size\n",
    "        self.n = n_init\n",
    "        \n",
    "        if verbose:\n",
    "            print('Initial threshold : %s' % self.init_threshold)\n",
    "            print('Number of peaks : %s' % self.Nt)\n",
    "            print('Grimshaw maximum log-likelihood estimation ... ', end = '')\n",
    "            \n",
    "        g,s,l = self._grimshaw()\n",
    "        self.extreme_quantile = self._quantile(g,s)\n",
    "        \n",
    "        if verbose:\n",
    "            print('[done]')\n",
    "            print('\\t'+chr(0x03B3) + ' = ' + str(g))\n",
    "            print('\\t'+chr(0x03C3) + ' = ' + str(s))\n",
    "            print('\\tL = ' + str(l))\n",
    "            print('Extreme quantile (probability = %s): %s' % (self.proba,self.extreme_quantile))\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _rootsFinder(fun,jac,bounds,npoints,method):\n",
    "        \"\"\"\n",
    "        Find possible roots of a scalar function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fun : function\n",
    "\t\t    scalar function \n",
    "        jac : function\n",
    "            first order derivative of the function  \n",
    "        bounds : tuple\n",
    "            (min,max) interval for the roots search    \n",
    "        npoints : int\n",
    "            maximum number of roots to output      \n",
    "        method : str\n",
    "            'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.array\n",
    "            possible roots of the function\n",
    "        \"\"\"\n",
    "        if method == 'regular':\n",
    "            step = (bounds[1]-bounds[0])/(npoints+1)\n",
    "            X0 = np.arange(bounds[0]+step,bounds[1],step)\n",
    "        elif method == 'random':\n",
    "            X0 = np.random.uniform(bounds[0],bounds[1],npoints)\n",
    "        \n",
    "        def objFun(X,f,jac):\n",
    "            g = 0\n",
    "            j = np.zeros(X.shape)\n",
    "            i = 0\n",
    "            for x in X:\n",
    "                fx = f(x)\n",
    "                g = g+fx**2\n",
    "                j[i] = 2*fx*jac(x)\n",
    "                i = i+1\n",
    "            return g,j\n",
    "        \n",
    "        opt = minimize(lambda X:objFun(X,fun,jac), X0, \n",
    "                       method='L-BFGS-B', \n",
    "                       jac=True, bounds=[bounds]*len(X0))\n",
    "        \n",
    "        X = opt.x\n",
    "        np.round(X,decimals = 5)\n",
    "        return np.unique(X)\n",
    "    \n",
    "    \n",
    "    def _log_likelihood(Y,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : numpy.array\n",
    "\t\t    observations\n",
    "        gamma : float\n",
    "            GPD index parameter\n",
    "        sigma : float\n",
    "            GPD scale parameter (>0)   \n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        n = Y.size\n",
    "        if gamma != 0:\n",
    "            tau = gamma/sigma\n",
    "            L = -n * log(sigma) - ( 1 + (1/gamma) ) * ( np.log(1+tau*Y) ).sum()\n",
    "        else:\n",
    "            L = n * ( 1 + log(Y.mean()) )\n",
    "        return L\n",
    "\n",
    "\n",
    "    def _grimshaw(self,epsilon = 1e-8, n_points = 10):\n",
    "        \"\"\"\n",
    "        Compute the GPD parameters estimation with the Grimshaw's trick\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "\t\t    numerical parameter to perform (default : 1e-8)\n",
    "        n_points : int\n",
    "            maximum number of candidates for maximum likelihood (default : 10)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        gamma_best,sigma_best,ll_best\n",
    "            gamma estimates, sigma estimates and corresponding log-likelihood\n",
    "        \"\"\"\n",
    "        def u(s):\n",
    "            return 1 + np.log(s).mean()\n",
    "            \n",
    "        def v(s):\n",
    "            return np.mean(1/s)\n",
    "        \n",
    "        def w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            return us*vs-1\n",
    "        \n",
    "        def jac_w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            jac_us = (1/t)*(1-vs)\n",
    "            jac_vs = (1/t)*(-vs+np.mean(1/s**2))\n",
    "            return us*jac_vs+vs*jac_us\n",
    "            \n",
    "    \n",
    "        Ym = self.peaks.min()\n",
    "        YM = self.peaks.max()\n",
    "        Ymean = self.peaks.mean()\n",
    "        \n",
    "        \n",
    "        a = -1/YM\n",
    "        if abs(a)<2*epsilon:\n",
    "            epsilon = abs(a)/n_points\n",
    "        \n",
    "        a = a + epsilon\n",
    "        b = 2*(Ymean-Ym)/(Ymean*Ym)\n",
    "        c = 2*(Ymean-Ym)/(Ym**2)\n",
    "    \n",
    "        # We look for possible roots\n",
    "        left_zeros = SPOT._rootsFinder(lambda t: w(self.peaks,t),\n",
    "                                 lambda t: jac_w(self.peaks,t),\n",
    "                                 (a+epsilon,-epsilon),\n",
    "                                 n_points,'regular')\n",
    "        \n",
    "        right_zeros = SPOT._rootsFinder(lambda t: w(self.peaks,t),\n",
    "                                  lambda t: jac_w(self.peaks,t),\n",
    "                                  (b,c),\n",
    "                                  n_points,'regular')\n",
    "    \n",
    "        # all the possible roots\n",
    "        zeros = np.concatenate((left_zeros,right_zeros))\n",
    "        \n",
    "        # 0 is always a solution so we initialize with it\n",
    "        gamma_best = 0\n",
    "        sigma_best = Ymean\n",
    "        ll_best = SPOT._log_likelihood(self.peaks,gamma_best,sigma_best)\n",
    "        \n",
    "        # we look for better candidates\n",
    "        for z in zeros:\n",
    "            gamma = u(1+z*self.peaks)-1\n",
    "            sigma = gamma/z\n",
    "            ll = dSPOT._log_likelihood(self.peaks,gamma,sigma)\n",
    "            if ll>ll_best:\n",
    "                gamma_best = gamma\n",
    "                sigma_best = sigma\n",
    "                ll_best = ll\n",
    "    \n",
    "        return gamma_best,sigma_best,ll_best\n",
    "\n",
    "    \n",
    "\n",
    "    def _quantile(self,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the quantile at level 1-q\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "\t\t    GPD parameter\n",
    "        sigma : float\n",
    "            GPD parameter\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            quantile at level 1-q for the GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        r = self.n * self.proba / self.Nt\n",
    "        if gamma != 0:\n",
    "            return self.init_threshold + (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "        else:\n",
    "            return self.init_threshold - sigma*log(r)\n",
    "\n",
    "        \n",
    "    def run(self, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Run biSPOT on the stream\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If False, SPOT will adapt the threshold assuming \\\n",
    "            there is no abnormal values\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            keys : 'upper_thresholds', 'lower_thresholds' and 'alarms'\n",
    "            \n",
    "            '***-thresholds' contains the extreme quantiles and 'alarms' contains \\\n",
    "            the indexes of the values which have triggered alarms\n",
    "            \n",
    "        \"\"\"\n",
    "        if (self.n>self.init_data.size):\n",
    "            print('Warning : the algorithm seems to have already been run, you \\\n",
    "            should initialize before running again')\n",
    "            return {}\n",
    "        \n",
    "        # actual normal window\n",
    "        W = self.init_data[-self.depth:]\n",
    "        \n",
    "        # list of the thresholds\n",
    "        th = []\n",
    "        alarm = []\n",
    "        # Loop over the stream\n",
    "        for i in tqdm.tqdm(range(self.data.size)):\n",
    "            Mi = W.mean()\n",
    "            # If the observed value exceeds the current threshold (alarm case)\n",
    "            if (self.data[i]-Mi)>self.extreme_quantile:\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks = np.append(self.peaks,self.data[i]-Mi-self.init_threshold)\n",
    "                    self.Nt += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw()\n",
    "                    self.extreme_quantile = self._quantile(g,s) #+ Mi\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "\n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif (self.data[i]-Mi)>self.init_threshold:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks = np.append(self.peaks,self.data[i]-Mi-self.init_threshold)\n",
    "                    self.Nt += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw()\n",
    "                    self.extreme_quantile = self._quantile(g,s) #+ Mi\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "            else:\n",
    "                self.n += 1\n",
    "                W = np.append(W[1:],self.data[i])\n",
    "\n",
    "                \n",
    "            th.append(self.extreme_quantile+Mi) # thresholds record\n",
    "        \n",
    "        return {'thresholds' : th, 'alarms': alarm}\n",
    "    \n",
    "\n",
    "    def plot(self,run_results, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Plot the results given by the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        run_results : dict\n",
    "            results given by the 'run' method\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If True, alarms are plotted.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        list\n",
    "            list of the plots\n",
    "            \n",
    "        \"\"\"\n",
    "        x = range(self.data.size)\n",
    "        K = run_results.keys()\n",
    "        \n",
    "        ts_fig, = plt.plot(x,self.data,color=\"blue\")\n",
    "        fig = [ts_fig]\n",
    "        ax=plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "#        if 'upper_thresholds' in K:\n",
    "#            thup = run_results['upper_thresholds']\n",
    "#            uth_fig, = plt.plot(x,thup,color=deep_saffron,lw=2,ls='dashed')\n",
    "#            fig.append(uth_fig)\n",
    "#            \n",
    "#        if 'lower_thresholds' in K:\n",
    "#            thdown = run_results['lower_thresholds']\n",
    "#            lth_fig, = plt.plot(x,thdown,color=deep_saffron,lw=2,ls='dashed')\n",
    "#            fig.append(lth_fig)\n",
    "        \n",
    "        if 'thresholds' in K:\n",
    "            th = run_results['thresholds']\n",
    "            th_fig, = plt.plot(x,th,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(th_fig)\n",
    "        \n",
    "        if with_alarm and ('alarms' in K):\n",
    "            alarm = run_results['alarms']\n",
    "            if len(alarm)>0:\n",
    "                plt.scatter(alarm,self.data[alarm],color='red')\n",
    "            \n",
    "        plt.xlim((0,self.data.size))\n",
    "\n",
    "        \n",
    "        return fig\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "=========================== DRIFT & DOUBLE BOUNDS =============================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class bidSPOT:\n",
    "    \"\"\"\n",
    "    This class allows to run DSPOT algorithm on univariate dataset (upper and lower bounds)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    proba : float\n",
    "        Detection level (risk), chosen by the user\n",
    "        \n",
    "    depth : int\n",
    "        Number of observations to compute the moving average\n",
    "        \n",
    "    extreme_quantile : float\n",
    "        current threshold (bound between normal and abnormal events)\n",
    "        \n",
    "    data : numpy.array\n",
    "        stream\n",
    "    \n",
    "    init_data : numpy.array\n",
    "        initial batch of observations (for the calibration/initialization step)\n",
    "    \n",
    "    init_threshold : float\n",
    "        initial threshold computed during the calibration step\n",
    "    \n",
    "    peaks : numpy.array\n",
    "        array of peaks (excesses above the initial threshold)\n",
    "    \n",
    "    n : int\n",
    "        number of observed values\n",
    "    \n",
    "    Nt : int\n",
    "        number of observed peaks\n",
    "    \"\"\"\n",
    "    def __init__(self, q = 1e-4, depth = 10):\n",
    "        self.proba = q\n",
    "        self.data = None\n",
    "        self.init_data = None\n",
    "        self.n = 0\n",
    "        self.depth = depth\n",
    "        \n",
    "        nonedict =  {'up':None,'down':None}\n",
    "        \n",
    "        self.extreme_quantile = dict.copy(nonedict)\n",
    "        self.init_threshold = dict.copy(nonedict)\n",
    "        self.peaks = dict.copy(nonedict)\n",
    "        self.gamma = dict.copy(nonedict)\n",
    "        self.sigma = dict.copy(nonedict)\n",
    "        self.Nt = {'up':0,'down':0}\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        s += 'Streaming Peaks-Over-Threshold Object\\n'\n",
    "        s += 'Detection level q = %s\\n' % self.proba\n",
    "        if self.data is not None:\n",
    "            s += 'Data imported : Yes\\n'\n",
    "            s += '\\t initialization  : %s values\\n' % self.init_data.size\n",
    "            s += '\\t stream : %s values\\n' % self.data.size\n",
    "        else:\n",
    "            s += 'Data imported : No\\n'\n",
    "            return s\n",
    "            \n",
    "        if self.n == 0:\n",
    "            s += 'Algorithm initialized : No\\n'\n",
    "        else:\n",
    "            s += 'Algorithm initialized : Yes\\n'\n",
    "            s += '\\t initial threshold : %s\\n' % self.init_threshold\n",
    "            \n",
    "            r = self.n-self.init_data.size\n",
    "            if r > 0:\n",
    "                s += 'Algorithm run : Yes\\n'\n",
    "                s += '\\t number of observations : %s (%.2f %%)\\n' % (r,100*r/self.n)\n",
    "                s += '\\t triggered alarms : %s (%.2f %%)\\n' % (len(self.alarm),100*len(self.alarm)/self.n)\n",
    "            else:\n",
    "                s += '\\t number of peaks  : %s\\n' % self.Nt\n",
    "                s += '\\t upper extreme quantile : %s\\n' % self.extreme_quantile['up']\n",
    "                s += '\\t lower extreme quantile : %s\\n' % self.extreme_quantile['down']\n",
    "                s += 'Algorithm run : No\\n'\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def fit(self,init_data,data):\n",
    "        \"\"\"\n",
    "        Import data to biDSPOT object\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    init_data : list, numpy.array or pandas.Series\n",
    "\t\t    initial batch to calibrate the algorithm\n",
    "            \n",
    "        data : numpy.array\n",
    "\t\t    data for the run (list, np.array or pd.series)\n",
    "\t\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            self.data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            self.data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "            \n",
    "        if isinstance(init_data,list):\n",
    "            self.init_data = np.array(init_data)\n",
    "        elif isinstance(init_data,np.ndarray):\n",
    "            self.init_data = init_data\n",
    "        elif isinstance(init_data,pd.Series):\n",
    "            self.init_data = init_data.values\n",
    "        elif isinstance(init_data,int):\n",
    "            self.init_data = self.data[:init_data]\n",
    "            self.data = self.data[init_data:]\n",
    "        elif isinstance(init_data,float) & (init_data<1) & (init_data>0):\n",
    "            r = int(init_data*data.size)\n",
    "            self.init_data = self.data[:r]\n",
    "            self.data = self.data[r:]\n",
    "        else:\n",
    "            print('The initial data cannot be set')\n",
    "            return\n",
    "        \n",
    "    def add(self,data):\n",
    "        \"\"\"\n",
    "        This function allows to append data to the already fitted data\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    data : list, numpy.array, pandas.Series\n",
    "\t\t    data to append\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "        \n",
    "        self.data = np.append(self.data,data)\n",
    "        return\n",
    "    \n",
    "    def initialize(self, verbose = True):\n",
    "        \"\"\"\n",
    "        Run the calibration (initialization) step\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    verbose : bool\n",
    "\t\t    (default = True) If True, gives details about the batch initialization\n",
    "        \"\"\"\n",
    "        n_init = self.init_data.size - self.depth\n",
    "        \n",
    "        M = backMean(self.init_data,self.depth)\n",
    "        T = self.init_data[self.depth:]-M[:-1] # new variable\n",
    "        \n",
    "        S = np.sort(T)     # we sort T to get the empirical quantile\n",
    "        self.init_threshold['up'] = S[int(0.98*n_init)] # t is fixed for the whole algorithm\n",
    "        self.init_threshold['down'] = S[int(0.02*n_init)] # t is fixed for the whole algorithm\n",
    "\n",
    "        # initial peaks\n",
    "        self.peaks['up'] = T[T>self.init_threshold['up']]-self.init_threshold['up']\n",
    "        self.peaks['down'] = -( T[ T<self.init_threshold['down'] ] - self.init_threshold['down'] )\n",
    "        self.Nt['up'] = self.peaks['up'].size\n",
    "        self.Nt['down'] = self.peaks['down'].size\n",
    "        self.n = n_init\n",
    "        \n",
    "        if verbose:\n",
    "            print('Initial threshold : %s' % self.init_threshold)\n",
    "            print('Number of peaks : %s' % self.Nt)\n",
    "            print('Grimshaw maximum log-likelihood estimation ... ', end = '')\n",
    "            \n",
    "        l = {'up':None,'down':None}\n",
    "        for side in ['up','down']:\n",
    "            g,s,l[side] = self._grimshaw(side)\n",
    "            self.extreme_quantile[side] = self._quantile(side,g,s)\n",
    "            self.gamma[side] = g\n",
    "            self.sigma[side] = s\n",
    "        \n",
    "        ltab = 20\n",
    "        form = ('\\t'+'%20s' + '%20.2f' + '%20.2f')\n",
    "        if verbose:\n",
    "            print('[done]')\n",
    "            print('\\t' + 'Parameters'.rjust(ltab) + 'Upper'.rjust(ltab) + 'Lower'.rjust(ltab))\n",
    "            print('\\t' + '-'*ltab*3)\n",
    "            print(form % (chr(0x03B3),self.gamma['up'],self.gamma['down']))\n",
    "            print(form % (chr(0x03C3),self.sigma['up'],self.sigma['down']))\n",
    "            print(form % ('likelihood',l['up'],l['down']))\n",
    "            print(form % ('Extreme quantile',self.extreme_quantile['up'],self.extreme_quantile['down']))\n",
    "            print('\\t' + '-'*ltab*3)\n",
    "        return \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _rootsFinder(fun,jac,bounds,npoints,method):\n",
    "        \"\"\"\n",
    "        Find possible roots of a scalar function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fun : function\n",
    "\t\t    scalar function \n",
    "        jac : function\n",
    "            first order derivative of the function  \n",
    "        bounds : tuple\n",
    "            (min,max) interval for the roots search    \n",
    "        npoints : int\n",
    "            maximum number of roots to output      \n",
    "        method : str\n",
    "            'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.array\n",
    "            possible roots of the function\n",
    "        \"\"\"\n",
    "        if method == 'regular':\n",
    "            step = (bounds[1]-bounds[0])/(npoints+1)\n",
    "            X0 = np.arange(bounds[0]+step,bounds[1],step)\n",
    "        elif method == 'random':\n",
    "            X0 = np.random.uniform(bounds[0],bounds[1],npoints)\n",
    "        \n",
    "        def objFun(X,f,jac):\n",
    "            g = 0\n",
    "            j = np.zeros(X.shape)\n",
    "            i = 0\n",
    "            for x in X:\n",
    "                fx = f(x)\n",
    "                g = g+fx**2\n",
    "                j[i] = 2*fx*jac(x)\n",
    "                i = i+1\n",
    "            return g,j\n",
    "        \n",
    "        opt = minimize(lambda X:objFun(X,fun,jac), X0, \n",
    "                       method='L-BFGS-B', \n",
    "                       jac=True, bounds=[bounds]*len(X0))\n",
    "        \n",
    "        X = opt.x\n",
    "        np.round(X,decimals = 5)\n",
    "        return np.unique(X)\n",
    "    \n",
    "    \n",
    "    def _log_likelihood(Y,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : numpy.array\n",
    "\t\t    observations\n",
    "        gamma : float\n",
    "            GPD index parameter\n",
    "        sigma : float\n",
    "            GPD scale parameter (>0)   \n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        n = Y.size\n",
    "        if gamma != 0:\n",
    "            tau = gamma/sigma\n",
    "            L = -n * log(sigma) - ( 1 + (1/gamma) ) * ( np.log(1+tau*Y) ).sum()\n",
    "        else:\n",
    "            L = n * ( 1 + log(Y.mean()) )\n",
    "        return L\n",
    "\n",
    "\n",
    "    def _grimshaw(self,side,epsilon = 1e-8, n_points = 8):\n",
    "        \"\"\"\n",
    "        Compute the GPD parameters estimation with the Grimshaw's trick\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "\t\t    numerical parameter to perform (default : 1e-8)\n",
    "        n_points : int\n",
    "            maximum number of candidates for maximum likelihood (default : 10)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        gamma_best,sigma_best,ll_best\n",
    "            gamma estimates, sigma estimates and corresponding log-likelihood\n",
    "        \"\"\"\n",
    "        def u(s):\n",
    "            return 1 + np.log(s).mean()\n",
    "            \n",
    "        def v(s):\n",
    "            return np.mean(1/s)\n",
    "        \n",
    "        def w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            return us*vs-1\n",
    "        \n",
    "        def jac_w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            jac_us = (1/t)*(1-vs)\n",
    "            jac_vs = (1/t)*(-vs+np.mean(1/s**2))\n",
    "            return us*jac_vs+vs*jac_us\n",
    "            \n",
    "    \n",
    "        Ym = self.peaks[side].min()\n",
    "        YM = self.peaks[side].max()\n",
    "        Ymean = self.peaks[side].mean()\n",
    "        \n",
    "        \n",
    "        a = -1/YM\n",
    "        if abs(a)<2*epsilon:\n",
    "            epsilon = abs(a)/n_points\n",
    "        \n",
    "        a = a + epsilon\n",
    "        b = 2*(Ymean-Ym)/(Ymean*Ym)\n",
    "        c = 2*(Ymean-Ym)/(Ym**2)\n",
    "    \n",
    "        # We look for possible roots\n",
    "        left_zeros = bidSPOT._rootsFinder(lambda t: w(self.peaks[side],t),\n",
    "                                 lambda t: jac_w(self.peaks[side],t),\n",
    "                                 (a+epsilon,-epsilon),\n",
    "                                 n_points,'regular')\n",
    "        \n",
    "        right_zeros = bidSPOT._rootsFinder(lambda t: w(self.peaks[side],t),\n",
    "                                  lambda t: jac_w(self.peaks[side],t),\n",
    "                                  (b,c),\n",
    "                                  n_points,'regular')\n",
    "    \n",
    "        # all the possible roots\n",
    "        zeros = np.concatenate((left_zeros,right_zeros))\n",
    "        \n",
    "        # 0 is always a solution so we initialize with it\n",
    "        gamma_best = 0\n",
    "        sigma_best = Ymean\n",
    "        ll_best = bidSPOT._log_likelihood(self.peaks[side],gamma_best,sigma_best)\n",
    "        \n",
    "        # we look for better candidates\n",
    "        for z in zeros:\n",
    "            gamma = u(1+z*self.peaks[side])-1\n",
    "            sigma = gamma/z\n",
    "            ll = bidSPOT._log_likelihood(self.peaks[side],gamma,sigma)\n",
    "            if ll>ll_best:\n",
    "                gamma_best = gamma\n",
    "                sigma_best = sigma\n",
    "                ll_best = ll\n",
    "    \n",
    "        return gamma_best,sigma_best,ll_best\n",
    "\n",
    "    \n",
    "\n",
    "    def _quantile(self,side,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the quantile at level 1-q for a given side\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        side : str\n",
    "            'up' or 'down'\n",
    "        gamma : float\n",
    "\t\t    GPD parameter\n",
    "        sigma : float\n",
    "            GPD parameter\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            quantile at level 1-q for the GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        if side == 'up':\n",
    "            r = self.n * self.proba / self.Nt[side]\n",
    "            if gamma != 0:\n",
    "                return self.init_threshold['up'] + (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "            else:\n",
    "                return self.init_threshold['up'] - sigma*log(r)\n",
    "        elif side == 'down':\n",
    "            r = self.n * self.proba / self.Nt[side]\n",
    "            if gamma != 0:\n",
    "                return self.init_threshold['down'] - (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "            else:\n",
    "                return self.init_threshold['down'] + sigma*log(r)\n",
    "        else:\n",
    "            print('error : the side is not right')\n",
    "\n",
    "        \n",
    "    def run(self, with_alarm = True, plot = True):\n",
    "        \"\"\"\n",
    "        Run biDSPOT on the stream\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If False, SPOT will adapt the threshold assuming \\\n",
    "            there is no abnormal values\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            keys : 'upper_thresholds', 'lower_thresholds' and 'alarms'\n",
    "            \n",
    "            '***-thresholds' contains the extreme quantiles and 'alarms' contains \\\n",
    "            the indexes of the values which have triggered alarms\n",
    "            \n",
    "        \"\"\"\n",
    "        if (self.n>self.init_data.size):\n",
    "            print('Warning : the algorithm seems to have already been run, you \\\n",
    "            should initialize before running again')\n",
    "            return {}\n",
    "        \n",
    "        # actual normal window\n",
    "        W = self.init_data[-self.depth:]\n",
    "        \n",
    "        # list of the thresholds\n",
    "        thup = []\n",
    "        thdown = []\n",
    "        alarm = []\n",
    "        # Loop over the stream\n",
    "        for i in tqdm.tqdm(range(self.data.size)):\n",
    "            Mi = W.mean()\n",
    "            Ni = self.data[i]-Mi\n",
    "            # If the observed value exceeds the current threshold (alarm case)\n",
    "            if Ni>self.extreme_quantile['up'] :\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks['up'] = np.append(self.peaks['up'],Ni-self.init_threshold['up'])\n",
    "                    self.Nt['up'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('up')\n",
    "                    self.extreme_quantile['up'] = self._quantile('up',g,s)\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "                    \n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif Ni>self.init_threshold['up']:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks['up'] = np.append(self.peaks['up'],Ni-self.init_threshold['up'])\n",
    "                    self.Nt['up'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "                    g,s,l = self._grimshaw('up')\n",
    "                    self.extreme_quantile['up'] = self._quantile('up',g,s)\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "                    \n",
    "            elif Ni<self.extreme_quantile['down'] :\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks['down'] = np.append(self.peaks['down'],-(Ni-self.init_threshold['down']))\n",
    "                    self.Nt['down'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('down')\n",
    "                    self.extreme_quantile['down'] = self._quantile('down',g,s)\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "                    \n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif Ni<self.init_threshold['down']:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks['down'] = np.append(self.peaks['down'],-(Ni-self.init_threshold['down']))\n",
    "                    self.Nt['down'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('down')\n",
    "                    self.extreme_quantile['down'] = self._quantile('down',g,s)\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "            else:\n",
    "                self.n += 1\n",
    "                W = np.append(W[1:],self.data[i])\n",
    "\n",
    "                \n",
    "            thup.append(self.extreme_quantile['up']+Mi) # upper thresholds record\n",
    "            thdown.append(self.extreme_quantile['down']+Mi) # lower thresholds record\n",
    "        \n",
    "        return {'upper_thresholds' : thup,'lower_thresholds' : thdown, 'alarms': alarm}\n",
    "    \n",
    "\n",
    "    def plot(self,run_results, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Plot the results given by the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        run_results : dict\n",
    "            results given by the 'run' method\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If True, alarms are plotted.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        list\n",
    "            list of the plots\n",
    "            \n",
    "        \"\"\"\n",
    "        x = range(self.data.size)\n",
    "        K = run_results.keys()\n",
    "        \n",
    "        ts_fig, = plt.plot(x,self.data,color=\"blue\")\n",
    "        fig = [ts_fig]\n",
    "        ax=plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "        if 'upper_thresholds' in K:\n",
    "            thup = run_results['upper_thresholds']\n",
    "            uth_fig, = plt.plot(x,thup,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(uth_fig)\n",
    "            \n",
    "        if 'lower_thresholds' in K:\n",
    "            thdown = run_results['lower_thresholds']\n",
    "            lth_fig, = plt.plot(x,thdown,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(lth_fig)\n",
    "        \n",
    "        if with_alarm and ('alarms' in K):\n",
    "            alarm = run_results['alarms']\n",
    "            if len(alarm)>0:\n",
    "                al_fig = plt.scatter(alarm,self.data[alarm],color='red')\n",
    "                fig.append(al_fig)\n",
    "            \n",
    "        plt.xlim((0,self.data.size))\n",
    "\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.751796Z",
     "start_time": "2020-05-20T04:31:46.747646Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Global Figure Parameters\n",
    "import matplotlib.pylab as pylab\n",
    "plot_params = {'legend.fontsize': 60,\n",
    "          'figure.figsize': (25*2, 15*2),\n",
    "         'axes.labelsize': 60*2,\n",
    "         'axes.titlesize':80*2,\n",
    "         'xtick.labelsize':40*2,\n",
    "         'ytick.labelsize':40*2}\n",
    "pylab.rcParams.update(plot_params)\n",
    "global fig_len\n",
    "global fig_wid\n",
    "global m_size\n",
    "\n",
    "fig_len=8\n",
    "fig_wid=8\n",
    "m_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.814010Z",
     "start_time": "2020-05-20T04:31:46.773140Z"
    },
    "code_folding": [
     0,
     11,
     119,
     150,
     184
    ]
   },
   "outputs": [],
   "source": [
    "def eval_perf(X,y):\n",
    "    np.random.seed(4323)\n",
    "    labels=y\n",
    "    outliers_fraction= np.sum(y)/len(y)\n",
    "    eval_metrics={}\n",
    "    eval_preds={}\n",
    "#     ros = RandomOverSampler(random_state=0)\n",
    "#     X_resampled, y_resampled = ros.fit_resample(np.arange(0,len(X),1).reshape(-1,1), y)\n",
    "#     train, test = train_test_split(X_resampled, test_size=0.2,random_state=200)\n",
    "#     train=train[:,0]\n",
    "    train=np.arange(0,len(X),1)\n",
    "    if 1:\n",
    "        eval_metrics['LOF']={}\n",
    "        eval_metrics['LOF']['fpr']={}\n",
    "        eval_metrics['LOF']['tpr']={}\n",
    "        eval_metrics['LOF']['thresholds']={}\n",
    "        eval_metrics['LOF']['fp']={}\n",
    "        eval_metrics['LOF']['tp']={}\n",
    "        eval_metrics['LOF']['fn']={}\n",
    "        eval_metrics['LOF']['tn']={}\n",
    "        eval_metrics['LOF']['recall']={}\n",
    "        eval_metrics['LOF']['specificity']={}\n",
    "        eval_metrics['LOF']['precision']={}\n",
    "        eval_metrics['LOF']['accuracy']={}\n",
    "        eval_metrics['LOF']['fmeasure']={}\n",
    "        eval_metrics['LOF']['purity']={}\n",
    "        eval_metrics['LOF']['auc']={}    \n",
    "\n",
    "        eval_metrics['Kmeans--']={}\n",
    "        eval_metrics['Kmeans--']['fpr']={}\n",
    "        eval_metrics['Kmeans--']['tpr']={}\n",
    "        eval_metrics['Kmeans--']['thresholds']={}\n",
    "        eval_metrics['Kmeans--']['fp']={}\n",
    "        eval_metrics['Kmeans--']['tp']={}\n",
    "        eval_metrics['Kmeans--']['fn']={}\n",
    "        eval_metrics['Kmeans--']['tn']={}\n",
    "        eval_metrics['Kmeans--']['recall']={}\n",
    "        eval_metrics['Kmeans--']['specificity']={}\n",
    "        eval_metrics['Kmeans--']['precision']={}\n",
    "        eval_metrics['Kmeans--']['accuracy']={}\n",
    "        eval_metrics['Kmeans--']['fmeasure']={}\n",
    "        eval_metrics['Kmeans--']['purity']={}\n",
    "        eval_metrics['Kmeans--']['auc']={}    \n",
    "\n",
    "        eval_metrics['knn']={}\n",
    "        eval_metrics['knn']['fpr']={}\n",
    "        eval_metrics['knn']['tpr']={}\n",
    "        eval_metrics['knn']['thresholds']={}\n",
    "        eval_metrics['knn']['fp']={}\n",
    "        eval_metrics['knn']['tp']={}\n",
    "        eval_metrics['knn']['fn']={}\n",
    "        eval_metrics['knn']['tn']={}\n",
    "        eval_metrics['knn']['recall']={}\n",
    "        eval_metrics['knn']['specificity']={}\n",
    "        eval_metrics['knn']['precision']={}\n",
    "        eval_metrics['knn']['accuracy']={}\n",
    "        eval_metrics['knn']['fmeasure']={}\n",
    "        eval_metrics['knn']['purity']={}\n",
    "        eval_metrics['knn']['auc']={}    \n",
    "\n",
    "        eval_metrics['ocsvm']={}\n",
    "        eval_metrics['ocsvm']['fpr']={}\n",
    "        eval_metrics['ocsvm']['tpr']={}\n",
    "        eval_metrics['ocsvm']['thresholds']={}\n",
    "        eval_metrics['ocsvm']['fp']={}\n",
    "        eval_metrics['ocsvm']['tp']={}\n",
    "        eval_metrics['ocsvm']['fn']={}\n",
    "        eval_metrics['ocsvm']['tn']={}\n",
    "        eval_metrics['ocsvm']['recall']={}\n",
    "        eval_metrics['ocsvm']['specificity']={}\n",
    "        eval_metrics['ocsvm']['precision']={}\n",
    "        eval_metrics['ocsvm']['accuracy']={}\n",
    "        eval_metrics['ocsvm']['fmeasure']={}\n",
    "        eval_metrics['ocsvm']['purity']={}\n",
    "        eval_metrics['ocsvm']['auc']={}    \n",
    "\n",
    "\n",
    "    # # try lof\n",
    "    for nn in range(10,90,10):\n",
    "        # fit the model for outlier detection (default)\n",
    "        clf = LocalOutlierFactor(n_neighbors=nn, contamination=outliers_fraction)\n",
    "        # use fit_predict to compute the predicted labels of the training samples\n",
    "        # (when LOF is used for outlier detection, the estimator has no predict,\n",
    "        # decision_function and score_samples methods).\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        n_errors = (y_pred != y).sum()\n",
    "        pred = clf.negative_outlier_factor_ \n",
    "\n",
    "        preds=np.zeros(len(np.array(pred)))\n",
    "        preds[np.argsort(-np.array(pred))[0:int(sum(y))]]=1\n",
    "        tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "        recall=tp/(tp+fn)\n",
    "        specificity=tn/(tn+fp)\n",
    "        precision=tp/(tp+fp)\n",
    "        accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "        fmeasure=2*precision*recall/(precision + recall)\n",
    "        purity=purity_score(y, preds)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "\n",
    "        eval_preds['LOF']=preds\n",
    "        eval_metrics['LOF']['fpr'][nn]=fpr\n",
    "        eval_metrics['LOF']['tpr'][nn]=tpr\n",
    "        eval_metrics['LOF']['thresholds'][nn]=thresholds\n",
    "        eval_metrics['LOF']['fp'][nn]=fp\n",
    "        eval_metrics['LOF']['tp'][nn]=tp\n",
    "        eval_metrics['LOF']['fn'][nn]=fn\n",
    "        eval_metrics['LOF']['tn'][nn]=tn\n",
    "        eval_metrics['LOF']['recall'][nn]=recall\n",
    "        eval_metrics['LOF']['specificity'][nn]=specificity\n",
    "        eval_metrics['LOF']['precision'][nn]=precision\n",
    "        eval_metrics['LOF']['accuracy'][nn]=accuracy\n",
    "        eval_metrics['LOF']['fmeasure'][nn]=fmeasure\n",
    "        eval_metrics['LOF']['purity'][nn]=purity\n",
    "        eval_metrics['LOF']['auc'][nn]=auc\n",
    "\n",
    "    # # Kmeans--\n",
    "    ks = [1,3,5,7,9,11,21]\n",
    "    for i in range(len(ks)):   \n",
    "        if ks[i]<len(X):\n",
    "            linds, C, c = kmeans__(X,ks[i],int(np.sum(labels)))\n",
    "            preds = np.zeros([labels.shape[0],])\n",
    "            preds[linds] = 1\n",
    "            tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "            recall=tp/(tp+fn)\n",
    "            specificity=tn/(tn+fp)\n",
    "            precision=tp/(tp+fp)\n",
    "            accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "            fmeasure=2*precision*recall/(precision + recall)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y, preds)\n",
    "            purity=purity_score(y, preds)\n",
    "            eval_preds['Kmeans-- '+str(ks[i])]=preds\n",
    "            eval_metrics['Kmeans--']['fpr'][ks[i]]=fpr\n",
    "            eval_metrics['Kmeans--']['tpr'][ks[i]]=tpr\n",
    "            eval_metrics['Kmeans--']['thresholds'][ks[i]]=thresholds\n",
    "            eval_metrics['Kmeans--']['fp'][ks[i]]=fp\n",
    "            eval_metrics['Kmeans--']['tp'][ks[i]]=tp\n",
    "            eval_metrics['Kmeans--']['fn'][ks[i]]=fn\n",
    "            eval_metrics['Kmeans--']['tn'][ks[i]]=tn\n",
    "            eval_metrics['Kmeans--']['recall'][ks[i]]=recall\n",
    "            eval_metrics['Kmeans--']['specificity'][ks[i]]=specificity\n",
    "            eval_metrics['Kmeans--']['precision'][ks[i]]=precision\n",
    "            eval_metrics['Kmeans--']['accuracy'][ks[i]]=accuracy\n",
    "            eval_metrics['Kmeans--']['fmeasure'][ks[i]]=fmeasure\n",
    "            eval_metrics['Kmeans--']['purity'][ks[i]]=purity\n",
    "            eval_metrics['Kmeans--']['auc'][ks[i]]=auc\n",
    "\n",
    "    # # KNN\n",
    "    ks = [1,3,5,7,9,11,21]\n",
    "    for i in range(len(ks)):    \n",
    "        #try knn\n",
    "        knn = KNeighborsClassifier(n_neighbors=ks[i])\n",
    "        knn.fit(X[train], y[train])\n",
    "        preds=knn.predict(X)\n",
    "        pred = preds\n",
    "        preds=np.zeros(len(np.array(pred)))\n",
    "        preds[np.argsort(-np.array(pred))[0:int(sum(y))]]=1\n",
    "        tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "        recall=tp/(tp+fn)\n",
    "        specificity=tn/(tn+fp)\n",
    "        precision=tp/(tp+fp)\n",
    "        accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "        fmeasure=2*precision*recall/(precision + recall)\n",
    "        purity=purity_score(y, preds)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "        eval_preds['knn '+str(ks[i])]=preds\n",
    "        eval_metrics['knn']['fpr'][ks[i]]=fpr\n",
    "        eval_metrics['knn']['tpr'][ks[i]]=tpr\n",
    "        eval_metrics['knn']['thresholds'][ks[i]]=thresholds\n",
    "        eval_metrics['knn']['fp'][ks[i]]=fp\n",
    "        eval_metrics['knn']['tp'][ks[i]]=tp\n",
    "        eval_metrics['knn']['fn'][ks[i]]=fn\n",
    "        eval_metrics['knn']['tn'][ks[i]]=tn\n",
    "        eval_metrics['knn']['recall'][ks[i]]=recall\n",
    "        eval_metrics['knn']['specificity'][ks[i]]=specificity\n",
    "        eval_metrics['knn']['precision'][ks[i]]=precision\n",
    "        eval_metrics['knn']['accuracy'][ks[i]]=accuracy\n",
    "        eval_metrics['knn']['fmeasure'][ks[i]]=fmeasure\n",
    "        eval_metrics['knn']['purity'][ks[i]]=purity\n",
    "        eval_metrics['knn']['auc'][ks[i]]=auc\n",
    "\n",
    "#     # try ocsvm\n",
    "    for gmma in np.arange(0.05,1,0.05):\n",
    "        ##print(gmma)\n",
    "#             oc = ocsvm(nu=nu,gamma=gmma)\n",
    "        oc=svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\",gamma=gmma)\n",
    "        oc.fit(X[train])\n",
    "        p = oc.predict(X)\n",
    "        preds = np.zeros(p.shape)\n",
    "        preds[p == -1] = 1\n",
    "        preds[p == 1] = 0\n",
    "        tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "        recall=tp/(tp+fn)\n",
    "        specificity=tn/(tn+fp)\n",
    "        precision=tp/(tp+fp)\n",
    "        accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "        fmeasure=2*precision*recall/(precision + recall)\n",
    "        purity=purity_score(y, preds)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, preds)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "\n",
    "        eval_preds['ocsvm '+str(gmma)]=preds\n",
    "        eval_metrics['ocsvm']['fpr'][gmma]=fpr\n",
    "        eval_metrics['ocsvm']['tpr'][gmma]=tpr\n",
    "        eval_metrics['ocsvm']['thresholds'][gmma]=thresholds\n",
    "        eval_metrics['ocsvm']['fp'][gmma]=fp\n",
    "        eval_metrics['ocsvm']['tp'][gmma]=tp\n",
    "        eval_metrics['ocsvm']['fn'][gmma]=fn\n",
    "        eval_metrics['ocsvm']['tn'][gmma]=tn\n",
    "        eval_metrics['ocsvm']['recall'][gmma]=recall\n",
    "        eval_metrics['ocsvm']['specificity'][gmma]=specificity\n",
    "        eval_metrics['ocsvm']['precision'][gmma]=precision\n",
    "        eval_metrics['ocsvm']['accuracy'][gmma]=accuracy\n",
    "        eval_metrics['ocsvm']['fmeasure'][gmma]=fmeasure\n",
    "        eval_metrics['ocsvm']['purity'][gmma]=purity\n",
    "        eval_metrics['ocsvm']['auc'][gmma]=auc\n",
    "\n",
    "    return eval_metrics,eval_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.859302Z",
     "start_time": "2020-05-20T04:31:46.832975Z"
    },
    "code_folding": [
     0,
     19,
     33,
     52,
     80,
     82,
     91,
     121,
     129,
     143,
     150
    ]
   },
   "outputs": [],
   "source": [
    "def plotClusters(thetas,z,samples):\n",
    "    thetameans = []\n",
    "    K = len(thetas)\n",
    "    plt.figure(figsize=(fig_len,fig_wid))\n",
    "    ax=plt.gca()\n",
    "    ax.set_facecolor('white')\n",
    "    ax.tick_params(labelsize=25)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.grid(color='k', linestyle='-.', linewidth=0.3)\n",
    "\n",
    "    for k in range(K):\n",
    "        thetameans.append(thetas[k][0])\n",
    "    thetameans = np.array(thetameans)\n",
    "    for k in range(K):\n",
    "        plt.scatter(samples[z == k,0],samples[z == k,1],marker='*',s=m_size)\n",
    "    plt.legend([str(k) for k in range(K)])\n",
    "    #plt.scatter(thetameans[:,0],thetameans[:,1],marker='x')\n",
    "    for k in range(K):\n",
    "        plt.text(thetameans[k,0],thetameans[k,1],str(k))\n",
    "def multivariatet(mu,Sigma,N,M):\n",
    "    '''\n",
    "    Output:\n",
    "    Produce M samples of d-dimensional multivariate t distribution\n",
    "    Input:\n",
    "    mu = mean (d dimensional numpy array or scalar)\n",
    "    Sigma = scale matrix (dxd numpy array)\n",
    "    N = degrees of freedom\n",
    "    M = # of samples to produce\n",
    "    '''\n",
    "    d = len(Sigma)\n",
    "    g = np.tile(np.random.gamma(N/2.,2./N,M),(d,1)).T\n",
    "    Z = np.random.multivariate_normal(np.zeros(d),Sigma,M)\n",
    "    return mu + Z/np.sqrt(g)\n",
    "def normalinvwishartsample(params):\n",
    "    '''\n",
    "    Generate sample from a Normal Inverse Wishart distribution\n",
    "\n",
    "    Inputs:\n",
    "    params - Parameters for the NIW distribution \n",
    "        mu    - Mean parameter: n x 1 numpy array\n",
    "        W     - Precision parameter: d x d numpy array\n",
    "        kappa - Scalar parameter for normal distribution covariance matrix\n",
    "        nu    - Scalar parameter for Wishart distribution\n",
    "\n",
    "    Output:\n",
    "    Sample - Sample mean vector, mu_s and Sample covariance matrix, W_s\n",
    "    '''\n",
    "    mu,W,kappa,nu = params\n",
    "    # first sample W from a Inverse Wishart distribution\n",
    "    W_s = invwishart(df=nu, scale=W).rvs()\n",
    "    mu_s = np.random.multivariate_normal(mu.flatten(),W_s/kappa,1) \n",
    "    return np.transpose(mu_s),W_s\n",
    "def normalinvwishartmarginal(X,params):\n",
    "    '''\n",
    "    Marginal likelihood of dataset X using a Normal Inverse Wishart prior\n",
    "\n",
    "    Inputs:\n",
    "    X      - Dataset matrix: n x d numpy array\n",
    "    params - Parameters for the NIW distribution \n",
    "        mu    - Mean parameter: n x 1 numpy array\n",
    "        W     - Precision parameter: d x d numpy array\n",
    "        kappa - Scalar parameter for normal distribution covariance matrix\n",
    "        nu    - Scalar parameter for Wishart distribution\n",
    "\n",
    "    Output:\n",
    "    Marginal likelihood of X - scalar\n",
    "    '''\n",
    "    mu,W,kappa,nu = params\n",
    "    mu=X\n",
    "\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    nu_n = nu + n\n",
    "    kappa_n = kappa + n\n",
    "    X_mean = np.mean(X,axis=0)\n",
    "    X_mean = X_mean[:,np.newaxis]\n",
    "    S = scatter(X)\n",
    "    W_n = W + S + ((kappa*n)/(kappa+n))*np.dot(mu - X_mean,np.transpose(mu - X_mean))\n",
    "    #(1/np.power(np.pi,n*d*0.5))*(gamma(nu_n*0.5)/gamma(nu*0.5))*(np.power(np.linalg.det(W),nu*0.5)/np.power(np.linalg.det(W_n),nu_n*0.5))*np.power(kappa/kappa_n,0.5*d)\n",
    "    return (1/np.power(np.pi,n*d*0.5))*(gamma(nu_n*0.5)/gamma(nu*0.5))*(np.power(np.linalg.det(W)/np.linalg.det(W_n),nu*0.5)/np.power(np.linalg.det(W_n),(nu_n-nu)*0.5))*np.power(kappa/kappa_n,0.5*d)\n",
    "def scatter(x):\n",
    "    return np.dot(np.transpose(x - np.mean(x,0)),x - np.mean(x,0))\n",
    "def plotAnomalies(I,samples):\n",
    "    for k in range(2):\n",
    "        plt.figure(figsize=(fig_len,fig_wid))\n",
    "        ax=plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "        ax.tick_params(labelsize=20)\n",
    "        ax.set_facecolor('white')\n",
    "        ax.grid(color='k', linestyle='-.', linewidth=0.3)\n",
    "        plt.scatter(samples[I == k,0],samples[I == k,1],marker='*',s=m_size)\n",
    "def kmeans__(data,k,l,maxiters=100,eps=0.0001):\n",
    "\n",
    "    # select k cluster centers\n",
    "    C = data[np.random.permutation(range(data.shape[0]))[0:k],:]\n",
    "    objVal = 0\n",
    "    for jj in range(maxiters):\n",
    "        # compute distance of each point to the clusters\n",
    "        dMat = pdist2(data,C)\n",
    "        d = np.min(dMat,axis=1).flatten()\n",
    "        c = np.argmin(dMat,axis=1).flatten()\n",
    "        # sort points by distance to their closest center\n",
    "        inds = np.argsort(d)[::-1]\n",
    "        linds = inds[0:l]\n",
    "        cinds = inds[l+1:]\n",
    "        # extract the non-outlier data objects\n",
    "        ci = c[cinds]\n",
    "        # recompute the means\n",
    "        for kk in range(k):\n",
    "            C[kk,:] = np.mean(data[np.where(ci == kk)[0],:],axis=0)\n",
    "        # compute objective function\n",
    "        objVal_ = objVal\n",
    "        objVal = 0\n",
    "        for kk in range(k):\n",
    "            objVal += np.sum(pdist2(data[np.where(ci == kk)[0],:],C[kk,:]))\n",
    "        if np.abs(objVal - objVal_) < eps:\n",
    "            break\n",
    "    # one final time\n",
    "    dMat = pdist2(data,C)\n",
    "    c = np.argmin(dMat,axis=1).flatten()\n",
    "    return linds, C, c\n",
    "def pdist2(X,C):\n",
    "    if len(C.shape) == 1:\n",
    "        C = C[:,np.newaxis]\n",
    "    distMat = np.zeros([X.shape[0],C.shape[0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(C.shape[0]):\n",
    "            distMat[i,j] = np.linalg.norm(X[i,:] - C[j,:])\n",
    "    return distMat\n",
    "def precAtK(true,predicted):\n",
    "    # find number of anomalies\n",
    "    k = np.sum(true)\n",
    "#     print(\"k=\",k)\n",
    "    # find the score of the k^th predicted anomaly\n",
    "    v = np.sort(predicted,axis=0)[::-1][k-1]\n",
    "#     print(\"v=\",v)\n",
    "    # find all objects that are above the threshold\n",
    "    inds = np.where(predicted >= v)[0]\n",
    "#     print(\"inds=\",inds)\n",
    "#     print(\"np.sum(true[inds])=\",np.sum(true[inds]))\n",
    "#     print(\"len(inds)=\",len(inds))\n",
    "#     print(\"np.sum(true[inds])/len(inds)=\",np.sum(true[inds])/len(inds))\n",
    "    return float(np.sum(true[inds]))/float(len(inds))\n",
    "def averageRank(true,predicted):\n",
    "    inds = np.where(true == 1)[0]\n",
    "    s = np.argsort(predicted)[::-1]\n",
    "    v = []\n",
    "    for ind in inds:\n",
    "        v.append(float(np.where(s == ind)[0]+1))\n",
    "    return np.mean(v)\n",
    "def purity_score(y_true, y_pred):\n",
    "    \"\"\"Purity score\n",
    "\n",
    "    To compute purity, each cluster is assigned to the class which is most frequent \n",
    "    in the cluster [1], and then the accuracy of this assignment is measured by counting \n",
    "    the number of correctly assigned documents and dividing by the number of documents.\n",
    "    We suppose here that the ground truth labels are integers, the same with the predicted clusters i.e\n",
    "    the clusters index.\n",
    "\n",
    "    Args:\n",
    "        y_true(np.ndarray): n*1 matrix Ground truth labels\n",
    "        y_pred(np.ndarray): n*1 matrix Predicted clusters\n",
    "    \n",
    "    Returns:\n",
    "        float: Purity score\n",
    "    \n",
    "    References:\n",
    "        [1] https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html\n",
    "    \"\"\"\n",
    "    # matrix which will hold the majority-voted labels\n",
    "    y_voted_labels = np.zeros(y_true.shape)\n",
    "    # Ordering labels\n",
    "    ## Labels might be missing e.g with set like 0,2 where 1 is missing\n",
    "    ## First find the unique labels, then map the labels to an ordered set\n",
    "    ## 0,2 should become 0,1\n",
    "    labels = np.unique(y_true)\n",
    "    ordered_labels = np.arange(labels.shape[0])\n",
    "    for k in range(labels.shape[0]):\n",
    "        y_true[y_true==labels[k]] = ordered_labels[k]\n",
    "    # Update unique labels\n",
    "    labels = np.unique(y_true)\n",
    "    # We set the number of bins to be n_classes+2 so that \n",
    "    # we count the actual occurence of classes between two consecutive bin\n",
    "    # the bigger being excluded [bin_i, bin_i+1[\n",
    "    bins = np.concatenate((labels, [np.max(labels)+1]), axis=0)\n",
    "\n",
    "    for cluster in np.unique(y_pred):\n",
    "        hist, _ = np.histogram(y_true[y_pred==cluster], bins=bins)\n",
    "        # Find the most present label in the cluster\n",
    "        winner = np.argmax(hist)\n",
    "        y_voted_labels[y_pred==cluster] = winner\n",
    "    \n",
    "    return accuracy_score(y_true, y_voted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.936119Z",
     "start_time": "2020-05-20T04:31:46.881996Z"
    },
    "code_folding": [
     0,
     47,
     92,
     106,
     140,
     167,
     188,
     204,
     211,
     247,
     270,
     291,
     320,
     341,
     367
    ]
   },
   "outputs": [],
   "source": [
    "def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n",
    "    \"\"\"Estimate the log Gaussian probability.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "    precisions_chol : array-like\n",
    "        Cholesky decompositions of the precision matrices.\n",
    "        'full' : shape of (n_components, n_features, n_features)\n",
    "        'tied' : shape of (n_features, n_features)\n",
    "        'diag' : shape of (n_components, n_features)\n",
    "        'spherical' : shape of (n_components,)\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "    Returns\n",
    "    -------\n",
    "    log_prob : array, shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components, _ = means.shape\n",
    "    # det(precision_chol) is half of det(precision)\n",
    "    log_det = _compute_log_det_cholesky(\n",
    "        precisions_chol, covariance_type, n_features)\n",
    "\n",
    "    if covariance_type == 'full':\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n",
    "            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == 'tied':\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, mu in enumerate(means):\n",
    "            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == 'diag':\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (np.sum((means ** 2 * precisions), 1) -\n",
    "                    2. * np.dot(X, (means * precisions).T) +\n",
    "                    np.dot(X ** 2, precisions.T))\n",
    "\n",
    "    elif covariance_type == 'spherical':\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (np.sum(means ** 2, 1) * precisions -\n",
    "                    2 * np.dot(X, means.T * precisions) +\n",
    "                    np.outer(row_norms(X, squared=True), precisions))\n",
    "    return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n",
    "def _compute_precision_cholesky(covariances, covariance_type):\n",
    "    \"\"\"Compute the Cholesky decomposition of the precisions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    covariances : array-like\n",
    "        The covariance matrix of the current components.\n",
    "        The shape depends of the covariance_type.\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        The type of precision matrices.\n",
    "    Returns\n",
    "    -------\n",
    "    precisions_cholesky : array-like\n",
    "        The cholesky decomposition of sample precisions of the current\n",
    "        components. The shape depends of the covariance_type.\n",
    "    \"\"\"\n",
    "    estimate_precision_error_message = (\n",
    "        \"Fitting the mixture model failed because some components have \"\n",
    "        \"ill-defined empirical covariance (for instance caused by singleton \"\n",
    "        \"or collapsed samples). Try to decrease the number of components, \"\n",
    "        \"or increase reg_covar.\")\n",
    "\n",
    "    if covariance_type == 'full':\n",
    "        n_components, n_features, _ = covariances.shape\n",
    "        precisions_chol = np.empty((n_components, n_features, n_features))\n",
    "        for k, covariance in enumerate(covariances):\n",
    "            try:\n",
    "                cov_chol = linalg.cholesky(covariance, lower=True)\n",
    "            except linalg.LinAlgError:\n",
    "                raise ValueError(estimate_precision_error_message)\n",
    "            precisions_chol[k] = linalg.solve_triangular(cov_chol,\n",
    "                                                         np.eye(n_features),\n",
    "                                                         lower=True).T\n",
    "    elif covariance_type == 'tied':\n",
    "        _, n_features = covariances.shape\n",
    "        try:\n",
    "            cov_chol = linalg.cholesky(covariances, lower=True)\n",
    "        except linalg.LinAlgError:\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),\n",
    "                                                  lower=True).T\n",
    "    else:\n",
    "        if np.any(np.less_equal(covariances, 0.0)):\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = 1. / np.sqrt(covariances)\n",
    "    return precisions_chol\n",
    "def _estimate_log_prob(means_,precisions_cholesky_,covariance_type,degrees_of_freedom_,mean_precision_, X):\n",
    "        _, n_features = X.shape\n",
    "        # We remove `n_features * np.log(degrees_of_freedom_)` because\n",
    "        # the precision matrix is normalized\n",
    "        log_gauss = (_estimate_log_gaussian_prob(\n",
    "            X, means_, precisions_cholesky_, covariance_type) -\n",
    "            .5 * n_features * np.log(degrees_of_freedom_))\n",
    "\n",
    "        log_lambda = n_features * np.log(2.) + np.sum(digamma(\n",
    "            .5 * (degrees_of_freedom_ -\n",
    "                  np.arange(0, n_features)[:, np.newaxis])), 0)\n",
    "\n",
    "        return log_gauss + .5 * (log_lambda -\n",
    "                                 n_features / mean_precision_)\n",
    "def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n",
    "    \"\"\"Compute the log-det of the cholesky decomposition of matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_chol : array-like\n",
    "        Cholesky decompositions of the matrices.\n",
    "        'full' : shape of (n_components, n_features, n_features)\n",
    "        'tied' : shape of (n_features, n_features)\n",
    "        'diag' : shape of (n_components, n_features)\n",
    "        'spherical' : shape of (n_components,)\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    log_det_precision_chol : array-like, shape (n_components,)\n",
    "        The determinant of the precision matrix for each component.\n",
    "    \"\"\"\n",
    "    if covariance_type == 'full':\n",
    "        n_components, _, _ = matrix_chol.shape\n",
    "        log_det_chol = (np.sum(np.log(\n",
    "            matrix_chol.reshape(\n",
    "                n_components, -1)[:, ::n_features + 1]), 1))\n",
    "\n",
    "    elif covariance_type == 'tied':\n",
    "        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))\n",
    "\n",
    "    elif covariance_type == 'diag':\n",
    "        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))\n",
    "\n",
    "    else:\n",
    "        log_det_chol = n_features * (np.log(matrix_chol))\n",
    "\n",
    "    return log_det_chol\n",
    "def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):\n",
    "    \"\"\"Estimate the Gaussian distribution parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input data array.\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "        The responsibilities for each data sample in X.\n",
    "    reg_covar : float\n",
    "        The regularization added to the diagonal of the covariance matrices.\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        The type of precision matrices.\n",
    "    Returns\n",
    "    -------\n",
    "    nk : array-like, shape (n_components,)\n",
    "        The numbers of data samples in the current components.\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "        The centers of the current components.\n",
    "    covariances : array-like\n",
    "        The covariance matrix of the current components.\n",
    "        The shape depends of the covariance_type.\n",
    "    \"\"\"\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    covariances = {\"full\": _estimate_gaussian_covariances_full                   \n",
    "                  }[covariance_type](resp, X, nk, means, reg_covar)\n",
    "    return nk, means, covariances\n",
    "def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):\n",
    "    \"\"\"Estimate the full covariance matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "    nk : array-like, shape (n_components,)\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "    reg_covar : float\n",
    "    Returns\n",
    "    -------\n",
    "    covariances : array, shape (n_components, n_features, n_features)\n",
    "        The covariance matrix of the current components.\n",
    "    \"\"\"\n",
    "    n_components, n_features = means.shape\n",
    "    covariances = np.empty((n_components, n_features, n_features))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n",
    "        covariances[k].flat[::n_features + 1] += reg_covar\n",
    "    return covariances\n",
    "def initialization(X,K,numiters,r0,alpha=1):\n",
    "    ########################### Data preprocessing\n",
    "    X,z,thetas,N,params,D=preprocess(X,K)\n",
    "\n",
    "    clusters, sizes = np.unique(z, return_counts=True)\n",
    "    m_para=sizes/N\n",
    "    F=np.zeros(N)\n",
    "#     params=tuple((np.array(pd.DataFrame(X).mean()),((np.array(pd.DataFrame(X).cov()))), 1, D))\n",
    "    \n",
    "#     m_para,F=F_est(np.ones,N,N,thetas,params,X)\n",
    "    \n",
    "    threshold=0.3\n",
    "    \n",
    "    I=(np.random.binomial(1, threshold, N))\n",
    "        \n",
    "    return X,z,I,thetas,N,params,D,clusters,sizes,m_para,F,threshold\n",
    "def convergence_check(thetas,centroids_old,conv_criteria):\n",
    "    centroids=np.copy(np.array(list([thetas[i][0] for i in range(len(thetas))])))\n",
    "    if len(centroids)<len(centroids_old):\n",
    "        change=np.sum(list(np.min(np.abs(np.linalg.norm(centroids_old-centroids[i],axis=1))) for i in range(len(centroids))))\n",
    "    else:\n",
    "        change=np.sum(list(np.min(np.abs(np.linalg.norm(centroids_old[i]-centroids,axis=1))) for i in range(len(centroids_old))))\n",
    "    return change\n",
    "def preprocess(X,K):\n",
    "    \n",
    "# Try different mean precision prior ie params[2] : No difference\n",
    "# mean_precision_prior float | None, optional.\n",
    "# The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. \n",
    "# Larger values concentrate the cluster means around mean_prior. The value of the parameter must be greater \n",
    "# than 0. If it is None, it is set to 1.\n",
    "\n",
    "# Try different reg_covar : Too volatile\n",
    "    if type(X) == list:\n",
    "        X = np.array(X)\n",
    "    if len(X.shape) == 1:\n",
    "        X = X[:,np.newaxis]\n",
    "    N = X.shape[0] #rows: observations\n",
    "    D = X.shape[1] #columns: dimensions\n",
    "\n",
    "    # Fit your data on the scaler object\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "#     X=normalize(X)\n",
    "    # Initialize z\n",
    "#     z=np.random.randint(K,size=N)\n",
    "    z=KMeans(K).fit(X).predict(X)\n",
    "    z+=1\n",
    "\n",
    "    if D>1:\n",
    "        params = tuple((np.mean(X,axis=0),(np.cov(X.T)), 1, D))\n",
    "    elif D==1:\n",
    "        params = tuple((np.mean(X),(np.var(X.T)),1, D))\n",
    "\n",
    "    z=np.random.randint(K,size=N)\n",
    "\n",
    "    thetas=[normalinvwishartsample(params) for k in range(K)]\n",
    "\n",
    "    return X,z,thetas,N,params,D\n",
    "def remove_cluster_new(X,z,K,thetas,params):\n",
    "#     if len(thetas)>len(np.unique(np.abs(z))):\n",
    "#         print(len(thetas)-len(np.unique(np.abs(z))),\" clusters removed\", len(np.unique(np.abs(z))) )\n",
    "    N=len(z)\n",
    "    z_temp=np.copy(z)\n",
    "    clusters, sizes = np.unique(np.abs(z_temp), return_counts=True)\n",
    "\n",
    "    c2=pd.DataFrame(clusters).copy()\n",
    "    temp=c2.index.copy()+1\n",
    "    c2.index=c2[0].copy()\n",
    "    c2[0]=temp.copy()\n",
    "    z=np.multiply(np.copy(c2[0][np.abs(z_temp)]),np.sign(z_temp+0.5))\n",
    "    \n",
    "    clusters, sizes = np.unique(np.abs(z), return_counts=True)\n",
    "    K=len(clusters)\n",
    "    \n",
    "    return z,K,thetas\n",
    "def compute_mixture_pdf(means_,precisions_cholesky_,covariance_type,mean_precision_, X,N,sizes):\n",
    "    degrees_of_freedom_=sizes+X.shape[1]\n",
    "    log_probs=_estimate_log_prob(means_,precisions_cholesky_,covariance_type,degrees_of_freedom_,mean_precision_, X)\n",
    "    MN=(np.exp(log_probs))\n",
    "    F=np.dot(sizes/N,MN.T)\n",
    "    return degrees_of_freedom_,log_probs,MN,F\n",
    "def compute_cluster_params(z,X,params,clusters,sizes,ind_matrix,reg_covar,covariance_type):\n",
    "    K=(len(clusters))\n",
    "    N=len(z)\n",
    "    thetas=[]\n",
    "    for k in (clusters-1):\n",
    "        ind_k=np.where((z) == (k+1))[0]\n",
    "        c = len(ind_k)\n",
    "        if c<1:\n",
    "#             print(\"Group anomaly\")\n",
    "            ind_k=np.where(np.abs(z) == (k+1))[0]\n",
    "            c = len(ind_k)\n",
    "        thetas.append((_estimate_gaussian_parameters(X[ind_k], \n",
    "                                            np.ones((c,1)), reg_covar, covariance_type)[1:3]))    \n",
    "    nk=sizes\n",
    "    means_=np.array([thetas[k][0].T for k in clusters-1])[:,:,0]\n",
    "    covariances=np.array([thetas[k][1][0,:,:] for k in clusters-1])\n",
    "    para_tuple=nk,means_,covariances\n",
    "    precisions_cholesky_= np.array([_compute_precision_cholesky(cov, \n",
    "                                                covariance_type) for cov in [covariances]])[0,:,:,:]\n",
    "    \n",
    "    return para_tuple,thetas,nk,means_,covariances,precisions_cholesky_\n",
    "def update_anomaly_labels(N,F,u,K,z,threshold,ppsa):\n",
    "    prob=1-ppsa\n",
    "    I=np.array(list(np.random.binomial(1,prob[i],1)[0] for i in range(N)))\n",
    "    z=(np.abs(z)*np.power(-1,I))\n",
    "    I[z<0]=1\n",
    "    for k in range(K):\n",
    "        ind_k=((z) == k)\n",
    "        c=sum(ind_k)\n",
    "        if (c<0.1*N) or ((c>0) and (np.mean(I[ind_k])>=0.75)):\n",
    "            I[ind_k]=1\n",
    "            z[ind_k]=np.abs(z[ind_k])*(-1)\n",
    "    #     else:\n",
    "    #         I[ind_k]=np.zeros(c)\n",
    "    #         z[ind_k]=np.abs(z[ind_k])\n",
    "\n",
    "    if np.mean(I)>0.3:\n",
    "        I[np.argsort(F)[np.int(0.3*N):]]=0\n",
    "        z[np.argsort(F)[np.int(0.3*N):]]=np.abs(z[np.argsort(F)[np.int(0.3*N):]])\n",
    "        \n",
    "#     if np.sum(z<0)<(0.01*N):\n",
    "    if np.sum(z<0)==0:\n",
    "        min_ana_count=max(np.int(0.01*N),10)\n",
    "        I[np.argsort(F)[:min_ana_count]]=1\n",
    "        z[np.argsort(F)[:min_ana_count]]=np.abs(z[np.argsort(F)[:min_ana_count]])*(-1)\n",
    "#         print(\"min_ana_count\",min_ana_count, \"sum(I)\", sum(I))\n",
    "\n",
    "    threshold=min(max(beta.ppf(threshold , sum(I)+1,N-sum(I)+1),4/N,0.01), 0.3)\n",
    "\n",
    "    return prob,I,threshold,z \n",
    "def _log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features):\n",
    "    \"\"\"Compute the log of the Wishart distribution normalization term.\n",
    "    Parameters\n",
    "    ----------\n",
    "    degrees_of_freedom : array-like, shape (n_components,)\n",
    "        The number of degrees of freedom on the covariance Wishart\n",
    "        distributions.\n",
    "    log_det_precision_chol : array-like, shape (n_components,)\n",
    "         The determinant of the precision matrix for each component.\n",
    "    n_features : int\n",
    "        The number of features.\n",
    "    Return\n",
    "    ------\n",
    "    log_wishart_norm : array-like, shape (n_components,)\n",
    "        The log normalization of the Wishart distribution.\n",
    "    \"\"\"\n",
    "    # To simplify the computation we have removed the np.log(np.pi) term\n",
    "    return -(degrees_of_freedom * log_det_precisions_chol +\n",
    "             degrees_of_freedom * n_features * .5 * math.log(2.) +\n",
    "             np.sum(gammaln(.5 * (degrees_of_freedom -\n",
    "                                  np.arange(n_features)[:, np.newaxis])), 0))\n",
    "def compute_log_likelihhod(z,sizes,K,precisions_cholesky_,covariance_type,features,degrees_of_freedom_,\n",
    "                           mean_precision_):\n",
    "        # Contrary to the original formula, we have done some simplification\n",
    "        # and removed all the constant terms.\n",
    "        log_resp=np.log(np.abs(z))\n",
    "        weight_concentration_ = (\n",
    "                1. + sizes,\n",
    "                (1/K +\n",
    "                 np.hstack((np.cumsum(sizes[::-1])[-2::-1], 0))))\n",
    "\n",
    "        # We removed `.5 * features * np.log(degrees_of_freedom_)`\n",
    "        # because the precision matrix is normalized.\n",
    "        log_det_precisions_chol = (_compute_log_det_cholesky(\n",
    "            precisions_cholesky_, covariance_type, features) -\n",
    "            .5 * features * np.log(degrees_of_freedom_))\n",
    "\n",
    "        log_wishart = np.sum(_log_wishart_norm(\n",
    "            degrees_of_freedom_, log_det_precisions_chol, features))\n",
    "        \n",
    "        log_norm_weight = -np.sum(betaln(weight_concentration_[0],\n",
    "                                         weight_concentration_[1]))\n",
    "\n",
    "        curr_log_likelihood=(-np.sum(np.exp(log_resp) * log_resp) -\n",
    "                log_wishart - log_norm_weight -\n",
    "                0.5 * features * np.sum(np.log(mean_precision_)))\n",
    "        return curr_log_likelihood\n",
    "def ppsa_vals(F,I,threshold):\n",
    "    N=len(F)\n",
    "    u=np.unique(F)\n",
    "    ps1=F\n",
    "    domain=((u>np.quantile(F,0.01))*1==(u<np.quantile(F,0.3))*1)\n",
    "    G_Y_domain=u*domain\n",
    "    G_Y_domain=(G_Y_domain[G_Y_domain>0])\n",
    "\n",
    "    G_Y=domain*[np.sum(F[(F<=u)]) for n,u in enumerate(np.unique(F))]\n",
    "    G_Y=G_Y[G_Y>0]\n",
    "    G_Y=np.array(G_Y/max(G_Y))\n",
    "    \n",
    "    g_Y=np.diff(G_Y)/np.diff(G_Y_domain)\n",
    "\n",
    "    aa=np.array(list(stats.percentileofscore(F, i)/100 for i in np.unique(F)))\n",
    "    th3=aa[aa<0.3][np.argmax(np.abs(np.diff(np.quantile(G_Y,aa[aa<0.3]))))]\n",
    "#     th3=threshold\n",
    "    len_tail=np.int(th3*N) #drop in F\n",
    "\n",
    "    u=np.quantile(ps1, th3)\n",
    "    inds = np.where(ps1<=u)[0]\n",
    "\n",
    "    iz=np.union1d(inds, np.where(I==1)[0])\n",
    "#     inds0=iz[np.argsort(ps1[iz])[:min(np.int(0.3*N),len(iz),np.int(th3*N))]]\n",
    "    inds0=np.argsort(ps1)[:min(np.int(0.3*N),len(iz),np.int(th3*N))]\n",
    "    psa = np.abs(ps1[inds0] - u) \n",
    "\n",
    "    gpdparams = stats.genpareto.fit(psa)\n",
    "    i_ppsa=np.zeros(N)\n",
    "    \n",
    "    i_ppsa[inds0] = stats.genpareto(1,0,gpdparams[2]).cdf(psa)\n",
    "    ppsa=np.ones(N)\n",
    "    ppsa[inds0]=1-(i_ppsa[inds0])\n",
    "    \n",
    "#     i_ppsa[iz] = stats.genpareto(1,0,gpdparams[2]).cdf(np.abs(ps1[iz] - u))\n",
    "#     ppsa=np.ones(N)\n",
    "#     ppsa[iz]=1-(i_ppsa[iz])\n",
    "    \n",
    "    ppsa[ppsa>1]=1\n",
    "    ppsa[ppsa==0]=sys.float_info.min\n",
    "    return ppsa,i_ppsa, inds, inds0,u,F, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:46.992016Z",
     "start_time": "2020-05-20T04:31:46.955901Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def incad_new_labels5(X,K,numiters,r0,conv_criteria):\n",
    "# if 1:\n",
    "    # initialization\n",
    "    start_time=time.time()\n",
    "    \n",
    "    output={}\n",
    "\n",
    "    log_likelihood=[]\n",
    "    converged_=0\n",
    "    covariance_type=\"full\"\n",
    "    \n",
    "    X,z,I,thetas,N,params,features,clusters,sizes,m_para,F,threshold = initialization(X,K,numiters,r0,alpha=1)\n",
    "    \n",
    "    DistanceMatrix = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(X, 'euclidean'))\n",
    "    \n",
    "    mean_precision_=params[2]\n",
    "    \n",
    "    reg_covar=np.power(np.unique(DistanceMatrix)[1],1)/2\n",
    "\n",
    "    niw_mat=np.array(list(normalinvwishartmarginal(X[i:i+1],\n",
    "                         tuple((np.mean(X[i:i+1],axis=0),params[1],params[2],params[3]))\n",
    "                                                  ) for i in range(N)))\n",
    "\n",
    "    clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "    \n",
    "    ind_matrix=np.array(list(1*(np.abs(z).astype('int')==c) for c in clusters.astype('int'))) # all points  \n",
    "    \n",
    "    para_tuple,thetas,nk,means_,covariances,precisions_cholesky_= compute_cluster_params(z,X,\n",
    "                                                    params,clusters,sizes,ind_matrix,reg_covar,covariance_type)\n",
    "    \n",
    "    degrees_of_freedom_,log_probs,MN,F = compute_mixture_pdf(means_,precisions_cholesky_,\n",
    "                                         covariance_type,mean_precision_, X,N,sizes)\n",
    "    \n",
    "    if r0:\n",
    "        ppsa,i_ppsa, inds, inds0,u,F, threshold = ppsa_vals(F,I,threshold)\n",
    "        alpha2=1/(ppsa**r0)\n",
    "        prob,I,threshold,z = update_anomaly_labels(N,F,u,K,z,threshold,ppsa)\n",
    "        \n",
    "    else:\n",
    "        alpha2=np.ones(N)\n",
    "        prob=np.zeros(N)\n",
    "        ppsa=i_ppsa=inds=inds0=[]\n",
    "        u=0\n",
    "        threshold=0\n",
    "        \n",
    "    init_time=time.time()\n",
    "\n",
    "    for n in range(numiters):\n",
    "        sys.stdout.write('*'); sys.stdout.flush()\n",
    "#         print(sizes)\n",
    "\n",
    "        ps_new_clust=((alpha2/(N + alpha2 - 1))*niw_mat)[:,np.newaxis]\n",
    "\n",
    "        ps_log=np.array([np.hstack(((np.log((sizes-(clusters==k0))[:,np.newaxis])-np.log(N+alpha2-1)).T+log_probs,\n",
    "                    np.log(ps_new_clust))) for k0 in clusters])\n",
    "        \n",
    "        z=np.array(list(((1+np.argmax(np.random.multinomial(1, \n",
    "                (np.exp(ps_log[np.int(np.abs(z[i])-1),i,:])+sys.float_info.min)/np.sum(\n",
    "                    np.exp(ps_log[np.int(np.abs(z[i])-1),i,:])+sys.float_info.min), \n",
    "                            size=1)))*np.power(-1,np.random.binomial(1,prob[i],1)[0])) for i in range(N)))\n",
    "        \n",
    "        # Update labels : drop empty clusters \n",
    "        z,K,thetas = remove_cluster_new(X,z,K,thetas,params)\n",
    "\n",
    "        clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "        \n",
    "        ind_matrix=np.array(list(1*(np.abs(z).astype('int')==c) for c in clusters.astype('int'))) # all points  \n",
    "\n",
    "        para_tuple,thetas,nk,means_,covariances,precisions_cholesky_= compute_cluster_params(z,X,\n",
    "                                                        params,clusters,sizes,ind_matrix,reg_covar,covariance_type)\n",
    "\n",
    "        degrees_of_freedom_,log_probs,MN,F = compute_mixture_pdf(means_,precisions_cholesky_,\n",
    "                                             covariance_type,mean_precision_, X,N,sizes)\n",
    "\n",
    "        if r0:\n",
    "            ppsa,i_ppsa, inds, inds0,u,F, threshold = ppsa_vals(F,I,threshold)\n",
    "            alpha2=1/(ppsa**r0)\n",
    "            prob,I,threshold,z = update_anomaly_labels(N,F,u,K,z,threshold,ppsa)\n",
    "            \n",
    "        \n",
    "        log_likelihood.append(compute_log_likelihhod(z,sizes,K,precisions_cholesky_,covariance_type,features,degrees_of_freedom_,\n",
    "                           mean_precision_))\n",
    "        \n",
    "        if n > 50 and (np.max(np.abs(np.diff(log_likelihood[-3:])))<conv_criteria):\n",
    "            converged_ += 1\n",
    "            if converged_>0:\n",
    "                print(\"Converged\")\n",
    "                break\n",
    "                \n",
    "    batch_time=time.time()\n",
    "            \n",
    "    output={}\n",
    "    output['n']=n\n",
    "    output['X']=np.copy(X)\n",
    "    output['time_lapsed_init_ms']=(init_time-start_time)*1000.0\n",
    "    output['time_lapsed_stream_ms']=(time.time()-batch_time)*1000.0\n",
    "    output['time_lapsed_batch_ms']=(batch_time-init_time)*1000.0\n",
    "    output['z']=np.copy(z)\n",
    "    output['u']=np.copy(u)\n",
    "    output['K']=copy.copy(K)\n",
    "    output['r0']=copy.copy(r0)\n",
    "    output['F']=np.copy(F)\n",
    "    output['I']=np.copy(I)\n",
    "    output['prob']=np.copy(prob)\n",
    "    output['alpha2']=np.copy(alpha2)\n",
    "    output['thetas']=thetas[:]\n",
    "    output['log_likelihood']=copy.copy(log_likelihood)\n",
    "    output['threshold']=copy.copy(threshold)\n",
    "    output['ppsa']=np.copy(ppsa)\n",
    "    output['i_ppsa']=np.copy(i_ppsa)\n",
    "    output['inds']=np.copy(inds)\n",
    "    output['inds0']=np.copy(inds0) \n",
    "    output['converged_']=converged_\n",
    "    output['niw_mat']=np.copy(niw_mat)\n",
    "    output['DistanceMatrix']=np.copy(DistanceMatrix)\n",
    "    output['reg_covar']=np.copy(reg_covar)\n",
    "    output['ind_matrix']=np.copy(ind_matrix)\n",
    "    output['para_tuple']=copy.copy(para_tuple)\n",
    "    output['means_']=np.copy(means_)\n",
    "    output['covariances']=np.copy(covariances)\n",
    "    output['precisions_cholesky_']=np.copy(precisions_cholesky_)\n",
    "    output['degrees_of_freedom_']=np.copy(degrees_of_freedom_)\n",
    "    output['log_probs']=np.copy(log_probs)\n",
    "    output['MN']=np.copy(MN)\n",
    "    output['params']=np.copy(params)\n",
    "    \n",
    "    return output           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:47.061456Z",
     "start_time": "2020-05-20T04:31:47.030617Z"
    },
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "def incad_new_labels_stream(X_full,K,batch_proportion,numiters,r0,conv_criteria):\n",
    "# if 1:\n",
    "    if 1:\n",
    "        start_time=time.time()\n",
    "        output={}\n",
    "\n",
    "        log_likelihood=[]\n",
    "        converged_=0\n",
    "        covariance_type=\"full\"\n",
    "\n",
    "        batch_size=np.int(X_full.shape[0]*batch_proportion)\n",
    "\n",
    "        N_full,D=X_full.shape\n",
    "\n",
    "        N=copy.copy(batch_size)\n",
    "\n",
    "        X=X_full[:batch_size]\n",
    "        \n",
    "        batch_output=incad_new_labels5(X,K,numiters,r0,conv_criteria)\n",
    "        \n",
    "        n=batch_output['n']\n",
    "        X=batch_output['X']\n",
    "        time_lapsed_init_ms=batch_output['time_lapsed_init_ms']\n",
    "        time_lapsed_stream_ms=batch_output['time_lapsed_stream_ms']\n",
    "        time_lapsed_batch_ms=batch_output['time_lapsed_batch_ms']\n",
    "        z=batch_output['z']\n",
    "        u=batch_output['u']\n",
    "        K=batch_output['K']\n",
    "        r0=batch_output['r0']\n",
    "        F=batch_output['F']\n",
    "        I=batch_output['I']\n",
    "        prob=batch_output['prob']\n",
    "        alpha2=batch_output['alpha2']\n",
    "        thetas=batch_output['thetas']\n",
    "        log_likelihood=batch_output['log_likelihood']\n",
    "        threshold=batch_output['threshold']\n",
    "        ppsa=batch_output['ppsa']\n",
    "        i_ppsa=batch_output['i_ppsa']\n",
    "        inds=batch_output['inds']\n",
    "        inds0=batch_output['inds0'] \n",
    "        converged_=batch_output['converged_']\n",
    "        niw_mat=batch_output['niw_mat']\n",
    "        DistanceMatrix=batch_output['DistanceMatrix']\n",
    "        reg_covar=batch_output['reg_covar']\n",
    "        ind_matrix=batch_output['ind_matrix']\n",
    "        para_tuple=batch_output['para_tuple']\n",
    "        means_=batch_output['means_']\n",
    "        covariances=batch_output['covariances']\n",
    "        precisions_cholesky_=batch_output['precisions_cholesky_']\n",
    "        degrees_of_freedom_=batch_output['degrees_of_freedom_']\n",
    "        log_probs=batch_output['log_probs']\n",
    "        MN=batch_output['MN']\n",
    "        params=batch_output['params']\n",
    "\n",
    "\n",
    "        mean_precision_=params[2]\n",
    "        \n",
    "        clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "\n",
    "        batch_time=time.time()\n",
    "    \n",
    "    print(\"Begin Streaming\")\n",
    "    \n",
    "    for i in np.arange(batch_size,N_full,1):\n",
    "        sys.stdout.write('*'); sys.stdout.flush()\n",
    "#         if i%100==0:\n",
    "#             print(sizes)\n",
    "        \n",
    "        # Need full obs info        \n",
    "        X=X_full[:i+1]\n",
    "        \n",
    "        if type(X) == list:\n",
    "            X = np.array(X)\n",
    "        if len(X.shape) == 1:\n",
    "            X = X[:,np.newaxis]\n",
    "\n",
    "        # Fit your data on the scaler object\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        if i%(0.1*N)==0:\n",
    "            if D>1:\n",
    "                params = tuple((np.mean(X,axis=0),(np.cov(X.T)), 1, D))\n",
    "            elif D==1:\n",
    "                params = tuple((np.mean(X),(np.var(X.T)),1, D))\n",
    "            DistanceMatrix = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(X[-np.int(0.1*N+1):,:],\n",
    "                                                                                            'euclidean'))\n",
    "            mean_precision_=params[2]\n",
    "            reg_covar=np.min([reg_covar,np.power(np.unique(DistanceMatrix)[1],1)/2])\n",
    "\n",
    "        z=np.append(z,K+1)\n",
    "        I=np.append(I,0)\n",
    "        prob=np.append(prob,0)\n",
    "        N=N+1\n",
    "        niw_mat=np.append(niw_mat,normalinvwishartmarginal(X[-1][:,np.newaxis],\n",
    "                                        tuple((X[-1],params[1],params[2],params[3]))))\n",
    "\n",
    "        clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "\n",
    "        ind_matrix=np.array(list(1*(np.abs(z).astype('int')==c) for c in clusters.astype('int'))) # all points  \n",
    "\n",
    "        para_tuple,thetas,nk,means_,covariances,precisions_cholesky_= compute_cluster_params(z,X,\n",
    "                                                        params,clusters,sizes,ind_matrix,reg_covar,covariance_type)\n",
    "\n",
    "        degrees_of_freedom_,log_probs,MN,F = compute_mixture_pdf(means_,precisions_cholesky_,\n",
    "                                             covariance_type,mean_precision_, X,N,sizes)\n",
    "        \n",
    "        \n",
    "        if r0:\n",
    "            ppsa,i_ppsa, inds, inds0,u,F, threshold = ppsa_vals(F,I,threshold)\n",
    "            alpha2=1/(ppsa**r0)\n",
    "            prob,I,threshold,z = update_anomaly_labels(N,F,u,K,z,threshold,ppsa)\n",
    "        \n",
    "        \n",
    "        # Update only tail points\n",
    "        inds_update=np.union1d(inds0,np.where(I==1))\n",
    "        inds_update=np.union1d(inds_update,i)\n",
    "        \n",
    "\n",
    "        ps_new_clust=((alpha2[inds_update]/(N + alpha2[inds_update] - 1))*niw_mat[inds_update])[:,np.newaxis]\n",
    "\n",
    "        ps_log=np.array([np.hstack(((np.log((sizes-(clusters==k0))[:,np.newaxis])-np.log(N+alpha2[inds_update]-1)).T\n",
    "                                    +log_probs[inds_update],\n",
    "                    np.log(ps_new_clust))) for k0 in clusters])\n",
    "        \n",
    "        ps_log+=sys.float_info.min\n",
    "        \n",
    "        z[inds_update]=np.array(list(((1+np.argmax(np.random.multinomial(1, \n",
    "                (np.exp(ps_log[np.int(np.abs(z[i])-1),ii,:])+sys.float_info.min)/np.sum(\n",
    "                    np.exp(ps_log[np.int(np.abs(z[i])-1),ii,:])+sys.float_info.min), \n",
    "                            size=1)))*np.power(-1,np.random.binomial(1,prob[i],1)[0])) \n",
    "                                      for ii,i in enumerate(inds_update)))\n",
    "        \n",
    "        # Update labels : drop empty clusters \n",
    "        z,K,thetas = remove_cluster_new(X,z,K,thetas,params)\n",
    "\n",
    "    clusters,sizes=np.unique(np.abs(z).astype('int'),return_counts=True)\n",
    "\n",
    "    ind_matrix=np.array(list(1*(np.abs(z).astype('int')==c) for c in clusters.astype('int'))) # all points  \n",
    "\n",
    "    para_tuple,thetas,nk,means_,covariances,precisions_cholesky_= compute_cluster_params(z,X,\n",
    "                                                    params,clusters,sizes,ind_matrix,reg_covar,covariance_type)\n",
    "\n",
    "    degrees_of_freedom_,log_probs,MN,F = compute_mixture_pdf(means_,precisions_cholesky_,\n",
    "                                         covariance_type,mean_precision_, X,N,sizes)\n",
    "\n",
    "    output={}\n",
    "    output['n']=n\n",
    "    output['X']=np.copy(X)\n",
    "    output['time_lapsed_init_ms']=time_lapsed_init_ms\n",
    "    output['time_lapsed_stream_ms']=(time.time()-batch_time)*1000.0\n",
    "    output['time_lapsed_batch_ms']=time_lapsed_batch_ms\n",
    "    output['z']=np.copy(z)\n",
    "    output['u']=np.copy(u)\n",
    "    output['K']=copy.copy(K)\n",
    "    output['r0']=copy.copy(r0)\n",
    "    output['F']=np.copy(F)\n",
    "    output['I']=np.copy(I)\n",
    "    output['prob']=np.copy(prob)\n",
    "    output['alpha2']=np.copy(alpha2)\n",
    "    output['thetas']=thetas[:]\n",
    "    output['log_likelihood']=copy.copy(log_likelihood)\n",
    "    output['threshold']=copy.copy(threshold)\n",
    "    output['ppsa']=np.copy(ppsa)\n",
    "    output['i_ppsa']=np.copy(i_ppsa)\n",
    "    output['inds']=np.copy(inds)\n",
    "    output['inds0']=np.copy(inds0) \n",
    "    output['converged_']=converged_\n",
    "    output['niw_mat']=np.copy(niw_mat)\n",
    "    output['DistanceMatrix']=np.copy(DistanceMatrix)\n",
    "    output['reg_covar']=np.copy(reg_covar)\n",
    "    output['ind_matrix']=np.copy(ind_matrix)\n",
    "    output['para_tuple']=copy.copy(para_tuple)\n",
    "    output['means_']=np.copy(means_)\n",
    "    output['covariances']=np.copy(covariances)\n",
    "    output['precisions_cholesky_']=np.copy(precisions_cholesky_)\n",
    "    output['degrees_of_freedom_']=np.copy(degrees_of_freedom_)\n",
    "    output['log_probs']=np.copy(log_probs)\n",
    "    output['MN']=np.copy(MN)\n",
    "    output['params']=np.copy(params)\n",
    "    \n",
    "    return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:49.358322Z",
     "start_time": "2020-05-20T04:31:49.355417Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:49.689072Z",
     "start_time": "2020-05-20T04:31:49.681081Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# date_label=datetime.now().strftime(\"_%m_%d_%Y\")\n",
    "date_label='_05_11_2020'\n",
    "newpath = os.getcwd()+'/'+date_label\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "data_source_ad=os.getcwd()+'/Data/AD/'\n",
    "data_source_clust=os.getcwd()+'/Data/Clustering/'\n",
    "data_source_stream=os.getcwd()+'/Data/Stream/'\n",
    "data_source_large=os.getcwd()+'/Data/Stream/Large'\n",
    "if not os.path.exists(data_source_large):\n",
    "    os.makedirs(data_source_large)\n",
    "\n",
    "results_path_stream=newpath+'/Results/streaming/'\n",
    "results_path_non_stream_ad=newpath+'/Results/non_streaming/AD/'\n",
    "results_path_non_stream_clust=newpath+'/Results/non_streaming/Clustering/'\n",
    "\n",
    "image_path_stream=results_path_stream+'images/'\n",
    "image_path_non_stream_ad=results_path_non_stream_ad+'images/'\n",
    "image_path_non_stream_clust=results_path_non_stream_clust+'images/'\n",
    "\n",
    "if not os.path.exists(image_path_stream):\n",
    "    os.makedirs(image_path_stream)\n",
    "if not os.path.exists(image_path_non_stream_ad):\n",
    "    os.makedirs(image_path_non_stream_ad)\n",
    "if not os.path.exists(image_path_non_stream_clust):\n",
    "    os.makedirs(image_path_non_stream_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:51.120796Z",
     "start_time": "2020-05-20T04:31:51.105902Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "def load_data(file_path):\n",
    "    filename, extension = os.path.splitext(file_path)\n",
    "    name=(os.path.basename(file_path))\n",
    "    if (extension=='.mat'):\n",
    "        try:\n",
    "            mat = scipy.io.loadmat(file_path)\n",
    "            df = pd.DataFrame(np.hstack((mat['X'], mat['y'])))\n",
    "            X,y=mat['X'], mat['y']\n",
    "            return df,X,y\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                arrays = {}\n",
    "                f = h5py.File(file_path)\n",
    "                for k, v in f.items():\n",
    "                    arrays[k] = np.array(v)\n",
    "                X,y=arrays['X'].T,arrays['y'].T\n",
    "                df = pd.DataFrame(np.hstack((arrays['X'].T, arrays['y'].T)))\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"1 Failed to load\", name)\n",
    "\n",
    "    elif extension=='.csv':\n",
    "        try:\n",
    "            df = pd.read_csv(file_path,low_memory=False,delimiter=',',header=None)\n",
    "            df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "            if df1.shape[1]==0:\n",
    "                df = pd.read_csv(file_path,low_memory=False,delimiter=',')\n",
    "                df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]                \n",
    "            X=np.array(df1).astype(float)\n",
    "            y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "            return df,X,y\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path,low_memory=False,delimiter=',')\n",
    "                df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "                X=np.array(df1).astype(float)\n",
    "                y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"2 Failed to load\", name)\n",
    "\n",
    "    elif extension=='.pickle':\n",
    "        try:\n",
    "            data=pickle.load(open(file_path,'rb'))\n",
    "            X=data['X']\n",
    "            y=data['y']\n",
    "            return data,X,y\n",
    "        except:\n",
    "            try:\n",
    "                d = pickle.load( open(file_path, \"rb\" ) )\n",
    "                data=d['rawdata']\n",
    "                labels=d['labels']\n",
    "                X=np.array(data).astype(float)\n",
    "                y=labels\n",
    "                df=np.hstack((X,y))\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"3 Failed to load\", name)\n",
    "                \n",
    "    elif extension=='.arff':\n",
    "        data = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data[0])\n",
    "        df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "        X=np.array(df1).astype(float)\n",
    "        y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "        return df,X,y\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to load the extension\",extension)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Non Stream- AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:22:16.750783Z",
     "start_time": "2020-05-11T13:39:18.298848Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http.mat (567498, 3)\n",
      "http.mat\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/http.mat' already exists\n",
      "pima.mat (768, 8)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************wine.mat (129, 13)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cardio.mat (1831, 21)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************ecoli.mat (336, 7)\n",
      "ecoli.mat\n",
      "Input contains NaN, infinity or a value too large for dtype('<f8').\n",
      "Failed to load the extension \n",
      ".DS_Store\n",
      "cannot unpack non-iterable NoneType object\n",
      "pendigits.mat (6870, 16)\n",
      "*********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************breast-cancer-unsupervised-ad.csv (367, 30)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************shuttle.mat (49097, 9)\n",
      "shuttle.mat\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/shuttle.mat' already exists\n",
      "letter.mat (1600, 32)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************smtp.pickle (113, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************shuttle-unsupervised-ad.csv (46464, 9)\n",
      "shuttle-unsupervised-ad.csv\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/shuttle-unsupervised-ad.csv' already exists\n",
      "thyroid.mat (3772, 6)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************seismic-bumps.arff (2584, 11)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************vowels.mat (1456, 12)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************breastw.mat (683, 9)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************lympho.mat (148, 18)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************annthyroid.mat (7200, 6)\n",
      "******************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************pen-global-unsupervised-ad.csv (809, 16)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************ionosphere.mat (351, 33)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************wbc.mat (378, 30)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cover.pickle (217, 10)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************http.pickle (110, 3)\n",
      "http.pickle\n",
      "1-th leading minor of the array is not positive definite\n",
      "smtp.mat (95156, 3)\n",
      "smtp.mat\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/smtp.mat' already exists\n",
      "mammography.mat (11183, 6)\n",
      "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************glass.mat (214, 9)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cover.mat (286048, 10)\n",
      "cover.mat\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/cover.mat' already exists\n",
      "kdd99-unsupervised-ad.csv (620098, 29)\n",
      "kdd99-unsupervised-ad.csv\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/kdd99-unsupervised-ad.csv' already exists\n",
      "vertebral.mat (240, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************************************************************************************************************************************************************************************************letter-unsupervised-ad.csv (1600, 32)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************large_dataset_count,error_count,len(onlyfiles) 6 9 30\n"
     ]
    }
   ],
   "source": [
    "# Anomaly Detection Non-Stream\n",
    "onlyfiles = [f for f in listdir(data_source_ad) if isfile(join(data_source_ad, f))]\n",
    "error_count=0\n",
    "large_dataset_count=0\n",
    "max_size=20001\n",
    "\n",
    "K=10\n",
    "r0=1\n",
    "conv_criteria=0.1\n",
    "\n",
    "for f in onlyfiles:\n",
    "    file_path=data_source_ad+f\n",
    "    try:\n",
    "        df,X,y=load_data(file_path)\n",
    "        XX=pd.DataFrame(X)\n",
    "        X=XX[XX.columns[(XX.var()!=0)]]\n",
    "        print(f,X.shape)\n",
    "        numiters=np.max([np.int(len(X)/16),300])\n",
    "        if X.shape[0]<max_size and X.shape[1]<=35:\n",
    "            if X.shape[0]/X.shape[1]<5 or X.shape[1]>35:\n",
    "                for n_comp in np.arange(25,X.shape[1],5):\n",
    "                    pca = PCA(n_components=n_comp)\n",
    "                    principalComponents = pca.fit_transform(X)\n",
    "                    principalDf = pd.DataFrame(data = principalComponents)\n",
    "                    if np.sum(pca.explained_variance_ratio_)>0.90 or n_comp==35:\n",
    "                        break    \n",
    "                X=np.array(principalDf)\n",
    "            output=incad_new_labels5(X,K,numiters,r0,conv_criteria)\n",
    "            output['df']=df\n",
    "            output['y']=y\n",
    "            pickle.dump( output, open(results_path_non_stream_ad+f+\".pickle\", \"wb\" ))\n",
    "\n",
    "        else:\n",
    "            large_dataset_count+=1\n",
    "            shutil.move(file_path, data_source_large)\n",
    "    except Exception as e:\n",
    "        error_count+=1\n",
    "        print(f)\n",
    "        print(e)\n",
    "print(\"large_dataset_count,error_count,len(onlyfiles)\",large_dataset_count,error_count,len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### Non Stream- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T01:04:04.289441Z",
     "start_time": "2020-05-11T14:22:16.817000Z"
    },
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk-4000n.arff (4000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************disk-5000n.arff (5000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************zelnik4.arff (622, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************chainlink.arff (1000, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************st900.arff (900, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************square3.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************sizes5.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************2d-10c.arff (2990, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************s-set1.arff (5000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************spiral.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************2d-4c-no9.arff (876, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************DS-577.arff (577, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************dpb.arff (4000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************tetra.arff (400, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************dpc.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************ds4c2sc8.arff (485, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************spiralsquare.arff (1500, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************rings.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************pathbased.arff (300, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************shapes.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cure-t1-2000n-2D.arff (2000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************sizes4.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************square2.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************dartboard1.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************2d-4c-no4.arff (863, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************zelnik5.arff (512, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************atom.arff (800, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************smile2.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cluto-t5-8k.arff (8000, 2)\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************zelnik2.arff (303, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************long2.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************square5.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************triangle1.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************sizes3.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************donut1.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************R15.arff (600, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************birch-rg1.arff (100000, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/birch-rg1.arff' already exists\n",
      "2d-4c.arff (1261, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cuboids.arff (1002, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************longsquare.arff (900, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************curves2.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************spherical_4_3.arff (400, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************disk-3000n.arff (3000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************twenty.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************sizes2.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************square4.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************golfball.arff (4002, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************spherical_6_2.arff (300, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************long3.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************zelnik3.arff (266, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************jain.arff (373, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************************************************************************************************************************************************************************************************flame.arff (240, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************simplex.arff (500, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************smile3.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cluto-t4-8k.arff (8000, 2)\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************compound.arff (399, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************dense-disk-5000.arff (5000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************insect.arff (30, 3)\n",
      "list index out of range\n",
      "cure-t2-4k.arff (4200, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************aggregation.arff (788, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************complex8.arff (2551, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************curves1.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************impossible.arff (3673, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cluto-t8-8k.arff (8000, 2)\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************birch-rg3.arff (100000, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/birch-rg3.arff' already exists\n",
      "donut3.arff (999, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************banana.arff (4811, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************twodiamonds.arff (800, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************sizes1.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************target.arff (770, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************wingnut.arff (1016, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************zelnik1.arff (299, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************long1.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************xclara.arff (3000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************diamond9.arff (3000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************elliptical_10_2.arff (500, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************triangle2.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************ds3c3sc6.arff (905, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************donut2.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************s-set4.arff (5000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************birch-rg2.arff (100000, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/birch-rg2.arff' already exists\n",
      "ds2c2sc13.arff (588, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************pmf.arff (649, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************disk-4600n.arff (4600, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************disk-6000n.arff (6000, 2)\n",
      "***************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************elly-2d10c13s.arff (2796, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************complex9.arff (3031, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************smile1.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************fourty.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cluto-t7-10k.arff (10000, 2)\n",
      "*************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************spherical_5_2.arff (250, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************s-set3.arff (5000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************dartboard2.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************square1.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************2d-3c-no123.arff (715, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************threenorm.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************hypercube.arff (800, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************zelnik6.arff (238, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************circle.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************gaussians1.arff (100, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************************************************************************************************************************************************************************************************disk-1000n.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************engytime.arff (4096, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************dense-disk-3000.arff (3000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cure-t0-2000n-2D.arff (2000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************blobs.arff (300, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************mopsi-finland.arff (13467, 2)\n",
      "*************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************hepta.arff (212, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************D31.arff (3100, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************donutcurves.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************xor.arff (1000, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************aml28.arff (804, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************3MC.arff (400, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************2dnormals.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************DS-850.arff (850, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************cassini.arff (1000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************2d-20c-no0.arff (1517, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************mopsi-joensuu.arff (4590, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************2sp2glob.arff (2000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************s-set2.arff (5000, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************disk-4500n.arff (4500, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************3-spiral.arff (312, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************lsun.arff (400, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************large_dataset_count,error_count,len(onlyfiles) 3 4 122\n",
      "wdbc.arff (569, 31)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************magic.gamma.preproc.csv (19020, 10)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************ecoli.arff (336, 7)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************imgseg.preproc.csv (2310, 18)\n",
      "*pvals < 0, pvals > 1 or pvals contains NaNs\n",
      "cpu.arff (209, 7)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************skin.preproc.csv (245057, 4)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/skin.preproc.csv' already exists\n",
      "shuttle.preproc.csv (58000, 10)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/shuttle.preproc.csv' already exists\n",
      "wine.arff (178, 13)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************haberman.arff (306, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************wisc.arff (699, 9)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************iono.arff (351, 33)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************zoo.arff (101, 16)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************water-treatment.arff (527, 22)\n",
      "Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "gas.preproc.csv (13790, 129)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/gas.preproc.csv' already exists\n",
      "letter.arff (20000, 16)\n",
      "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************iris.arff (150, 4)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************wine.preproc.csv (6497, 12)\n",
      "**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************comm.and.crime.preproc.csv (1994, 102)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/comm.and.crime.preproc.csv' already exists\n",
      "segment.arff (2310, 18)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************vehicle.arff (846, 18)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************vowel.arff (990, 10)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************fault.preproc.csv (1941, 28)\n",
      "14-th leading minor of the array is not positive definite\n",
      "sonar.arff (208, 60)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/sonar.arff' already exists\n",
      "String attributes not supported yet, sorry\n",
      "dermatology.arff (366, 1)\n",
      "Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "balance-scale.arff (625, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************************************************************************************************************************************************************************************************pageb.preproc.csv (5473, 11)\n",
      "******************************************************************************************************************************************************************************************************************************************************************************************************************************************************spambase.preproc.csv (4601, 58)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/spambase.preproc.csv' already exists\n",
      "synthetic.preproc.csv (20000, 10)\n",
      "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************wave.preproc.csv (5000, 22)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************tae.arff (151, 3)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************yeast.preproc.csv (1484, 8)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************abalone.preproc.csv (4177, 8)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************heart-statlog.arff (270, 13)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************thy.arff (215, 5)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************glass.arff (214, 9)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************german.arff (1000, 7)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************arrhythmia.arff (452, 262)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/arrhythmia.arff' already exists\n",
      "letter.rec.preproc.csv (20000, 16)\n",
      "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************opt.digits.preproc.csv (5620, 63)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/opt.digits.preproc.csv' already exists\n",
      "concrete.preproc.csv (1030, 9)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************large_dataset_count,error_count,len(onlyfiles) 8 13 41\n"
     ]
    }
   ],
   "source": [
    "# Clustering Non-Stream\n",
    "onlyfolders = [f for f in listdir(data_source_clust) if not isfile(join(data_source_clust, f))]\n",
    "for folder in onlyfolders:\n",
    "    data_source_clust2=data_source_clust+folder+'/'\n",
    "    onlyfiles=([f for f in listdir(data_source_clust2) if isfile(join(data_source_clust2, f))])\n",
    "\n",
    "    error_count=0\n",
    "    large_dataset_count=0\n",
    "    max_size=20001\n",
    "\n",
    "    K=25\n",
    "    r0=1\n",
    "    conv_criteria=0.1\n",
    "\n",
    "    for f in onlyfiles:\n",
    "        file_path=data_source_clust2+f\n",
    "        try:\n",
    "            df,X,y=load_data(file_path)\n",
    "            XX=pd.DataFrame(X)\n",
    "            X=XX[XX.columns[(XX.var()!=0)]]\n",
    "            print(f,X.shape)\n",
    "#             numiters=1\n",
    "            numiters=np.max([np.int(len(X)/16),300])\n",
    "            if X.shape[0]<max_size and X.shape[1]<=35:\n",
    "                if X.shape[0]/X.shape[1]<5 or X.shape[1]>35:\n",
    "                    for n_comp in np.arange(25,X.shape[1],5):\n",
    "                        pca = PCA(n_components=n_comp)\n",
    "                        principalComponents = pca.fit_transform(X)\n",
    "                        principalDf = pd.DataFrame(data = principalComponents)\n",
    "                        if np.sum(pca.explained_variance_ratio_)>0.90 or n_comp==35:\n",
    "                            break    \n",
    "                    X=np.array(principalDf)\n",
    "                output=incad_new_labels5(X,K,numiters,r0,conv_criteria)\n",
    "                output['df']=df\n",
    "                output['y']=y\n",
    "                pickle.dump( output, open(results_path_non_stream_clust+f+\".pickle\", \"wb\" ))\n",
    "\n",
    "            else:\n",
    "                large_dataset_count+=1\n",
    "                shutil.move(file_path, data_source_large)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count+=1\n",
    "            print(e)\n",
    "    print(\"large_dataset_count,error_count,len(onlyfiles)\",large_dataset_count,error_count,len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:56:41.705310Z",
     "start_time": "2020-05-13T17:36:18.164415Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambient_temperature_system_failure.csv (7267, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/ambient_temperature_system_failure.csv' already exists\n",
      "http.mat (567498, 3)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/http.mat' already exists\n",
      "machine_temperature_system_failure.csv (22695, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/machine_temperature_system_failure.csv' already exists\n",
      "skin.preproc.csv (245057, 4)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/skin.preproc.csv' already exists\n",
      "mnist.mat (7603, 78)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/mnist.mat' already exists\n",
      "Failed to load the extension \n",
      "cannot unpack non-iterable NoneType object\n",
      "shuttle.preproc.csv (58000, 10)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/shuttle.preproc.csv' already exists\n",
      "shuttle.mat (49097, 9)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/shuttle.mat' already exists\n",
      "rogue_agent_key_updown.csv (5315, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/rogue_agent_key_updown.csv' already exists\n",
      "shuttle-unsupervised-ad.csv (46464, 9)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/shuttle-unsupervised-ad.csv' already exists\n",
      "nyc_taxi.csv (10320, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/nyc_taxi.csv' already exists\n",
      "birch-rg1.arff (100000, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/birch-rg1.arff' already exists\n",
      "gas.preproc.csv (13790, 129)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/gas.preproc.csv' already exists\n",
      "optdigits.mat (5216, 62)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/optdigits.mat' already exists\n",
      "satellite.mat (6435, 36)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/satellite.mat' already exists\n",
      "birch-rg3.arff (100000, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/birch-rg3.arff' already exists\n",
      "cpu_utilization_asg_misconfiguration.csv (18050, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/cpu_utilization_asg_misconfiguration.csv' already exists\n",
      "birch-rg2.arff (100000, 2)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/birch-rg2.arff' already exists\n",
      "smtp.mat (95156, 3)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/smtp.mat' already exists\n",
      "satimage-2.mat (5803, 36)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/satimage-2.mat' already exists\n",
      "cover.mat (286048, 10)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/cover.mat' already exists\n",
      "kdd99-unsupervised-ad.csv (620098, 29)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/kdd99-unsupervised-ad.csv' already exists\n",
      "opt.digits.preproc.csv (5620, 63)\n",
      "Destination path '/Users/lekhag/Documents/INCAD Improvements/Parallel Gibbs/Cleaned Version/Data/Stream/Large/opt.digits.preproc.csv' already exists\n",
      "large_dataset_count,error_count,len(onlyfiles) 22 23 23\n",
      "\n",
      "ec2_request_latency_system_failure.csv (4032, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************Begin Streaming\n",
      "*****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************Failed to load the extension \n",
      "cannot unpack non-iterable NoneType object\n",
      "arrhythmia.mat (452, 257)\n",
      "****************************************************Converged\n",
      "Begin Streaming\n",
      "******************************************************************************************************************************************************************************************************************************************************************************************************musk.mat (3062, 166)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************Begin Streaming\n",
      "***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************'numpy.ndarray' object has no attribute 'columns'\n",
      "comm.and.crime.preproc.csv (1994, 102)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************Begin Streaming\n",
      "*************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************speech.mat (3686, 400)\n",
      "****************************************************Converged\n",
      "Begin Streaming\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************'dict' object has no attribute 'columns'\n",
      "sonar.arff (208, 60)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************Begin Streaming\n",
      "****************************************************************************************************************************************rogue_agent_key_hold.csv (1882, 2)\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************Begin Streaming\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************spambase.preproc.csv (4601, 58)\n",
      "**Fitting the mixture model failed because some components have ill-defined empirical covariance (for instance caused by singleton or collapsed samples). Try to decrease the number of components, or increase reg_covar.\n",
      "arrhythmia.arff (452, 262)\n",
      "****************************************************Converged\n",
      "Begin Streaming\n",
      "******************************************************************************************************************************************************************************************************************************************************************************************************large_dataset_count,error_count,len(onlyfiles) 0 4 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AD Stream\n",
    "onlyfolders = [f for f in listdir(data_source_stream) if (not isfile(join(data_source_stream, f)) and ('gas_sensors' not in f))]\n",
    "for folder in onlyfolders:\n",
    "    data_source_stream2=data_source_stream+folder+'/'\n",
    "    onlyfiles=([f for f in listdir(data_source_stream2) if isfile(join(data_source_stream2, f))])\n",
    "    error_count=0\n",
    "    large_dataset_count=0\n",
    "\n",
    "    K=10\n",
    "    r0=1\n",
    "    conv_criteria=0.1\n",
    "    batch_proportion=0.35\n",
    "    max_size=5000\n",
    "\n",
    "\n",
    "    for f in onlyfiles:\n",
    "        file_path=data_source_stream2+f\n",
    "        try:\n",
    "            df,X,y=load_data(file_path)\n",
    "            if 'timestamp' in df.columns:\n",
    "                df1=df\n",
    "                df1['timestamp']=pd.to_datetime(df1['timestamp'])\n",
    "                df1['new_date']=[d.date() for d in df1['timestamp']]\n",
    "                df1['new_time']=[d.time() for d in df1['timestamp']]\n",
    "                df1['mins']=[d.hour*60+d.minute for d in df1['new_time']]\n",
    "                df1=df.drop(['timestamp','new_date','new_time'],axis=1)\n",
    "                X=np.array(df1).astype(float)                \n",
    "            XX=pd.DataFrame(X)\n",
    "            X=XX[XX.columns[(XX.var()!=0)]]\n",
    "            print(f,X.shape)\n",
    "            numiters=1\n",
    "            numiters=np.max([np.int(len(X)/16),300])\n",
    "            if X.shape[0]<max_size:\n",
    "                if X.shape[0]/X.shape[1]<5 or X.shape[1]>35:\n",
    "                    for n_comp in np.arange(25,X.shape[1],5):\n",
    "                        pca = PCA(n_components=n_comp)\n",
    "                        principalComponents = pca.fit_transform(X)\n",
    "                        principalDf = pd.DataFrame(data = principalComponents)\n",
    "                        if np.sum(pca.explained_variance_ratio_)>0.90 or n_comp==35:\n",
    "                            break    \n",
    "                    X=np.array(principalDf)\n",
    "                output=incad_new_labels_stream(X,K,batch_proportion,numiters,r0,conv_criteria)\n",
    "                output['df']=df\n",
    "                output['y']=y\n",
    "                pickle.dump( output, open(results_path_stream+f+\".pickle\", \"wb\" ))\n",
    "            else:\n",
    "                large_dataset_count+=1\n",
    "                shutil.move(file_path, data_source_large)\n",
    "        except Exception as e:\n",
    "            error_count+=1\n",
    "            print(e)\n",
    "    print(\"large_dataset_count,error_count,len(onlyfiles)\",large_dataset_count,error_count,len(onlyfiles))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T05:43:58.557025Z",
     "start_time": "2020-05-13T20:56:42.248181Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "# AD Gas Sensor\n",
    "if 1:\n",
    "    folder='gas_sensors'\n",
    "    data_source_stream2=data_source_stream+folder+'/'\n",
    "    onlyfiles=([f for f in listdir(data_source_stream2) if isfile(join(data_source_stream2, f))])\n",
    "    \n",
    "    error_count=0\n",
    "    large_dataset_count=0\n",
    "\n",
    "    K=5\n",
    "    r0=1\n",
    "    conv_criteria=0.1\n",
    "    batch_proportion=0.25\n",
    "    max_size=50000\n",
    "    \n",
    "    onlyfiles = [f for f in listdir(data_source_stream2) if isfile(join(data_source_stream2, f))]\n",
    "\n",
    "    df0=pd.concat([pd.read_table(data_source_stream2+onlyfiles[0], sep=' ',header=None, dtype='str')] )\n",
    "\n",
    "    for f in onlyfiles[1:]:\n",
    "        df0=df0.append(pd.read_table(data_source_stream2+f, sep=' ',header=None, dtype='str'))\n",
    "    for c in df0.columns:\n",
    "        if c>0:\n",
    "            df0[c]=df0[c].str.split(':').str[1]\n",
    "    df0=df0.astype(float)\n",
    "\n",
    "    df=df0.drop([0], axis=1)\n",
    "    y=np.array(df0[0])\n",
    "    print(df0.shape, df.shape, y.shape)\n",
    "    X=np.array(df)\n",
    "    numiters=np.max([np.int(len(X)/16),300])\n",
    "#     numiters=1\n",
    "    if X.shape[0]/X.shape[1]<5 or X.shape[1]>35:\n",
    "        for n_comp in np.arange(25,X.shape[1],5):\n",
    "            pca = PCA(n_components=n_comp)\n",
    "            principalComponents = pca.fit_transform(X)\n",
    "            principalDf = pd.DataFrame(data = principalComponents)\n",
    "            if np.sum(pca.explained_variance_ratio_)>0.90 or n_comp==35:\n",
    "                break    \n",
    "        X=np.array(principalDf)\n",
    "#     output=incad_new_labels5(X,K,numiters,r0,conv_criteria)\n",
    "    output=incad_new_labels_stream(X,K,batch_proportion,numiters,r0,conv_criteria)\n",
    "    output['df']=df\n",
    "    output['y']=y\n",
    "    pickle.dump( output, open(results_path_stream+folder+\".pickle\", \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### GAS Sensor Data Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T21:58:40.056549Z",
     "start_time": "2020-05-19T21:58:40.051008Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batch8.dat',\n",
       " 'batch9.dat',\n",
       " 'batch4.dat',\n",
       " 'batch5.dat',\n",
       " 'batch7.dat',\n",
       " 'batch6.dat',\n",
       " 'batch2.dat',\n",
       " 'batch3.dat',\n",
       " 'batch1.dat',\n",
       " 'batch10.dat']"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypath=data_source_stream+'gas_sensors/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T00:32:36.603193Z",
     "start_time": "2020-05-19T22:48:01.241646Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch8.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "*****************************************************************************************************************************************************************************************************************************saved\n",
      "batch9.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "*****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************saved\n",
      "batch4.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "*************************************************************************************************************************saved\n",
      "batch5.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "****************************************************************************************************************************************************saved\n",
      "batch7.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************saved\n",
      "batch6.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "*********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************saved\n",
      "batch2.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "*********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************saved\n",
      "batch3.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************saved\n",
      "batch1.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "**********************************************************************************************************************************************************************************************************************************************************************************************************************************************saved\n",
      "batch10.dat\n",
      "****************************************************************************************************Begin Streaming\n",
      "************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************saved\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "for file_num,file in enumerate(onlyfiles):\n",
    "    df0=pd.concat([pd.read_table(mypath+file, sep=' ',header=None, dtype='str')] )\n",
    "    df0=df0.sort_values(by=0)\n",
    "    print(file)\n",
    "    for c in df0.columns:\n",
    "        if c>0:\n",
    "            df0[c]=df0[c].str.split(':').str[1]\n",
    "    df0=df0.astype(float)\n",
    "\n",
    "    df=df0.drop([0], axis=1)\n",
    "    truth=np.array(df0[0])\n",
    "    pca_df = PCA(n_components=10)\n",
    "    X = pca_df.fit_transform(df)\n",
    "    # print(np.sum(pca_df.explained_variance_ratio_),X.shape, df0.shape, df.shape, truth.shape)\n",
    "\n",
    "    K=10\n",
    "    numiters=100\n",
    "    batch_proportion=0.25\n",
    "    r0=1\n",
    "    conv_criteria=0.001\n",
    "    # output=incad_new_labels5(X,K,numiters,r0,conv_criteria)\n",
    "    output=incad_new_labels_stream(X,K,batch_proportion,numiters,r0,conv_criteria)\n",
    "    output['X']=X\n",
    "    output['y']=truth\n",
    "    name='Gas_sensor_'+str(file)\n",
    "    pickle.dump( output, open( results_path_stream+name+\".pickle\", \"wb\" ) )\n",
    "    print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### Large Non-Streaming Data Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:31:56.464031Z",
     "start_time": "2020-05-20T04:31:56.458018Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mypath=data_source_stream\n",
    "onlyfolders = [f for f in listdir(mypath) if  not isfile(join(mypath, f)) and(f!=\"gas_sensors\")]\n",
    "\n",
    "onlyfiles=np.array([])\n",
    "for folder in onlyfolders:\n",
    "    ss=np.array([folder+\"/\"+f for f in listdir(mypath+folder+'/') if isfile(join(mypath+folder+'/', f))])\n",
    "    onlyfiles=np.append(onlyfiles,ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:32:07.632591Z",
     "start_time": "2020-05-20T04:32:00.490239Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large/http.mat\n",
      "Large/skin.preproc.csv\n",
      "Large/mnist.mat\n",
      "Large/.DS_Store\n",
      "Failed to load the extension \n",
      "cannot unpack non-iterable NoneType object\n",
      "Large/arrhythmia.mat\n",
      "Large/shuttle.preproc.csv\n",
      "Large/shuttle.mat\n",
      "Large/musk.mat\n",
      "Large/shuttle-unsupervised-ad.csv\n",
      "Large/birch-rg1.arff\n",
      "Large/mnist.raw.pickle\n",
      "'numpy.ndarray' object has no attribute 'columns'\n",
      "Large/gas.preproc.csv\n",
      "Large/optdigits.mat\n",
      "Large/comm.and.crime.preproc.csv\n",
      "Large/satellite.mat\n",
      "Large/speech.mat\n",
      "Large/satimage.pickle\n",
      "'dict' object has no attribute 'columns'\n",
      "Large/birch-rg3.arff\n",
      "Large/sonar.arff\n",
      "Large/birch-rg2.arff\n",
      "Large/smtp.mat\n",
      "Large/spambase.preproc.csv\n",
      "Large/satimage-2.mat\n",
      "Large/cover.mat\n",
      "Large/kdd99-unsupervised-ad.csv\n",
      "Large/arrhythmia.arff\n",
      "Large/opt.digits.preproc.csv\n",
      "Stream_data/ambient_temperature_system_failure.csv\n",
      "Stream_data/machine_temperature_system_failure.csv\n",
      "Stream_data/ec2_request_latency_system_failure.csv\n",
      "Stream_data/.DS_Store\n",
      "Failed to load the extension \n",
      "cannot unpack non-iterable NoneType object\n",
      "Stream_data/rogue_agent_key_updown.csv\n",
      "Stream_data/nyc_taxi.csv\n",
      "Stream_data/cpu_utilization_asg_misconfiguration.csv\n",
      "Stream_data/rogue_agent_key_hold.csv\n"
     ]
    }
   ],
   "source": [
    "for folder in onlyfolders:\n",
    "    onlyfiles= [folder+\"/\"+f for f in listdir(mypath+folder+'/') if isfile(join(mypath+folder+'/', f))]\n",
    "    for filename in onlyfiles:\n",
    "        print(filename)\n",
    "        \n",
    "        try:\n",
    "            df,X,y=load_data(mypath+filename)\n",
    "            if ('timestamp' in df.columns) and (folder!='Stream_data'):\n",
    "                shutil.move(mypath+filename, data_source_stream+'Stream_data')\n",
    "            elif ('timestamp' not in df.columns) and (folder=='Stream_data'):\n",
    "                shutil.move(mypath+filename, data_source_large)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T06:41:18.810547Z",
     "start_time": "2020-05-20T06:31:27.732641Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smtp.mat (95156, 3)\n",
      "spambase.preproc.csv (4601, 35)\n",
      "***spambase.preproc.csv\n",
      "Fitting the mixture model failed because some components have ill-defined empirical covariance (for instance caused by singleton or collapsed samples). Try to decrease the number of components, or increase reg_covar.\n",
      "satimage-2.mat (5803, 25)\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************cover.mat (286048, 10)\n",
      "kdd99-unsupervised-ad.csv (620098, 29)\n",
      "arrhythmia.arff (452, 30)\n",
      "****************************************************Converged\n",
      "opt.digits.preproc.csv (5620, 35)\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************large_dataset_count,error_count,len(onlyfiles) 3 1 26\n"
     ]
    }
   ],
   "source": [
    "mypath=data_source_large+'/'\n",
    "onlyfiles= [f for f in listdir(mypath) if isfile(join(mypath, f)) and ('.DS_Store' not in f) ]\n",
    "\n",
    "\n",
    "error_count=0\n",
    "large_dataset_count=0\n",
    "max_size=40001\n",
    "\n",
    "K=10\n",
    "r0=1\n",
    "conv_criteria=0.1\n",
    "\n",
    "for filename in onlyfiles[19:]:\n",
    "    try:\n",
    "        df,X,y=load_data(mypath+filename)\n",
    "        XX=pd.DataFrame(X)\n",
    "        X=XX[XX.columns[(XX.var()!=0)]]\n",
    "        numiters=500\n",
    "#         numiters=np.max([np.int(len(X)/16),500])\n",
    "        if X.shape[0]<max_size:\n",
    "            if X.shape[0]/X.shape[1]<5 or X.shape[1]>35:\n",
    "                for n_comp in np.arange(25,X.shape[1],5):\n",
    "                    pca = PCA(n_components=n_comp)\n",
    "                    principalComponents = pca.fit_transform(X)\n",
    "                    principalDf = pd.DataFrame(data = principalComponents)\n",
    "                    if np.sum(pca.explained_variance_ratio_)>0.90 or n_comp==35:\n",
    "                        break    \n",
    "                X=np.array(principalDf)\n",
    "            print(filename, X.shape)\n",
    "            output=incad_new_labels5(X,K,numiters,r0,conv_criteria)\n",
    "            output['df']=df\n",
    "            output['y']=y\n",
    "            pickle.dump( output, open(results_path_non_stream_ad+\"LARGE_\"+filename+\".pickle\", \"wb\" ))\n",
    "        else:\n",
    "            print(filename, X.shape)\n",
    "            large_dataset_count+=1\n",
    "    except Exception as e:\n",
    "        error_count+=1\n",
    "        print(filename)\n",
    "        print(e)\n",
    "\n",
    "print(\"large_dataset_count,error_count,len(onlyfiles)\",large_dataset_count,error_count,len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-20T06:33:18.899Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http.mat (567498, 3)\n",
      "skin.preproc.csv (245057, 4)\n",
      "mnist.mat (7603, 78)\n",
      "arrhythmia.mat (452, 257)\n",
      "shuttle.preproc.csv (58000, 10)\n"
     ]
    }
   ],
   "source": [
    "mypath=data_source_large+'/'\n",
    "onlyfiles= [f for f in listdir(mypath) if isfile(join(mypath, f)) and ('.DS_Store' not in f) ]\n",
    "\n",
    "\n",
    "error_count=0\n",
    "large_dataset_count=0\n",
    "max_size=100001\n",
    "\n",
    "K=10\n",
    "r0=1\n",
    "conv_criteria=0.1\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    try:\n",
    "        df,X,y=load_data(mypath+filename)\n",
    "        XX=pd.DataFrame(X)\n",
    "        X=XX[XX.columns[(XX.var()!=0)]]\n",
    "        numiters=300\n",
    "#         numiters=np.max([np.int(len(X)/16),500])\n",
    "        if X.shape[0]<max_size and X.shape[0]>40000:\n",
    "            if X.shape[0]/X.shape[1]<5 or X.shape[1]>35:\n",
    "                for n_comp in np.arange(25,X.shape[1],5):\n",
    "                    pca = PCA(n_components=n_comp)\n",
    "                    principalComponents = pca.fit_transform(X)\n",
    "                    principalDf = pd.DataFrame(data = principalComponents)\n",
    "                    if np.sum(pca.explained_variance_ratio_)>0.90 or n_comp==35:\n",
    "                        break    \n",
    "                X=np.array(principalDf)\n",
    "            print(filename, X.shape)\n",
    "            output=incad_new_labels5(X,K,numiters,r0,conv_criteria)\n",
    "            output['df']=df\n",
    "            output['y']=y\n",
    "            pickle.dump( output, open(results_path_non_stream_ad+\"LARGE_\"+filename+\".pickle\", \"wb\" ))\n",
    "        else:\n",
    "            print(filename, X.shape)\n",
    "            large_dataset_count+=1\n",
    "    except Exception as e:\n",
    "        error_count+=1\n",
    "        print(filename)\n",
    "        print(e)\n",
    "\n",
    "print(\"large_dataset_count,error_count,len(onlyfiles)\",large_dataset_count,error_count,len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### Streaming Data Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:43:53.775808Z",
     "start_time": "2020-05-20T03:43:53.631900Z"
    },
    "hidden": true
   },
   "source": [
    "mypath=data_source_stream+'Stream_data/'\n",
    "onlyfiles= [f for f in listdir(mypath) if isfile(join(mypath, f)) and ('.DS_Store' not in f) ]\n",
    "for filename in onlyfiles:\n",
    "        df,X,y=load_data(mypath+filename)\n",
    "        print(filename, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
